<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文解读 - EMVS: Event-Based Multi-View Stereo</title>
    <link href="/2020/06/10/EMVS%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <url>/2020/06/10/EMVS%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<p>EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time 描述了一种利用 Event Camera 的高速特性，提出的一种 MVS 方法。将传统 MVS 的 Space-Sweep 方法应用到 Event-based MVS 中。<br><a id="more"></a></p><h1 id="EMVS-Event-Based-Multi-View-Stereo—3D-Reconstruction-with-an-Event-Camera-in-Real-Time"><a href="#EMVS-Event-Based-Multi-View-Stereo—3D-Reconstruction-with-an-Event-Camera-in-Real-Time" class="headerlink" title="EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time"></a>EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time</h1><blockquote><p>Rebecq, H., Gallego, G., Mueggler, E., Scaramuzza, D.,<br><em><a href="https://doi.org/10.1007/s11263-017-1050-6" target="_blank" rel="noopener">EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time</a></em>,<br>Int. J. of Computer Vision (IJCV), 126(12):1394-1414, 2018. <a href="http://rpg.ifi.uzh.ch/docs/IJCV17_Rebecq.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://youtu.be/EFpZcpd9XJ0" target="_blank" rel="noopener">YouTube</a>, <a href="https://github.com/uzh-rpg/rpg_emvs" target="_blank" rel="noopener">Code</a>.</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇 paper 主要描述了一种利用 Event Camera 的高速特性，提出的一种 MVS 方法。将传统 MVS 的 Space-Sweep 方法应用到 Event-based MVS 中；在 Event-based 场景运用 Space-Sweep 具有天然的优势：</p><ol><li><p>Event Camera 本来就是检测的场景中的 edge。</p></li><li><p>Event Camera 相比于固定帧率的传统 Camera，viewpoints 更加的稠密。</p><p><img src="/assets/image-20200614164507222.jpeg" srcset="/img/loading.gif" alt="image-20200614164507222" style="zoom:50%;" /></p></li></ol><p>文章的整体思路简而言之，就是将“每一帧”的 Events 进行反投影得到了一束光线，通过计算经过 DSI 空间中每一个 voxel 的光线的数量，高于一个阈值即可确定为一个空间的 3D 点。具体实施起来，分为下文中的五步。</p><p>完整的公式推导见原论文。</p><h2 id="主要过程"><a href="#主要过程" class="headerlink" title="主要过程"></a>主要过程</h2><h3 id="Feature-Viewing-Rays-by-Event-Back-Projection"><a href="#Feature-Viewing-Rays-by-Event-Back-Projection" class="headerlink" title="Feature-Viewing Rays by Event Back-Projection"></a>Feature-Viewing Rays by Event Back-Projection</h3><p>将每一个 Event 进行反投影，得到一条光线。</p><blockquote><p>This higher abundance of measurements and viewpoints in the event-based setting generates many more viewing rays than in frame-based MVS, and therefore, it facilitates the detection of scene points by analyzing the regions of high ray density.</p><p>A major advantage of our method is that no explicit data association is needed.</p></blockquote><p>不需要 Event 之间的数据关联、不需要 intensity 信息。</p><h3 id="Volumetric-Ray-Counting-Creating-the-Disparity-Space-Image-DSI"><a href="#Volumetric-Ray-Counting-Creating-the-Disparity-Space-Image-DSI" class="headerlink" title="Volumetric Ray Counting. Creating the Disparity Space Image (DSI)"></a>Volumetric Ray Counting. Creating the Disparity Space Image (DSI)</h3><p>在 DSI 中进行计数。这个时候问题就来了，如何确定这个 DSI 空间呢？</p><p>作者考虑到 <code>the reconstruction of large scenes in a scalable way</code>，采用了一个分批的方式，将一组 Event 归位一批，选取一个虚拟帧 RV，基于它做一个 <code>local 3D reconstruction</code>。</p><p><img src="/assets/image-20200614172203728.jpeg" srcset="/img/loading.gif" alt="image-20200614172203728" style="zoom:50%;" /></p><h3 id="Detection-of-Scene-Structure-by-Maximization-of-Ray-Density"><a href="#Detection-of-Scene-Structure-by-Maximization-of-Ray-Density" class="headerlink" title="Detection of Scene Structure by Maximization of Ray Density"></a>Detection of Scene Structure by Maximization of Ray Density</h3><p>Local DSI 建立好之后，就卡一个阈值，是一个 <code>local maxima</code> 的操作。一个DSI的实例以及使用 <code>local maxima</code> 而不是 <code>global maxima</code> 的原因如下：</p><p><img src="/assets/image-20200614172541662.jpeg" srcset="/img/loading.gif" alt="image-20200614172541662" style="zoom:50%;" /></p><p>作者使用了一个 Adaptive Gaussian 阈值函数：</p><blockquote><p>a pixel $(x, y)$ is selected if $c(x, y) &gt; T(x, y)$, with $T(x, y)=c(x, y) * G \sigma(x, y)-C$. In practice, we use a 5×5 neighborhood in $G \sigma(x, y)$ and $C$ = −10</p></blockquote><p>从 DSI 中提取结构：</p><p><img src="/assets/image-20200614174118964.jpeg" srcset="/img/loading.gif" alt="image-20200614174118964" style="zoom:50%;" /></p><h3 id="Merging-Depth-Maps-from-Multiple-Reference-Viewpoints"><a href="#Merging-Depth-Maps-from-Multiple-Reference-Viewpoints" class="headerlink" title="Merging Depth Maps from Multiple Reference Viewpoints"></a>Merging Depth Maps from Multiple Reference Viewpoints</h3><p>既然有了上面提到的 Local reconstruction，那么就肯定有 key frame 的概念和一次 local 的大小的确定。</p><blockquote><p>we select a new <em>key</em> reference view as soon as the distance to the previous <em>key</em> reference view exceeds a certain percent- age of the mean scene depth (typically a number between 15 and 40%), and use the subset of events until the next <em>key</em> ref- erence view to estimate the corresponding semi-dense depth map of the scene.</p></blockquote><p>然后再做滤波去除一些噪声。融合的话，有了每组 local 点云的 pose ，有很多的方法就可以选了。</p><h3 id="Map-Cleaning"><a href="#Map-Cleaning" class="headerlink" title="Map Cleaning"></a>Map Cleaning</h3><p>为了得到更好的效果，又进一步做了滤波。</p><blockquote><p>we use a median filter on the semi-dense depth maps</p><p>we also apply a radius filter (Rusu and Cousins 2011) to the final point cloud, which discards the points whose number of neighbors within a given radius is less than a threshold.</p></blockquote><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><h3 id="实现实时、高效的-Event-反投影到-DSI-中"><a href="#实现实时、高效的-Event-反投影到-DSI-中" class="headerlink" title="实现实时、高效的 Event 反投影到 DSI 中"></a>实现实时、高效的 Event 反投影到 DSI 中</h3><p>主要思路：DSI 是由不同 depth 的平面们组成的，也就是说明只有 translation，无 rotation。那么，求出一个相机的当前帧与一个深度为 $Z_0$ 的 plane 之间的 homography，与其他 DSI 中的深度为 $Z_i$ 的 plane 的 homography 就可以很简单的来计算，可以推导为一步公式。</p><p>相机平面、EV 虚拟相机平面、不同深度 $Zi$ DSI 平面之间的关系：</p><script type="math/tex; mode=display">\left(x\left(Z_{0}\right), y\left(Z_{0}\right), 1\right)^{\top} \sim_{\mathrm{H} Z_{0}}(u, v, 1)^{\top}</script><script type="math/tex; mode=display">\left(x\left(Z_{i}\right), y\left(Z_{i}\right), 1\right)^{\top} \sim H_{Z_{i}} H_{Z_{0}}^{-1}\left(x\left(Z_{0}\right), y\left(Z_{0}\right), 1\right)^{\top}</script><script type="math/tex; mode=display">\mathrm{H}_{Z_{i}} \mathrm{H}_{Z_{0}}^{-1} \stackrel{(8)}{=}\left(\mathrm{R}+\frac{1}{Z_{i}} \mathbf{e}_{3}^{\top}\right)^{-1}\left(\mathrm{R}+\frac{1}{Z_{0}} \mathbf{e}_{3}^{\top}\right)</script><p>其中，$\mathbf{e}_{3}$为平面法向量，因为与 EV 相机平面垂直，所以为$(0, 0, 1)$，在后面的公式化简中有用到。</p><p>令$\left(C<em>{x}, C</em>{y}, C<em>{z}\right)^{\top} \doteq \mathbf{C}=-R^{\top} \mathbf{t}$，通过展开、以及利用$\mathbf{e}</em>{3}$性质，</p><script type="math/tex; mode=display">\mathrm{H}_{Z_{i}} \mathrm{H}_{Z_{0}}^{-1} \sim \mathrm{I}+\frac{Z_{0}-Z_{i}}{Z_{0}\left(Z_{i}-C_{z}\right)} \mathbf{C e}_{3}^{\top}</script><p>进一步代入上面几个平面的关系，化简得：</p><script type="math/tex; mode=display">\begin{array}{l}x\left(Z_{i}\right)=\frac{Z_{0}}{Z_{i}} \delta x\left(Z_{0}\right)+\frac{1}{Z_{i}}(1-\delta) C_{x} \\y\left(Z_{i}\right)=\frac{Z_{0}}{Z_{i}} \delta y\left(Z_{0}\right)+\frac{1}{Z_{i}}(1-\delta) C_{y}\end{array}</script><p>其中$\delta=\left(Z<em>{i}-Z</em>{0}\right) /\left(Z<em>{0}-C</em>{z}\right)$。</p><p>至此，计算效率就提高上来了。</p><p>算法流程：</p><p><img src="/assets/image-20200614203043203.jpeg" srcset="/img/loading.gif" alt="image-20200614203043203" style="zoom:50%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>3D-Vision</tag>
      
      <tag>MVS</tag>
      
      <tag>3D-Reconstruction</tag>
      
      <tag>Event-Camera</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cartographer Laser-SLAM 算法流程解析与调优</title>
    <link href="/2019/12/07/Cartographer%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%B0%83%E4%BC%98/"/>
    <url>/2019/12/07/Cartographer%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%B0%83%E4%BC%98/</url>
    
    <content type="html"><![CDATA[<p>本篇博客的目的是记录自己在实物机器人上调试 Cartographer 算法的过程中，参考的资料和结合部分代码，一些自己总结的内容。Cartographer 是 Google 开源的一个激光 SLAM 项目，有着非常惊艳的工程实现。<br><a id="more"></a></p><p>本文参考cartographer<a href="https://google-cartographer-ros.readthedocs.io/en/latest/" target="_blank" rel="noopener">官方文档</a>。</p><p>首先要明确的一点，Cartographer的官方文档也提到，这个<strong>简介</strong>仅仅给出直觉概述层面上的Cartographer的不同子系统的介绍和配置说明。如果为了更详细的描述和更严谨的表达，应该去看Cartographer的Paper。虽然Paper中仅仅给出了2D SLAM的重要概念的严格表达，但这些概念可以非常自然的推广到3D。</p><p>Paper：</p><blockquote><p>W. Hess, D. Kohler, H. Rapp, and D. Andor, <a href="https://research.google.com/pubs/pub45466.html" target="_blank" rel="noopener">Real-Time Loop Closure in 2D LIDAR SLAM</a>, in <em>Robotics and Automation (ICRA), 2016 IEEE International Conference on</em>. IEEE, 2016. pp. 1271–1278.</p></blockquote><h1 id="Cartographer-Algorithm-walkthrough-for-tuning"><a href="#Cartographer-Algorithm-walkthrough-for-tuning" class="headerlink" title="Cartographer Algorithm walkthrough for tuning"></a>Cartographer Algorithm walkthrough for tuning</h1><h2 id="cartographer基本思路简介"><a href="#cartographer基本思路简介" class="headerlink" title="cartographer基本思路简介"></a>cartographer基本思路简介</h2><p><a href="https://blog.csdn.net/jsgaobiao/article/details/53116042" target="_blank" rel="noopener">这篇</a>CSDN博客说的挺简洁，但是现在必须成VIP才能看的文章了，有点醉…主要两张图如下：</p><ol><li><p>算法的基本思路</p><p><img src="assets/20161110144436386.png" srcset="/img/loading.gif" alt="img"></p></li><li><p>基本的ROS框架如下，当然可以有更多类型的输入，比如里程计信息等。</p><p><img src="assets/20161110145019043.png" srcset="/img/loading.gif" alt="img"></p></li></ol><p>Cartographer它的设计目标是实现低计算资源消耗，实时优化，不追求高精度（可以达到r=5cm级别的精度） Paper中说了，<strong>它主要的贡献并不在算法层面，而是提供了工程上高效率、Robust的代码实现。</strong></p><p><img src="assets/high_level_system_overview.png" srcset="/img/loading.gif" alt="Cartographer系统结构图"></p><p>Cartographer可以看做两个独立但有关系的子系统，Local SLAM 和 Global SLAM。</p><ul><li><p>Local SLAM可以被看做是前端(local trajectory builder)，任务是建立一个个的submap。各个submap是本地一致的，但是会慢慢的漂移。Local SLAM相关的配置文件是<a href="https://github.com/googlecartographer/cartographer/blob/df337194e21f98f8c7b0b88dab33f878066d4b56/configuration_files/trajectory_builder_2d.lua" target="_blank" rel="noopener">trajectory_builder_2d.lua</a>或<a href="https://github.com/googlecartographer/cartographer/blob/df337194e21f98f8c7b0b88dab33f878066d4b56/configuration_files/trajectory_builder_3d.lua" target="_blank" rel="noopener">trajectory_builder_3d.lua</a>。后面用<em>TRAJECTORY_BUILDER_nD</em>表示相通的配置options。</p></li><li><p>Golbal SLAM是后端，单开了一个线程运行在后端，主要作用是寻找 loop closure constraints(回环约束)。通过<strong>scans</strong>(gathered in <strong>nodes</strong>) 与submaps进行matching的方法来工作。并且可以结合其他传感器的数据来进一步提高精度，来确定一致性最强的Global optimization 方案。3D SLAM中还会尝试去估计重力方向。相关配置文件是<a href="https://github.com/googlecartographer/cartographer/blob/df337194e21f98f8c7b0b88dab33f878066d4b56/configuration_files/pose_graph.lua" target="_blank" rel="noopener">pose_graph.lua</a>。</p></li></ul><blockquote><p>TODO:</p><ul><li>.lua修改为本地gitlab仓库链接</li><li>Global中的scans和nodes的关系？<strong>scans</strong>(gathered in <strong>nodes</strong>)</li></ul></blockquote><p>总的来说，Local SLAM 的主要工作是生成更好的 submaps，Global SLAM的工作是将这些 submaps 更好的结合起来。</p><h2 id="Input-Process"><a href="#Input-Process" class="headerlink" title="Input Process"></a>Input Process</h2><h3 id="Laser-Data"><a href="#Laser-Data" class="headerlink" title="Laser Data"></a>Laser Data</h3><blockquote><p>注意！Cartographer中关于距离的参数的单位均为<strong>米/m</strong></p></blockquote><p>由于在实际运用的过程中，雷达在机器人身上的安装位置导致雷达被机器人身上其他部件挡了、或者雷达自身落灰了，或者一些从不期望的来源得到的最远的测量值（比如反射、或者自身传感器噪声）等，都是SLAM过程中所不期望的。对于这些噪声，Cartographer starts by applying a bandpass filter and only keeps range values between a certain min and max range。下面的这俩值根据你机器人和雷达的实际情况来确定。</p><p>这两个参数就是雷达扫描数据的距离范围。参数为下面两个。</p><pre><code class="hljs lua">TRAJECTORY_BUILDER_nD.min_rangeTRAJECTORY_BUILDER_nD.max_range</code></pre><p>Cartographer会用<code>TRAJECTORY_BUILDER_2D.missing_data_ray_length</code>的值来替换实际大于 <code>TRAJECTORY_BUILDER_nD.max_range</code> 的ranges。</p><blockquote><p>TODO:</p><ul><li><p>考虑根据具体的自己机器人的形状和雷达的实际效果，确定更好的卡值的方法？但又仔细一想好像又没这必要。</p></li><li><p><code>TRAJECTORY_BUILDER_2D.missing_data_ray_length</code>参数的说法还是有点迷……代码中是这样写的:</p><p><code>cartographer/mapping/internal/2d/local_trajectory_builder_2d.h</code> line: 170-189</p><pre><code class="hljs c++"><span class="hljs-comment">// Drop any returns below the minimum range and convert returns beyond the</span><span class="hljs-comment">// maximum range into misses.</span><span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; synchronized_data.ranges.<span class="hljs-built_in">size</span>(); ++i) &#123;  <span class="hljs-keyword">const</span> Eigen::Vector4f&amp; hit = synchronized_data.ranges[i].point_time;  <span class="hljs-keyword">const</span> Eigen::Vector3f origin_in_local =      range_data_poses[i] *      synchronized_data.origins.at(synchronized_data.ranges[i].origin_index);  <span class="hljs-keyword">const</span> Eigen::Vector3f hit_in_local = range_data_poses[i] * hit.head&lt;<span class="hljs-number">3</span>&gt;();  <span class="hljs-keyword">const</span> Eigen::Vector3f delta = hit_in_local - origin_in_local;  <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> range = delta.norm();  <span class="hljs-keyword">if</span> (range &gt;= options_.min_range()) &#123;    <span class="hljs-keyword">if</span> (range &lt;= options_.max_range()) &#123;      accumulated_range_data_.returns.push_back(hit_in_local);    &#125; <span class="hljs-keyword">else</span> &#123;      accumulated_range_data_.misses.push_back(          origin_in_local +          options_.missing_data_ray_length() / range * delta);    &#125;  &#125;&#125;</code></pre><p>具体待跑起来后分析实际数据</p></li></ul></blockquote><p>如果将 3D 雷达用在 2D SLAM的话,提供一个”截断”的参数,就是把一定高度范围内的扫描点映射到 2D 的一个平面上。</p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D.max_zTRAJECTORY_BUILDER_2D.min_z</code></pre><p>一组激光雷达的距离数据是一段时间内测量出来的，而这一段时间内机器人是运动的，这就导致了激光数据会产生畸变。这些距离数据被封装到一帧一帧的ROS message中，每一帧都带有时间戳信息，Cartographer会把多(&gt;=1)帧集合(accumulate)成一个大帧作为算法的输入。Cartographer认为这每一帧都是独立的，以帧为单位补偿运动导致的激光雷达数据产生的畸变，然后再把这些帧集合到一块去。所以当然Cartographer接收到的数据帧的频率越高，Cartographer的补偿效果越好，算法的输入数据质量越高。</p><p>所以<code>TRAJECTORY_BUILDER_nD.num_accumulated_range_data</code>参数的意义就是集合多少帧运动补偿后形成算法输入的大数据帧。(该参数要根据实际雷达的数据采集频率和每一帧数据的扫描范围来定，比如Rplidar A3的一帧数据是采样一圈；而VLP-16的一帧数据可以调成一个udp数据包而不是一圈数据，并且这样更好，因为转一圈也是需要时间的，这样就可以把一圈内的数据也做了运动补偿)</p><p>相关代码(<code>cartographer/mapping/internal/2d/local_trajectory_builder_2d.h</code> line:142-205)：</p><pre><code class="hljs c++"><span class="hljs-keyword">if</span> (num_accumulated_ == <span class="hljs-number">0</span>) &#123;  accumulation_started_ = <span class="hljs-built_in">std</span>::chrono::steady_clock::now();&#125;...<span class="hljs-keyword">if</span> (num_accumulated_ == <span class="hljs-number">0</span>) &#123;  <span class="hljs-comment">// 'accumulated_range_data_.origin' is uninitialized until the last</span>  <span class="hljs-comment">// accumulation.</span>  accumulated_range_data_ = sensor::RangeData&#123;&#123;&#125;, &#123;&#125;, &#123;&#125;&#125;;&#125;<span class="hljs-comment">// Drop any returns below the minimum range and convert returns beyond the</span><span class="hljs-comment">// maximum range into misses.</span><span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; synchronized_data.ranges.<span class="hljs-built_in">size</span>(); ++i) &#123;  ...  <span class="hljs-keyword">if</span> (range &gt;= options_.min_range()) &#123;    <span class="hljs-keyword">if</span> (range &lt;= options_.max_range()) &#123;      accumulated_range_data_.returns.push_back(hit_in_local);    &#125; <span class="hljs-keyword">else</span> &#123;      accumulated_range_data_.misses.push_back(          origin_in_local +          options_.missing_data_ray_length() / range * delta);    &#125;  &#125;&#125;++num_accumulated_;<span class="hljs-keyword">if</span> (num_accumulated_ &gt;= options_.num_accumulated_range_data()) &#123;  num_accumulated_ = <span class="hljs-number">0</span>;  <span class="hljs-keyword">const</span> transform::Rigid3d gravity_alignment = transform::Rigid3d::Rotation(      extrapolator_-&gt;EstimateGravityOrientation(time));  <span class="hljs-comment">// TODO(gaschler): This assumes that 'range_data_poses.back()' is at time</span>  <span class="hljs-comment">// 'time'.</span>  accumulated_range_data_.origin = range_data_poses.back().translation();  <span class="hljs-keyword">return</span> AddAccumulatedRangeData(      time,      TransformToGravityAlignedFrameAndFilter(          gravity_alignment.cast&lt;<span class="hljs-keyword">float</span>&gt;() * range_data_poses.back().inverse(),          accumulated_range_data_),      gravity_alignment);&#125;</code></pre><h4 id="Voxel-Filter"><a href="#Voxel-Filter" class="headerlink" title="Voxel Filter"></a>Voxel Filter</h4><p>较近的表面(如路面)经常扫描得到更多的points,而远处的物体的points经常比较稀少. 为了降低计算量, 需要对点云数据进行下采样, 简单的随机采样仍然会导致低密度区的点更少,而高密度区的点仍然比较多.因此cartographer 采用 voxel_filter (体素滤波)的方法。通过输入的点云数据创建一个三维体素栅格（可把体素栅格想象为微小的空间三维立方体的集合），然后在每个体素（即，三维立方体）内，用体素中所有点的重心来近似显示体素中其他点，这样该体素就内所有点就用一个重心点最终表示，对于所有体素处理后得到过滤后的点云。<code>TRAJECTORY_BUILDER_nD.voxel_filter_size</code>即为立方体的大小。如果立方体较小的话会导致更密集的数据，所耗的计算量更大。而较大的话可能会导致数据丢失但是计算速度会更快。</p><p>在提供了定大小的 voxel_filter, Cartographer还提供了一个 adaptive_voxel_filter,  adaptive_voxel_filter 可以在最大边长<code>TRAJECTORY_BUILDER_nD.*adaptive_voxel_filter.max_length</code>的限制下优化确定voxel_filter_size来实现目标的points数<code>TRAJECTORY_BUILDER_nD.*adaptive_voxel_filter.min_num_points</code> 。</p><p>官方给的默认配置：</p><p><code>trajectory_builder_2d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D = &#123;  ...  voxel_filter_size = <span class="hljs-number">0.025</span>,  adaptive_voxel_filter = &#123;    max_length = <span class="hljs-number">0.5</span>,    min_num_points = <span class="hljs-number">200</span>,    max_range = <span class="hljs-number">50.</span>,  &#125;,  ...&#125;</code></pre><p><code>trajectory_builder_3d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_3D = &#123;  ...    voxel_filter_size = <span class="hljs-number">0.15</span>,  high_resolution_adaptive_voxel_filter = &#123;    max_length = <span class="hljs-number">2.</span>,    min_num_points = <span class="hljs-number">150</span>,    max_range = <span class="hljs-number">15.</span>,  &#125;,  low_resolution_adaptive_voxel_filter = &#123;    max_length = <span class="hljs-number">4.</span>,    min_num_points = <span class="hljs-number">200</span>,    max_range = MAX_3D_RANGE,  &#125;,  ...&#125;</code></pre><h3 id="IMU-Data"><a href="#IMU-Data" class="headerlink" title="IMU Data"></a>IMU Data</h3><p>IMU对于SLAM可以提供非常有用的信息，Cartographer 直接利用IMU所提供的三轴线加速度与角速度信息，可以提供一个较为精确的重力方向，以及提供带有噪声但是整体方向大概正确的关于机器人旋转的信息。为了滤掉IMU的噪声，gravity is observed over a certain amount of time。在2D SLAM 中，可以做到无外界的一些补充信息来源而实时处理数据，所以 2D SLAM 可以选择是否使用 IMU 的消息。但是在 3D SLAM 中需要提供 IMU 数据作判断scans方向的先验，可以大大降低 scan 匹配的复杂性。</p><blockquote><p>cartographer中关于时间的参数单位均为s(秒).</p></blockquote><pre><code class="hljs ini"><span class="hljs-attr">TRAJECTORY_BUILDER_nD.use_imu_data</span> = <span class="hljs-literal">true</span><span class="hljs-attr">TRAJECTORY_BUILDER_nD.imu_gravity_time_constant</span> = <span class="hljs-number">10.0</span></code></pre><p>下文 imu 数据还在 Global optimization 中的应用.</p><h2 id="Local-SLAM"><a href="#Local-SLAM" class="headerlink" title="Local SLAM"></a>Local SLAM</h2><p>Once a scan has been assembled and filtered from multiple range data, it is ready for the local SLAM algorithm. Local SLAM可以利用来自与pose extrapolator的输出信息作为一个先验，把一帧新的scans通过scan matching的方法插入到当下的submap中。使用pose extrapolator的idea是，通过除了激光雷达以外的传感器的数据来预测当前scan插入到submap的位置，比如里程计信息、IMU等。</p><h2 id="Two-scan-matching-strategies"><a href="#Two-scan-matching-strategies" class="headerlink" title="Two scan matching strategies"></a>Two scan matching strategies</h2><p>CeresScanMatcher 以及 RealTimeCorrelativeScanMatcher</p><ul><li><p>CeresScanMatcher 利用上面说的先验 initial guess ,寻找 scan 与当前的 submap 最匹配的位置。通过对 submap 进行插值然后与 scan 进行对齐（It does this by interpolating the submap and sub-pixel aligning the scan）。这种做法比较快速，但是无法修复远大于子图分辨率的误差。如果你的传感器的设置与累积的那个时间区间都是合理的话,仅仅使用 CeresScanMatcher 通常是最好的选择。</p><p>CeresScanMatcher 可以为每一个输入源配置一个权重weight，权重就是对其数据的信任度，可以把它视为静态协方差。这个weight是无量纲的，并且是不能相互比较的。weight越大，Cartographer在进行scan matching的时候就对他更关注。数据来源可以包括 occupied space (points from the scan), translation and rotation from the pose extrapolator (or <code>RealTimeCorrelativeScanMatcher</code>)</p><pre><code class="hljs css"><span class="hljs-selector-tag">TRAJECTORY_BUILDER_3D</span><span class="hljs-selector-class">.ceres_scan_matcher</span><span class="hljs-selector-class">.occupied_space_weight</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_3D</span><span class="hljs-selector-class">.ceres_scan_matcher</span><span class="hljs-selector-class">.occupied_space_weight_0</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_3D</span><span class="hljs-selector-class">.ceres_scan_matcher</span><span class="hljs-selector-class">.occupied_space_weight_1</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.ceres_scan_matcher</span><span class="hljs-selector-class">.translation_weight</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.ceres_scan_matcher</span><span class="hljs-selector-class">.rotation_weight</span></code></pre><p>In 3D, the <code>occupied_space_weight_0</code> and <code>occupied_space_weight_1</code> parameters are related, respectively, to the high resolution and low resolution filtered point clouds.</p><p>CeresScanMatcher的名字来源于<a href="http://ceres-solver.org/" target="_blank" rel="noopener">Ceres Solver</a>。这个scan matching问题被建模为一个最小二乘问题，两帧间的motion是待优化变量。（GN等方法）Ceres optimizes the motion using a descent algorithm for a given number of iterations. Ceres can be configured to adapt the convergence speed to your own needs.</p><pre><code class="hljs lua">TRAJECTORY_BUILDER_nD.ceres_scan_matcher.ceres_solver_options.use_nonmonotonic_stepsTRAJECTORY_BUILDER_nD.ceres_scan_matcher.ceres_solver_options.max_num_iterationsTRAJECTORY_BUILDER_nD.ceres_scan_matcher.ceres_solver_options.num_threads</code></pre><p><code>use_nonmonotonic_steps</code>这个暂时还未深究，参见<a href="http://ceres-solver.org/nnls_solving.html#non-monotonic-steps" target="_blank" rel="noopener">Ceres-Solver官方文档讲解</a></p><p>官方给的默认配置：</p><p><code>trajectory_builder_2d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D = &#123;  ...  ceres_scan_matcher = &#123;    occupied_space_weight = <span class="hljs-number">1.</span>,    translation_weight = <span class="hljs-number">10.</span>,    rotation_weight = <span class="hljs-number">40.</span>,    ceres_solver_options = &#123;      use_nonmonotonic_steps = <span class="hljs-literal">false</span>,      max_num_iterations = <span class="hljs-number">20</span>,      num_threads = <span class="hljs-number">1</span>,    &#125;,  &#125;,  ...&#125;</code></pre></li><li><p>RealTimeCorrelativeScanMatcher 在你比较不信任你的其他传感器或者不存在其他的传感器的情况下可以启用。它的做法类似于回环检测中的做法将 scan 与当前 submap 进行 match。Best match 然后被用作 CeresScanMatcher 的先验。这种 match 的方式对计算资源要求较高，并且开了后，就忽略了其他传感器的数据。但这种做法在 feature rich 的环境中的鲁棒性非常好。</p><p>同样 RealTimeCorrelativeScanMatcher 也可以根据对 sensors 的信任度进行配置(即可以配置不同权重/置信度/weight)。它的工作原理是在一个搜索窗口中(搜索窗口的大小由搜索的最大距离半径和角度范围来指定)。在此窗口中进行 scan match 的时候,可以为 translation 和 rotation 选择不同的权重。</p><blockquote><p><em>TODO</em></p><p><em>例如当已知机器人不会旋转过多的话,就可以改变对应 weight。</em></p><p> <em>所以这个weight到底是指？具体还是看代码吧</em></p></blockquote><pre><code class="hljs lua">TRAJECTORY_BUILDER_nD.use_online_correlative_scan_matchingTRAJECTORY_BUILDER_nD.real_time_correlative_scan_matcher.linear_search_windowTRAJECTORY_BUILDER_nD.real_time_correlative_scan_matcher.angular_search_windowTRAJECTORY_BUILDER_nD.real_time_correlative_scan_matcher.translation_delta_cost_weightTRAJECTORY_BUILDER_nD.real_time_correlative_scan_matcher.rotation_delta_cost_weight</code></pre><p>官方给的默认配置：</p><p><code>trajectory_builder_2d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D = &#123;  ...  use_online_correlative_scan_matching = <span class="hljs-literal">false</span>,  real_time_correlative_scan_matcher = &#123;    linear_search_window = <span class="hljs-number">0.1</span>,    angular_search_window = <span class="hljs-built_in">math</span>.<span class="hljs-built_in">rad</span>(<span class="hljs-number">20.</span>),    translation_delta_cost_weight = <span class="hljs-number">1e-1</span>,    rotation_delta_cost_weight = <span class="hljs-number">1e-1</span>,  &#125;,  ...&#125;</code></pre></li></ul><h2 id="motion-filter"><a href="#motion-filter" class="headerlink" title="motion_filter"></a>motion_filter</h2><p>为避免将过多的 scan 插入到 submap 里，当两个 scan 成功 match 后，会得到两个 match 之间的运动关系。当两者运动关系不明显的话,这个 match 结果就不会被插入到 submap 中去。这个操作通过运动滤波器中卡time,distance以及angle的阈值来实现。</p><pre><code class="hljs css"><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.motion_filter</span><span class="hljs-selector-class">.max_time_seconds</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.motion_filter</span><span class="hljs-selector-class">.max_distance_meters</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.motion_filter</span><span class="hljs-selector-class">.max_angle_radians</span></code></pre><p>官方给的默认配置：</p><p><code>trajectory_builder_2d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D = &#123;  ...  motion_filter = &#123;    max_time_seconds = <span class="hljs-number">5.</span>,    max_distance_meters = <span class="hljs-number">0.2</span>,    max_angle_radians = <span class="hljs-built_in">math</span>.<span class="hljs-built_in">rad</span>(<span class="hljs-number">1.</span>),  &#125;,  ...&#125;</code></pre><h2 id="Submap"><a href="#Submap" class="headerlink" title="Submap"></a>Submap</h2><p>当Local SLAM接收到一定数量的range data时可认为此时当前的submap被完成了，这个即由<code>TRAJECTORY_BUILDER_nD.submaps.num_range_data</code>参数来指定。Local SLAM的结果在时间上积累后会产生漂移误差，Global SLAM可以来fix这个误差。这个Submap必须足够的小从而可以认为它是局部正确的。但从另一方面来看，他又必须足够的大从而可以做回环。</p><pre><code class="hljs css"><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.submaps</span><span class="hljs-selector-class">.num_range_data</span></code></pre><p>这里总结一下，雷达数据到建图的流程如下：</p><p><img src="assets/CartographerLaserData-1575206288976.png" srcset="/img/loading.gif" alt="CartographerLaserData"></p><p>Submap 可以采用不止一种的数据结构来存储。但是现在大多都是采用的概率栅格地图（probability grids）的方式来存储。但是在2D中，还可以采用TSDF（Truncated Signed Distance Fields）地图类型。</p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D.submaps.grid_options_2d.grid_type</code></pre><p>概率栅格地图的资料很多，去搜一下即可。这里就贴文档原句了。Odds are updated according to “<em>hits</em>” (where the range data is measured) and “<em>misses</em>” (the free space between the sensor and the measured points)。可以根据对被占据occupied和free space的雷达数据的置信度，加减hits和misses的weight值（Both <em>hits</em> and <em>misses</em> can have a different weight in occupancy probability calculations giving more or less trust to occupied or free space measurements）。</p><blockquote><p><em>TODO</em></p><p><em>这里为啥分了两个…是因为不同的地图类型吗</em></p></blockquote><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D.submaps.range_data_inserter.probability_grid_range_data_inserter.hit_probabilityTRAJECTORY_BUILDER_2D.submaps.range_data_inserter.probability_grid_range_data_inserter.miss_probabilityTRAJECTORY_BUILDER_3D.submaps.range_data_inserter.hit_probabilityTRAJECTORY_BUILDER_3D.submaps.range_data_inserter.miss_probability</code></pre><p>官方给的默认配置：</p><p><code>trajectory_builder_2d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D = &#123;  ...  submaps = &#123;    num_range_data = <span class="hljs-number">90</span>,    grid_options_2d = &#123;      grid_type = <span class="hljs-string">"PROBABILITY_GRID"</span>,      resolution = <span class="hljs-number">0.05</span>,    &#125;,    range_data_inserter = &#123;      range_data_inserter_type = <span class="hljs-string">"PROBABILITY_GRID_INSERTER_2D"</span>,      probability_grid_range_data_inserter = &#123;        insert_free_space = <span class="hljs-literal">true</span>,        hit_probability = <span class="hljs-number">0.55</span>,        miss_probability = <span class="hljs-number">0.49</span>,      &#125;,    &#125;,  &#125;,  ...&#125;</code></pre><p>2D SLAM中一个submap仅有一个栅格地图（probability grid）被存储。在3D SLAM中因为scan matching性能的原因，维护了两个<em>hybrid</em> probability grid (the term “hybrid” only refers to an internal tree-like data representation and is abstracted to the user)，并分别应用了一个adaptive_voxel_filter。：</p><ul><li>一个是用于远距离测量的低分辨率 hybrid grid</li><li>另一个是用于近距离测量的高分辨率 hybrid grid</li></ul><p>Scan match 首先将远处的低分辨率的点云与低分辨率 hybrid grid 对齐，然后通过高分辨率的近处点云与高分辨率的hybrid grid对齐来refine pose。</p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D.submaps.grid_options_2d.resolutionTRAJECTORY_BUILDER_3D.submaps.high_resolutionTRAJECTORY_BUILDER_3D.submaps.low_resolutionTRAJECTORY_BUILDER_3D.high_resolution_adaptive_voxel_filter.max_rangeTRAJECTORY_BUILDER_3D.low_resolution_adaptive_voxel_filter.max_range</code></pre><blockquote><p><em>TODO</em></p><p><em>上面的adaptive_voxel_filter.max_range是指的近和远的距离的界定吗</em></p></blockquote><p>Cartographer ROS提供了一个可以在rviz中可视化submaps的插件。可以选择submap通过他们的id。3D SLAM中rviz仅仅显示3D hybrid probability grids的2D 投影（in grayscale）。通过Rviz左侧栏可以切换high resolution hybrid grids来看。</p><p><code>trajectory_builder_3d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_3D = &#123;  ...  submaps = &#123;    high_resolution = <span class="hljs-number">0.10</span>,    high_resolution_max_range = <span class="hljs-number">20.</span>,    low_resolution = <span class="hljs-number">0.45</span>,    num_range_data = <span class="hljs-number">160</span>,    range_data_inserter = &#123;      hit_probability = <span class="hljs-number">0.55</span>,      miss_probability = <span class="hljs-number">0.49</span>,      num_free_space_voxels = <span class="hljs-number">2</span>,    &#125;,  &#125;,  ...&#125;</code></pre><h1 id="Global-SLAM"><a href="#Global-SLAM" class="headerlink" title="Global SLAM"></a>Global SLAM</h1><p>当Local SLAM成功生成submaps的同时，在后端运行着一个全局的优化程序（sparse pose adjustment）。通过调整submap的位置来保持全局一致。然后还会考虑回环优化。</p><p>每当<code>POSE_GRAPH.optimize_every_n_nodes</code>数目的node被插入地图的时候运行一次optimization。</p><p>通常先将<code>POSE_GRAPH.optimize_every_n_nodes</code> 置零来关闭Global SLAM，然后专心的来调Local SLAM。这通常是调试Cartographer的第一步。</p><p>我们把估计出来的一个Scan的绝对位姿称为trajectory上的一个节点(Node)，那么节点与节点的彼此之间的相对位姿就可以称为一个约束(Constraint)。<a href="https://zhuanlan.zhihu.com/p/50055546" target="_blank" rel="noopener">参考链接</a></p><blockquote><p>上面的说法的确比较容易理解，并且做Pose Graph的确仅依赖与位姿。但是通过代码来看，这个Node所带的信息不止有Scan的绝对位姿（gravity aligned，用代码中的注释：Transform to approximately gravity align the tracking frame as determined by local SLAM.），还带有gravity aligned的PointCloud。</p><p>trajectory_node_data.proto文件：</p><pre><code class="hljs protobuf">syntax = <span class="hljs-string">"proto3"</span>;<span class="hljs-keyword">package</span> cartographer.mapping.proto;<span class="hljs-keyword">import</span> <span class="hljs-string">"cartographer/sensor/proto/sensor.proto"</span>;<span class="hljs-keyword">import</span> <span class="hljs-string">"cartographer/transform/proto/transform.proto"</span>;<span class="hljs-comment">// Serialized state of a mapping::TrajectoryNode::Data.</span><span class="hljs-class"><span class="hljs-keyword">message</span> <span class="hljs-title">TrajectoryNodeData</span> </span>&#123;  <span class="hljs-built_in">int64</span> timestamp = <span class="hljs-number">1</span>;  transform.proto.Quaterniond gravity_alignment = <span class="hljs-number">2</span>;  sensor.proto.CompressedPointCloud      filtered_gravity_aligned_point_cloud = <span class="hljs-number">3</span>;  sensor.proto.CompressedPointCloud high_resolution_point_cloud = <span class="hljs-number">4</span>;  sensor.proto.CompressedPointCloud low_resolution_point_cloud = <span class="hljs-number">5</span>;  <span class="hljs-keyword">repeated</span> <span class="hljs-built_in">float</span> rotational_scan_matcher_histogram = <span class="hljs-number">6</span>;  transform.proto.Rigid3d local_pose = <span class="hljs-number">7</span>;&#125;</code></pre><p>cartographer/mapping/trajectory_node.h文件：</p><pre><code class="hljs c++"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">TrajectoryNode</span> &#123;</span>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">Data</span> &#123;</span>    common::Time time;    <span class="hljs-comment">// Transform to approximately gravity align the tracking frame as</span>    <span class="hljs-comment">// determined by local SLAM.</span>    Eigen::Quaterniond gravity_alignment;    <span class="hljs-comment">// Used for loop closure in 2D: voxel filtered returns in the</span>    <span class="hljs-comment">// 'gravity_alignment' frame.</span>    sensor::PointCloud filtered_gravity_aligned_point_cloud;    <span class="hljs-comment">// Used for loop closure in 3D.</span>    sensor::PointCloud high_resolution_point_cloud;    sensor::PointCloud low_resolution_point_cloud;    Eigen::VectorXf rotational_scan_matcher_histogram;    <span class="hljs-comment">// The node pose in the local SLAM frame.</span>    transform::Rigid3d local_pose;  &#125;;  <span class="hljs-function">common::Time <span class="hljs-title">time</span><span class="hljs-params">()</span> <span class="hljs-keyword">const</span> </span>&#123; <span class="hljs-keyword">return</span> constant_data-&gt;time; &#125;  <span class="hljs-comment">// This must be a shared_ptr. If the data is used for visualization while the</span>  <span class="hljs-comment">// node is being trimmed, it must survive until all use finishes.</span>  <span class="hljs-built_in">std</span>::<span class="hljs-built_in">shared_ptr</span>&lt;<span class="hljs-keyword">const</span> Data&gt; constant_data;  <span class="hljs-comment">// The node pose in the global SLAM frame.</span>  transform::Rigid3d global_pose;&#125;;</code></pre></blockquote><p>后端是一个Pose Graph的优化。通过调整nodes和submaps之间的约束关系来优化最终的图。Constraints直觉上来感觉就是由一根根小绳子将所有的nodes给捆起来，pose adjustement就是把这些小绳子给全部接起来。这也就是所谓的Pose Graph。</p><blockquote><p>Rviz中可以对这些Constraints进行可视化，这对调试Global SLAM来说是非常方便的。</p><p>还可以开启<code>POSE_GRAPH.constraint_builder.log_matches</code>来看关于constraints builder的report。</p></blockquote><p>约束分为非全局约束与全局约束：</p><ul><li>非全局约束也被称作(也被称作是 inter submaps constraints)。它在一条trajectory上离得近的 nodes 之间被自动构建。直觉上来看，这些约束使得trajectory的局部结构是一致的。</li><li>全局约束(也被称作是loop closure constraints或者intra submaps contraints)的运行：通常是在一个新的new submap和之前的nodes之间进行搜索，当满足空间上足够的相近（被一个search window限定）以及一个极强的scan match结果。直观的来讲,相当于在两个绳子(约束)之间打一个结点使得两根绳子里的更近。</li></ul><pre><code class="hljs lua">POSE_GRAPH.constraint_builder.max_constraint_distancePOSE_GRAPH.fast_correlative_scan_matcher.linear_search_windowPOSE_GRAPH.fast_correlative_scan_matcher_3d.linear_xy_search_windowPOSE_GRAPH.fast_correlative_scan_matcher_3d.linear_z_search_windowPOSE_GRAPH.fast_correlative_scan_matcher*.angular_search_window</code></pre><p>官方默认配置：</p><p><code>pose_graph.lua</code></p><pre><code class="hljs lua">POSE_GRAPH = &#123;  ...  constraint_builder = &#123;    max_constraint_distance = <span class="hljs-number">15.</span>,    ...    fast_correlative_scan_matcher = &#123;      linear_search_window = <span class="hljs-number">7.</span>,      angular_search_window = <span class="hljs-built_in">math</span>.<span class="hljs-built_in">rad</span>(<span class="hljs-number">30.</span>),      ...    &#125;,    fast_correlative_scan_matcher_3d = &#123;      ...      linear_xy_search_window = <span class="hljs-number">5.</span>,      linear_z_search_window = <span class="hljs-number">1.</span>,      angular_search_window = <span class="hljs-built_in">math</span>.<span class="hljs-built_in">rad</span>(<span class="hljs-number">15.</span>),      ...    &#125;,  &#125;  ...&#125;</code></pre><blockquote><p>// TODO ICRA RM两台机器人，这一点可以考虑</p><p>Cartographer不止可以单单在单个轨迹上loop closure，还可以在多个机器人的多个轨迹上align。这个部分的文档 the parameters related to “global localization” out of the scope of this document.</p></blockquote><p>为了限制constraints的数量（也是降低计算力），Cartographer对这些node做了一个下采样，通过<code>POSE_GRAPH.constraint_builder.sampling_ratio</code>参数来控制。下采样过度会导致约束缺失以及不容易进行loop closure，下采样力度过小会导致Global SLAM的运行速度过慢以及不能实时的loop closure。</p><p>官方默认配置：</p><p><code>pose_graph.lua</code></p><pre><code class="hljs lua">POSE_GRAPH = &#123;  ...  constraint_builder = &#123;    sampling_ratio = <span class="hljs-number">0.3</span>,    ...  &#125;  ...&#125;</code></pre><p>当一个node和一个submap进行constraint building的时候，通过的是一个叫做FastCorrelativeScanMatcher的机制。这个scan matcher是Cartographer的独创并且使得real-time loop closures scan matching成为可能。它引入了分支界限法（Branch and bound），可以在不同分辨率的地图网格上进行工作并且非常有效的去除误匹配。关于这个在Cartographer的论文中被详细描述了。所用的搜索树的深度是可以被控制的。</p><pre><code class="hljs lua">POSE_GRAPH.constraint_builder.fast_correlative_scan_matcher.branch_and_bound_depthPOSE_GRAPH.constraint_builder.fast_correlative_scan_matcher_3d.branch_and_bound_depthPOSE_GRAPH.constraint_builder.fast_correlative_scan_matcher_3d.full_resolution_depth</code></pre><p>官方默认配置：</p><p><code>pose_graph.lua</code></p><pre><code class="hljs lua">POSE_GRAPH = &#123;  ...  constraint_builder = &#123;    max_constraint_distance = <span class="hljs-number">15.</span>,    ...    fast_correlative_scan_matcher = &#123;      ...      branch_and_bound_depth = <span class="hljs-number">7</span>,    &#125;,    fast_correlative_scan_matcher_3d = &#123;      ...      branch_and_bound_depth = <span class="hljs-number">8</span>,      full_resolution_depth = <span class="hljs-number">3</span>,      ...    &#125;,  &#125;  ...&#125;</code></pre><p>一旦FastCorrelativeScanMatcher达到一定的效果的时候（大于<code>POSE_GRAPH.constraint_builder.min_score</code>参数值），会把它再继续扔到CeresScanMatcher中来进行refine。</p><pre><code class="hljs lua">POSE_GRAPH.constraint_builder.min_scorePOSE_GRAPH.constraint_builder.ceres_scan_matcher_3dPOSE_GRAPH.constraint_builder.ceres_scan_matcher</code></pre><p>官方默认配置：</p><p><code>pose_graph.lua</code></p><pre><code class="hljs lua">POSE_GRAPH = &#123;  ...  constraint_builder = &#123;    ...    min_score = <span class="hljs-number">0.55</span>,    ...    ceres_scan_matcher = &#123;      occupied_space_weight = <span class="hljs-number">20.</span>,      translation_weight = <span class="hljs-number">10.</span>,      rotation_weight = <span class="hljs-number">1.</span>,      ceres_solver_options = &#123;        use_nonmonotonic_steps = <span class="hljs-literal">true</span>,        max_num_iterations = <span class="hljs-number">10</span>,        num_threads = <span class="hljs-number">1</span>,      &#125;,    &#125;,    ceres_scan_matcher_3d = &#123;      occupied_space_weight_0 = <span class="hljs-number">5.</span>,      occupied_space_weight_1 = <span class="hljs-number">30.</span>,      translation_weight = <span class="hljs-number">10.</span>,      rotation_weight = <span class="hljs-number">1.</span>,      only_optimize_yaw = <span class="hljs-literal">false</span>,      ceres_solver_options = &#123;        use_nonmonotonic_steps = <span class="hljs-literal">false</span>,        max_num_iterations = <span class="hljs-number">10</span>,        num_threads = <span class="hljs-number">1</span>,      &#125;,    &#125;,  &#125;,  ...&#125;</code></pre><h3 id="The-Optimization-Problem"><a href="#The-Optimization-Problem" class="headerlink" title="The Optimization Problem"></a>The Optimization Problem</h3><p>当 Cartographer 运行优化问题的时候, Cartographer会通过多个残差项来对submaps进行调整。每一项残差通过被加权的cost function来计算（和SLAM中常见的做法一样）。这些每一个cost function取自多个数据源，全局(回环)约束<code>global (loop closure) constraints</code>，非全局(scan match)的约束<code>the non-global (matcher) constraints</code>，IMU 的测量值<code>IMU acceleration and rotation measurements</code>，local SLAM 的粗略的 pose 估计<code>local SLAM rough pose estimations</code>，外部的odometry 信息或者GPS等<code>an odometry source or a fixed frame (such as a GPS system)</code>。可通过下面项来进行配置：</p><pre><code class="hljs lua">POSE_GRAPH.constraint_builder.loop_closure_translation_weightPOSE_GRAPH.constraint_builder.loop_closure_rotation_weightPOSE_GRAPH.matcher_translation_weightPOSE_GRAPH.matcher_rotation_weightPOSE_GRAPH.optimization_problem.*_weightPOSE_GRAPH.optimization_problem.ceres_solver_options</code></pre><blockquote><p>TODO ?</p><p>One can find useful information about the residuals used in the optimization problem by toggling <code>POSE_GRAPH.max_num_final_iterations</code></p></blockquote><p>官方默认配置：</p><p><code>pose_graph.lua</code></p><pre><code class="hljs lua">POSE_GRAPH = &#123;  ...  constraint_builder = &#123;    ...    loop_closure_translation_weight = <span class="hljs-number">1.1e4</span>,    loop_closure_rotation_weight = <span class="hljs-number">1e5</span>,    ...  &#125;,  ...  matcher_translation_weight = <span class="hljs-number">5e2</span>,  matcher_rotation_weight = <span class="hljs-number">1.6e3</span>,  optimization_problem = &#123;    huber_scale = <span class="hljs-number">1e1</span>,    acceleration_weight = <span class="hljs-number">1e3</span>,    rotation_weight = <span class="hljs-number">3e5</span>,    local_slam_pose_translation_weight = <span class="hljs-number">1e5</span>,    local_slam_pose_rotation_weight = <span class="hljs-number">1e5</span>,    odometry_translation_weight = <span class="hljs-number">1e5</span>,    odometry_rotation_weight = <span class="hljs-number">1e5</span>,    fixed_frame_pose_translation_weight = <span class="hljs-number">1e1</span>,    fixed_frame_pose_rotation_weight = <span class="hljs-number">1e2</span>,    log_solver_summary = <span class="hljs-literal">false</span>,    ceres_solver_options = &#123;      use_nonmonotonic_steps = <span class="hljs-literal">false</span>,      max_num_iterations = <span class="hljs-number">50</span>,      num_threads = <span class="hljs-number">7</span>,    &#125;,  &#125;,&#125;</code></pre><p>在代码中，各种cost function的定义文件路径：</p><div align="left">    <img src="assets/Screenshot from 2019-12-07 21-31-20.png" srcset="/img/loading.gif" style="zoom:100%;" /></div><p>Cartographer提供了如下的cost functions：</p><div align="left">    <img src="assets/Screenshot from 2019-12-07 21-31-48.png" srcset="/img/loading.gif" style="zoom:100%;" /></div><h2 id="全局优化时对于IMU数据的处理"><a href="#全局优化时对于IMU数据的处理" class="headerlink" title="全局优化时对于IMU数据的处理"></a>全局优化时对于IMU数据的处理</h2><p>Global optimization 对 imu 的 pose 信息提供了更多的灵活性。默认的 Ceres 会优化你 IMU 和 tracking frame 之间的外参。如果你不信任你的 imu 的数据的话, Ceres’ global optimization 的结果可以被记录然后来用来优化它俩之间的外参矩阵。如果 Ceres 不能够很好的优化 IMU 的 pose (它俩之间的外参矩阵)或者你非常信任你校准的它俩之间的外参矩阵的话，可以当做常量来使用自己标定的外参矩阵。</p><pre><code class="hljs lua">POSE_GRAPH.optimization_problem.log_solver_summaryPOSE_GRAPH.optimization_problem.use_online_imu_extrinsics_in_3d</code></pre><h2 id="Huber-loss"><a href="#Huber-loss" class="headerlink" title="Huber loss"></a>Huber loss</h2><p>在residuals中，采用Huber loss function 从而控制outliers的影响。Huber loss function的Huber scale是定的，通过下面参数指定。</p><pre><code class="hljs lua">POSE_GRAPH.optimization_problem.huber_scale</code></pre><p>这个值选的越大，潜在的outliers对系统的影响可能就越大。</p><p>关于Huber loss <a href="https://blog.csdn.net/qq_29981283/article/details/83042231" target="_blank" rel="noopener">博客1</a> <a href="https://blog.csdn.net/lj6052317/article/details/87885658" target="_blank" rel="noopener">博客2</a>：</p><p>huber loss 是一种优化平方loss的一种方式，使得loss变化没有那么大。</p><p><img src="assets/20190222210736618.png" srcset="/img/loading.gif" alt="img"></p><p><img src="assets/20190222210820112.png" srcset="/img/loading.gif" alt="img"></p><h2 id="最终的全局优化"><a href="#最终的全局优化" class="headerlink" title="最终的全局优化"></a>最终的全局优化</h2><p>一旦 当 trajectory 完成后，Cartographer 经常会运行一个新的全局优化，迭代的次数通常比之前的要多得多。这样做的原因是要尽可能的去优化最终建图的效果，并且通常并没有实时性的要求。所以选择大量的迭代是正确的选择.</p><pre><code class="hljs lua">POSE_GRAPH.max_num_final_iterations</code></pre><h5 id="附-PointCloud2消息类型的数据格式"><a href="#附-PointCloud2消息类型的数据格式" class="headerlink" title="附: PointCloud2消息类型的数据格式"></a>附: PointCloud2消息类型的数据格式</h5><p><a href="http://docs.ros.org/api/sensor_msgs/html/msg/PointCloud2.html" target="_blank" rel="noopener">官方点云数据格式</a></p><p><a href="https://blog.csdn.net/Fourier_Legend/article/details/83656798" target="_blank" rel="noopener">点云数据格式解释</a></p><p>手动解析点云数据时不可以直接解析，因为在field里是以二进制方式存储的，可以通过<a href="https://answers.ros.org/question/273182/trying-to-understand-pointcloud2-msg/" target="_blank" rel="noopener">ros包里的工具来进行解析</a></p>]]></content>
    
    
    <categories>
      
      <category>SLAM原理与实践</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Robotics</tag>
      
      <tag>SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读 - DroNet: Deep learning 在无人机导航中的应用</title>
    <link href="/2019/06/08/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-Deep-Learning%E5%9C%A8%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%AF%BC%E8%88%AA%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <url>/2019/06/08/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-Deep-Learning%E5%9C%A8%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%AF%BC%E8%88%AA%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>DroNet: Learning to Fly by Driving 提出了一个结构非常简单但是又非常强大的网络结构，可以通过输入的每帧图像输出当前飞行器 yaw 的目标角度值与前方障碍物的概率值，从而可以利用这两个信息推断出飞行器当前运动时的 yaw 应转角度 $\theta_k$ 与前进飞行速度 $v_k$，从而达到自主导航的目的。</p><a id="more"></a><h1 id="DroNet-Learning-to-Fly-by-Driving"><a href="#DroNet-Learning-to-Fly-by-Driving" class="headerlink" title="DroNet: Learning to Fly by Driving"></a>DroNet: Learning to Fly by Driving</h1><blockquote><p>A. Loquercio, A.I. Maqueda, C.R. Del Blanco, D. Scaramuzza<br>DroNet: Learning to Fly by Driving,<br>IEEE Robotics and Automation Letters (RA-L), 2018.<br><a href="http://rpg.ifi.uzh.ch/docs/RAL18_Loquercio.pdf" target="_blank" rel="noopener">PDF</a> <a href="https://youtu.be/ow7aw9H4BcA" target="_blank" rel="noopener">YouTube</a> <a href="https://github.com/uzh-rpg/rpg_public_dronet" target="_blank" rel="noopener">Software and Datasets</a>.</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>DroNet 是一个结构非常简单但是又非常强大的网络结构，可以通过输入的每帧图像输出当前飞行器 yaw 的目标角度值与前方障碍物的概率值，从而可以利用这两个信息推断出飞行器当前运动时的 yaw 应转角度 $\theta_k$ 与前进飞行速度 $v_k$，从而达到自主导航的目的。对比论文发布当时的其他相关网络模型，达到了最好的准确度与处理速度的平衡。</p><p><img src="/assets/1560001100081.png" srcset="/img/loading.gif" alt="1560001100081"></p><p>该系统在非机载处理资源上运行 （Intel Core i7 2.6 GHz CPU）上可以达到 30Hz 的控制指令输出。可想如果机载选择 TX2 此类处理器, 采用 GPU 进行推理计算的话, 速度会更快。 </p><p>该模型的训练数据采用室外场景下在地面交通工具上采集的数据集，比如自行车, 汽车等在城市环境内第一视角的图像与其他数据。实验结果惊奇的发现该方法不仅在室外不同视角下表现极好（5m飞行高度），在室内场景中也有极强的泛化能力。飞机可以在没有先验信息的环境中也可以有一个非常好的导航效果。</p><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><p>文章以“无人机应该像其他地面交通工具一样, 在 roadway 中有相同的 behavior”为出发点，通过来自于地面交通工具的数据集，做了以下主要工作：</p><ul><li>提出了一种 residual convolutional architecture (DroNet)，可以预测飞机要转的偏航角与前方发生与障碍物碰撞的概率，可以为飞机在城市环境中提供安全的飞行。通过来自于室外场景下汽车，自行车的数据集来训练该网络。</li><li>建立了一个关于预测是前方障碍物概率的数据集。</li><li>可以到到很好的 performance 和 real-time。</li><li>通过一些扩展场景的检验，发现该系统的泛化能力极强，可以在没有任何先验信息的环境中正常运行，包括数据集中没有的室内走廊场景，一个高高度（视角不一样）等场景.</li></ul><p>文章作者还提到该方法并不是为了替代传统的 map-localize-plan 方法, 作者认为将来有一天传统方法与基于深度学习的方法会互补.</p><h2 id="关于网络与训练方法"><a href="#关于网络与训练方法" class="headerlink" title="关于网络与训练方法"></a>关于网络与训练方法</h2><p><strong>网络的结构图如下:</strong></p><p><img src="/assets/Screenshot from 2019-06-08 19-56-55.png" srcset="/img/loading.gif" alt=""></p><p>输出的转角与障碍物信息两个功能在前面共用同一个网络结构（共享同一套参数）。输入的图像是一张200<em>200</em>1 的灰度图，通过一个 5*5 的卷积核降维后，通过三层 res block，然后经过 dropout（作者实验时设的是0.5）后再分叉,  通过 Relu 后作用于两个全连接层（节点数为 1）分别输出信息（大小范围均在0-1之间）。关于为何有这样的网络设计的想法，作者并未多提。</p><blockquote><p>通过代码看到最终 collision 这个全连接节点最终输出还被作用了一个 sigmod 函数，这个论文中并未提到为何。猜测是用于归一化（概率值必须大于 0 小于 1，而刚开始训练的时候 loss 大多依据（后面会讲到） steering 的 loss，可能会出现 [-1, 0] 之间的数，所以要归一化到 (0,1)） / 某种意义上的数据增强？具体只能等跑训练代码的时候看一下实际该参数的值。</p><p>cnn_models.py</p><pre><code class="hljs python"><span class="hljs-number">85</span>    <span class="hljs-comment"># Collision channel</span><span class="hljs-number">86</span>    coll = Dense(output_dim)(x)<span class="hljs-number">87</span>    coll = Activation(<span class="hljs-string">'sigmoid'</span>)(coll)</code></pre></blockquote><p><strong>关于训练方法:</strong></p><p>关于转角的预测本质是一个回归问题，关于障碍物的检测本质是一个二分类问题（虽然最后输出的是概率, 但是从数据集本质来看是一个二分类问题，这个后续详述）。该网络比较特殊（两种不同的问题模型的输出，共享网络），所以要设计出一种合理的 loss 函数. </p><p>根据两类问题的本质，转角预测本质为回归问题所以采用均方误差（MSE）衡量 loss；障碍物概率预测本质为二分类问题所以采用二值交叉熵（BCE）来衡量学习到的分布与样本真实分布的差异，作为 loss。但是整个网络不能简单使用两个 loss 叠加来来作为最终的 loss，会导致特别差的收敛结果，因为回归问题和分类问题在模型刚开始训练的时候，梯度大小差异非常大<a href="https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf" target="_blank" rel="noopener">参考文献</a>。</p><p>实际中，回归问题的梯度在刚开始的时候会非常大，MSE 的梯度正比于转角的误差值。所以策略就是刚开始的时候几乎只选择用转角的 loss，后面随着 epoch 的增加慢慢增大障碍物概率检测的 loss 的权重，等到两者 loss 在一个数量级可比的时候，optimizer 就会自动为两者找到一个很好的 solution。该方式的解释也在上一个参考文献链接里，不设权重或者权重恒定的方法都会导致不好的结果或者收敛时间过长，所以依据此作者提出了下面的 loss：</p><script type="math/tex; mode=display">L_{t o t}=L_{M S E}+\max \left(0,1-\exp ^{-d e c a y\left(e p o c h-e p o c h_{0}\right)}\right) L_{B C E}</script><p>该方式就可以达到上面所说的期望的训练过程中的 loss 函数变化情况。作者在实验时选择 $decay=\frac{1}{10}$,  $epoch_{0}=10$。</p><p>optimizer 选择 Adam，初始学习率设为0.001，decay=1e-5。</p><p>最后作者为 optimization 还采用了 hard negative mining 来建立负样本集, 在每一个 epoch 中选择 loss 最高的 k 个样本, 采用上面计算 loss 的式子计算整体 loss。k 会随着时间会减小。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>关于转角预测（Steering angle）采用来自 Udacity’s project 的公共数据集，该数据集是基于汽车拍摄的。里面有三个摄像头以及 imu，gps，steering angle 等其他同步的数据，作者只选用前置摄像头与 steering angle 作为模型训练时所采用的数据。</p><p>关于用于计算障碍物概率的数据集，由于没有合适的数据集，作者们自制了一套相关数据集，137 个场景序列中包含了 32000 张图片。根据视野障碍物是否离得特别近来标注 0（无碰撞风险）和 1（有碰撞风险）。例图如下，绿色表示无碰撞风险，红色表示有碰撞风险。</p><p><img src="/assets/1560001239435.png" srcset="/img/loading.gif" alt="1560001239435"></p><h2 id="飞机运动控制方法"><a href="#飞机运动控制方法" class="headerlink" title="飞机运动控制方法"></a>飞机运动控制方法</h2><p>整体的导航思路很简单，飞机一直在同一高度飞行，只控制飞机的两个自由度，机体坐标系下前进的速度 $v_k$ 和世界坐标系下的 yaw 值 $\theta_k$.</p><ul><li><p>根据网络输出的前方发生与障碍物碰撞的概率 $p_t$ 计算前进的速度 $v_k$:</p><script type="math/tex; mode=display">v_{k}=(1-\alpha) v_{k-1}+\alpha\left(1-p_{t}\right) V_{\max }</script><p>公式很简单, 即前方发生碰撞概率越大, 前进的速度越低, 发生碰撞概率为 1 的时候速度为0.然后加了低通滤波使速度输出更平滑($0&lt;\alpha&lt;1$). </p></li><li><p>根据网络输出的 steering angle 换算成实际要转的偏航角大小。网络输出的范围为 [-1, 1]，换算成$\left[-\frac{\pi}{2}, \frac{\pi}{2}\right]$。然后也同理加了个低通滤波：</p><script type="math/tex; mode=display">\theta_{k}=(1-\beta) \theta_{k-1}+\beta \frac{\pi}{2} s_{k}</script></li></ul><p>然后根据这两个值赋给飞机就可以控制飞机运动了。作者在试验中选择的 $\alpha=0.7$ 和 $\beta=0.5$。$V_{\max }$根据实验场景不同选择合适的值即可。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>作者在大量场景中进行了测试，具体的测试结果这里不在赘述。</p><p>DroNet 模型是一个平衡结果准确性与运算速度的最佳的模型。</p>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep-Learning</tag>
      
      <tag>Navigation</tag>
      
      <tag>UAV</tag>
      
      <tag>Robotics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NVIDIA Jetson Xavier 通过 USB Ethernet 实现 ssh 功能与高带宽进行 sftp 文件传输</title>
    <link href="/2019/05/08/NVIDIA-Jetson-Xavier-%E9%80%9A%E8%BF%87-USB-Ethernet-%E5%AE%9E%E7%8E%B0-ssh-%E5%8A%9F%E8%83%BD%E4%B8%8E%E9%AB%98%E5%B8%A6%E5%AE%BD%E8%BF%9B%E8%A1%8C-sftp-%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93/"/>
    <url>/2019/05/08/NVIDIA-Jetson-Xavier-%E9%80%9A%E8%BF%87-USB-Ethernet-%E5%AE%9E%E7%8E%B0-ssh-%E5%8A%9F%E8%83%BD%E4%B8%8E%E9%AB%98%E5%B8%A6%E5%AE%BD%E8%BF%9B%E8%A1%8C-sftp-%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93/</url>
    
    <content type="html"><![CDATA[<p>通过阅读 Xavier 刷机后提供的文档，发现它的背面的 type-C 的 usb 口可以实现 ssh 与 sftp 功能，hosts可以通过链接桥连接的方式来与 Xavier 建立链接。</p><a id="more"></a><ol><li>首先当然通过那个 type-C 的口把 Xavier 和你的电脑连起来.</li><li>然后你会看到你的 Network Manager中就会多出来两个”网卡”, 然后在 NVIDIA 的那个下面建立网络.类型选择正常的 Ethernet 即可. 配置DHCP(自动配置ip) 和手动都可以,手动配置的时候注意一下配置好 netmask 和 gateway. Xavier那边的 ip 是192.168.55.1, 觉得手动麻烦的话最好 DHCP 方式即可.建立好连接后就可以用 Xavier 的192.168.55.1这个 ip 即可.ssh 和 scp 都可以正常用.(scp时的带宽是真的大…瞬间几十M甚至可能上百M)</li></ol><p>发现的bug:</p><p>连着这个 usb 网络时不能正常 ssh 登录其他 Jetson 设备…比如登另一块 TX2 的时候就一直 Permission denied…文档里好像说这个问题了,解决办法待更新.</p>]]></content>
    
    
    <categories>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Onboard-Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模式识别入门（一） - 模式识别概论</title>
    <link href="/2019/05/02/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%85%A5%E9%97%A8-%E4%B8%80-%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E6%A6%82%E8%AE%BA/"/>
    <url>/2019/05/02/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%85%A5%E9%97%A8-%E4%B8%80-%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E6%A6%82%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<p>本学期选修了学校自动化学院的张绍武教授《模式识别》课程。此为期末总结时候的复习总结。此篇介绍了模式识别入门的一些常识与概念。（由于期末复习来不及了……后面停更了）</p><a id="more"></a><h1 id="Chapter-1-概论"><a href="#Chapter-1-概论" class="headerlink" title="Chapter 1 概论"></a>Chapter 1 概论</h1><h2 id="1-1-模式识别基本概念"><a href="#1-1-模式识别基本概念" class="headerlink" title="1.1 模式识别基本概念"></a>1.1 模式识别基本概念</h2><h3 id="1-1-1-模式识别的基本定义"><a href="#1-1-1-模式识别的基本定义" class="headerlink" title="1.1.1 模式识别的基本定义"></a>1.1.1 模式识别的基本定义</h3><ul><li><p>样本 (sample, object)</p><p>一类事物的一个具体体现, 对具体的个别事物进行观测所得到的某种形式的信号. 一般用向量表示.</p></li><li><p>模式 (pattern)</p><p>表示一类事物.</p></li></ul><blockquote><p>样本是具体的事物,而模式是对同一类事物概念性的概括</p></blockquote><ul><li><p>模式类与模式联合使用时,模式表示具体的事物,而模式类则是对这一类事物的概念性描述</p><p>模式与模式类的例子:</p><ul><li>模式类: 老年人; 模式: 王老太, 老头, 老太</li><li>模式类: 老头; 模式: 王老头</li><li>模式类: 老太; 模式: 王老太</li></ul></li></ul><p><strong>模式识别</strong> (Pattern Recognition)</p><p>主要研究相似与分类的问题, 用计算机实现对人和各种事物或者现象的分析, 描述, 判断与识别.</p><p><strong><em>&gt; Pattern recognition:</em></strong></p><blockquote><p>is the study of how machines can observe the environment, learn to distinguish patterns of interest from their background, and make sound and reasonable decisions about the categories of the patterns. (Anil K. Jain)</p></blockquote><p><strong>模式识别的作用与目的</strong></p><p>将某一具体事物正确的归入某一类别. 也就是从样本到类别的映射.</p><p><img src="/assets/Screenshot from 2019-05-02 12-42-57.png" srcset="/img/loading.gif" alt=""></p><h3 id="1-1-2-模式识别的发展史"><a href="#1-1-2-模式识别的发展史" class="headerlink" title="1.1.2 模式识别的发展史"></a>1.1.2 模式识别的发展史</h3><ul><li>1929年 G. Tauschek发明阅读机, 能够阅读0-9的数字</li><li>30年代 Fisher提出统计分类理论, 奠定了统计模式识别的基础. 因此, 在60~70年代, 统计模式识别发展很快, 但由于被识别的模式愈来愈复杂, 特征也愈多, 就出现“维数灾难”. 随着计算机运算速度的迅猛发展, 这个问题得到一定解决. 统计模式识别仍是模式识别的主要理论</li><li>50年代 Noam Chemsky 提出形式语言理论, 美籍华人付京荪提出句法结构模式识别</li><li>60年代 L.A. Zadeh提出了模糊集理论,模糊模式识别理论得到了较广泛的应用</li><li>80年代 Hopfield提出神经元网络模型理论.人工神经元网络在模式识别和人工智能上得到较广泛的应用</li><li>90年代 V.N. Vapnik 提出了小样本学习理论,支持向量机也受到了很大的重视</li><li>2012年后, Deep Learning.</li></ul><h3 id="1-1-3-关于模式识别的国内-国际学术组织"><a href="#1-1-3-关于模式识别的国内-国际学术组织" class="headerlink" title="1.1.3 关于模式识别的国内, 国际学术组织"></a>1.1.3 关于模式识别的国内, 国际学术组织</h3><ul><li><p>1973年 IEEE发起了第一次关于模式识别的国际会议“ICPR”,成立了国际模式识别协会—“IAPR”,每2年召开一次国际学术会议</p></li><li><p>1977年 IEEE的计算机学会成立了模式分析与机器智能(PAMI)委员会,每2年召开一次模式识别与图象处理学术会议</p></li><li><p>CVPR, PRCV等</p></li><li><p>电子学会, 通信学会, 自动化学会, 中文信息学会</p><p>(TODO)</p></li></ul><h2 id="1-2-模式识别系统"><a href="#1-2-模式识别系统" class="headerlink" title="1.2 模式识别系统"></a>1.2 模式识别系统</h2><p>执行模式识别的计算机系统称为模式识别系统. 该系统被用来执行模式分类的具体任务.</p><p>一个典型的模式识别系统如下图所示结构框图组成</p><p><img src="/assets/Screenshot from 2019-05-02 12-55-05.png" srcset="/img/loading.gif" alt=""></p><p>一般由<strong>数据获取, 预处理, 特征提取选择, 分类器设计及分类决策</strong>五部分组成.分类器设计在<strong>训练过程</strong>中完成, 利用样本进行训练, 确定分类器的具体参数.而分类决策在<strong>识别过程</strong>中起作用, 对待识别的样本进行分类决策.</p><p><img src="/assets/Screenshot from 2019-05-02 14-19-23.png" srcset="/img/loading.gif" alt=""></p><p><strong>流程样例:</strong></p><p><img src="/assets/Screenshot from 2019-05-02 14-24-49.png" srcset="/img/loading.gif" alt=""></p><p><strong>说明:</strong></p><ul><li>这一系统构造适合于统计模式识别, 模糊模式识别, 人工神经网络中有监督方法</li><li>对于结构模式识别方法,只需用基元提取代替特征提取与选择</li><li>对于聚类分析, <strong>分类器设计与决策合二为一</strong>, 一步完成</li></ul><p><strong>步骤:</strong></p><ul><li><p><strong>信息获取</strong> (data acquisition)</p></li><li><p><strong>预处理</strong> (preprocessing)</p><p>目的: 去除所获取信息中的噪声,增强有用的信息,及一切必要的使信息纯化的处理过程</p><p>常用技术: A\D ,二值化,一维信号滤波去噪,及图象的平滑、变换、增强、恢复、滤波等处理</p></li><li><p><strong>特征选择与提取</strong> (feature selection and extraction)</p><p>一般说来它包括将所获取的原始量测数据转换成能反映事物本质,并将其最有效分类的特征表示</p><p>样本及模式都是用特征来描述,识别与训练在<strong>特征空间</strong>中进行.量测仪器或传感器获取的原始数据组成的空间叫<strong>测量空间</strong>.</p><blockquote><p>特征选择与提取模块的功能是对所获取的信息实现<strong>从测量<br>空间到特征空间的转换</strong></p></blockquote><p>找到合适的特征描述对识别的可靠性,计算复杂度、有效性都是十分重要的</p></li><li><p><strong>分类器设计</strong> (classifier design)</p><p>分类器设计的主要功能是通过训练确定判决规则,按此类判决规则分类时,错误率最低. 把这些判决规则建成标准库.</p></li><li><p><strong>分类决策</strong> (classification decision)</p><p>在特征空间中对被识别的对象进行分类. 分界线的类型可由设计者直接确定,也可通过训练过程产生,但是这些分界线的具体参数则利用训练样本<strong>经训练过程确定</strong></p><ul><li><p>分类决策是对事物辨识的最后一步,其主要方法是计算待识别事物的属性,分析它是否满足是某类事物的条件</p></li><li><p>对于每个事物来说,由它的属性得到它的描述,表示成相应的特征向量,它在特征空间中表示成一个点,称为数据点</p></li><li><p>特征空间的分布中往往表现出同类事物的特征向量聚集在一起,聚集在一个相对集中的区域,而不同事物则分别占据不同的区域</p></li><li><p>待识别的事物,如果它的特征向量出现在某一类事物经常出现或可能出现的区域内,该事物就被识别为该类事物</p></li><li>在特征空间中,哪个区域是某类事物典型所在区域,需要用数学式子划定,这样一来,满足这种数学式子与否就成为分类决策的依据</li><li>如何<strong>确定这些数学式子</strong>就是分类器设计的任务,而一旦这种数学<br>式子确定后,分类决策的方法也就确定了</li></ul></li></ul><h2 id="1-3-模式识别方法"><a href="#1-3-模式识别方法" class="headerlink" title="1.3 模式识别方法"></a>1.3 模式识别方法</h2><ul><li><p>模板匹配方法 (templete matching)</p></li><li><p>统计方法 (statistical pattern recognition)</p></li><li><p>神经网络方法 (neural network)</p></li><li><p>结构方法 (句法方法) (structural pattern recognition)</p></li><li><p>模糊模式识别 (Fuzzy pattern recognition)</p></li><li><p>逻辑推理方法 (Logic inference)</p><p>(TODO 详细的后面补充)</p></li></ul><h2 id="1-4-模式识别应用"><a href="#1-4-模式识别应用" class="headerlink" title="1.4 模式识别应用"></a>1.4 模式识别应用</h2><p>略</p><h2 id="1-5-模式识别基本问题"><a href="#1-5-模式识别基本问题" class="headerlink" title="1.5 模式识别基本问题"></a>1.5 模式识别基本问题</h2><h3 id="1-5-1-模式-样本-表示方法"><a href="#1-5-1-模式-样本-表示方法" class="headerlink" title="1.5.1 模式(样本)表示方法"></a>1.5.1 模式(样本)表示方法</h3><ol><li><p>向量表示: 假设一个样本有n个变量(特征)</p><script type="math/tex; mode=display">X=\left[\mathrm{x}_{1}, \mathrm{x}_{2}, \ldots, \mathrm{x}_{\mathrm{n}}\right]^{\top}</script></li><li><p>矩阵表示</p></li><li>几何表示</li></ol><h3 id="1-5-2-模式类的紧致性"><a href="#1-5-2-模式类的紧致性" class="headerlink" title="1.5.2 模式类的紧致性"></a>1.5.2 模式类的紧致性</h3><ol><li><p>紧致集: 同一类模式类样本的分布比较集中, 没有或者临界样本很少, 这样的模式类称为<strong>紧致集</strong></p><p><img src="/assets/Screenshot from 2019-05-02 20-47-24.png" srcset="/img/loading.gif" alt=""></p></li><li><p>临界点(样本): 在多类样本中, 某些样本值有微小的变化时就变成了另一类样本称为<strong>临界样本(点)</strong></p></li><li><p>紧致集的性质:</p><ol><li>与样本总数相比, 临界点的数量非常少</li><li>集合内任意两点可以用光滑线连线, 在线上的点属于同一集合</li><li>集合内的每一个点都有足够强大的领域, 在邻域内包含同一集合的点</li></ol></li><li><p>模式识别的要求:</p><ol><li>满足紧致集, 才能很好的分类</li><li>如果不能满足紧致集, 就要采取变换的方法来使其满足紧致集.</li></ol><blockquote><p>只要各个模式类是可分的, 总存在这样一个空间, 使变换到这个空间中的集合满足紧致集的要求</p></blockquote></li></ol><h3 id="1-5-3-相似与分类"><a href="#1-5-3-相似与分类" class="headerlink" title="1.5.3 相似与分类"></a>1.5.3 相似与分类</h3><p>两个样本$X<em>{i}, X</em>{j}$之间的相似度量满足以下要求:</p><ol><li>应该为非负值</li><li>样本本身相似性度量应该最大</li><li>度量应满足对称性</li><li>在满足紧致性的条件下, 相似性应该是点间距离的单调函数</li></ol><h4 id="1-5-3-1-用各种距离表示相似性"><a href="#1-5-3-1-用各种距离表示相似性" class="headerlink" title="1.5.3.1 用各种距离表示相似性"></a>1.5.3.1 用各种距离表示相似性</h4><p>已知两个样本:</p><script type="math/tex; mode=display">X_{\mathrm{i}}=\left[\mathrm{x}_{\mathrm{i} 1}, \mathrm{x}_{\mathrm{i} 2}, \mathrm{x}_{\mathrm{i} 3}, \ldots, \mathrm{x}_{\mathrm{in}}\right]^{\top}</script><script type="math/tex; mode=display">X_{\mathrm{j}}=\left[\mathrm{x}_{\mathrm{j} 1}, \mathrm{x}_{\mathrm{j} 2}, \mathrm{x}_{\mathrm{j} 3}, \ldots, \mathrm{x}_{\mathrm{jn}}\right]^{\top}</script><ol><li><p>绝对值距离:</p><script type="math/tex; mode=display">d_{i j}=\sum_{k=1}^{n}\left|X_{i k}-X_{j k}\right|</script></li><li><p>欧几里得距离:</p><script type="math/tex; mode=display">d_{i j}=\sqrt{\sum_{k=1}^{n}\left(X_{i k}-X_{j k}\right)^{2}}</script></li><li><p><strong>明考夫斯基距离</strong>:</p><script type="math/tex; mode=display">d_{i j}(\boldsymbol{q})=\left(\sum_{k=1}^{n}\left|\boldsymbol{X}_{i k}-\boldsymbol{X}_{j k}\right|^{q}\right)^{\mathbf{1} / \boldsymbol{q}}</script><p>其中当q=1时为绝对值距离,当q=2时为欧氏距离</p></li><li><p>切比雪夫距离:</p><script type="math/tex; mode=display">d_{i j}(\infty)=\max _{1 \leq k \leq n}\left|X_{i k}-X_{j k}\right|</script><p>切氏距离为q趋向于无穷大时的明氏距离的极限情况</p></li><li><p><strong>马哈拉诺比斯距离</strong> (Mahalanobis diatance):</p><script type="math/tex; mode=display">d_{i j}(M)=\sqrt{\left(X_{i}-X_{j}\right)^{T} \Sigma^{-1}\left(X_{i}-X_{j}\right)}</script><p>其中${x_i, x_j}$为特征向量, ${\Sigma}$为协方差矩阵.使用条件为样本符合正态分布</p></li><li><p>夹角余弦:</p><script type="math/tex; mode=display">C_{i j}=\cos ^{-1} \frac{\sum_{k=1}^{n} X_{i k} X_{j k}}{\sqrt{\left(\sum_{k=1}^{n} X_{i k}^{2}\right)\left(\sum_{k=1}^{n} X_{j k}^{2}\right)}}</script><p>即之间夹角小的样本具有相似性, 可分为一类.</p></li><li><p>相关系数:</p><script type="math/tex; mode=display">r_{i j}=\frac{\sum_{k=1}^{n}\left(X_{k i}-\overline{X}_{i}\right)\left(X_{k j}-\overline{X}_{j}\right)}{\sqrt{\sum_{k=1}^{n}\left(X_{k i}-\overline{X}_{i}\right)^{2} \sum_{k=1}^{n}\left(X_{k j}-\overline{X}_{j}\right)^{2}}}</script><p>$\overline{X}<em>{i}, \overline{X}</em>{j}$为$X<em>{i}, X</em>{j}$的均值.</p><p><strong>注意</strong>: 在求相关系数之前,要将数据标准化(极差标准化,方差标准化)</p></li></ol><h4 id="1-5-3-2-分类的主观性和客观性"><a href="#1-5-3-2-分类的主观性和客观性" class="headerlink" title="1.5.3.2 分类的主观性和客观性"></a>1.5.3.2 分类的主观性和客观性</h4><ol><li><p>分类带有主观性:目的不同,分类不同</p><p>例: 鲸鱼、牛、马从生物学的角度来讲都属于哺乳类,但是从产业角度来讲鲸鱼属于水产业,牛和马属于畜牧业</p></li><li><p>分类的客观性:科学性</p><p>判断分类必须有客观标准,因此分类是追求客观性的. 但主观性也很难避免,这就是分类的复杂性.</p></li></ol><h4 id="1-5-3-3-特征的生成"><a href="#1-5-3-3-特征的生成" class="headerlink" title="1.5.3.3 特征的生成"></a>1.5.3.3 特征的生成</h4><p>特征是决定相似性与分类的关键,寻找合适的特征是认知与识别的核心问题。可粗略地分为底层、中层、高层3个层次.</p><ol><li><p>低层特征:</p><ol><li>无序尺度: 有明确的数量和数值.</li><li>有序尺度: 有先后、好坏的次序关系,如酒分为上,中,下三个等级.</li><li>名义尺度: 无数量、无次序关系,如有红、黄两种颜色.</li></ol></li><li><p>中层特征:</p><ol><li>经过计算、变换得到的特征</li></ol></li><li><p>高层特征:</p><ol><li><p>在中层特征的基础上有目的的经过运算形成</p><p>例如: 椅子的重量=体积*比重, 体积与长、宽、高有关;比重与材料、纹理、颜色有关.这里低、中、高三层特征都有了.</p></li></ol></li></ol><h4 id="1-5-3-4-数据标准化"><a href="#1-5-3-4-数据标准化" class="headerlink" title="1.5.3.4 数据标准化"></a>1.5.3.4 数据标准化</h4><ol><li><p>极差标准化</p><p>极差: 一批样本中,每个特征的最大值与最小值之差.</p><script type="math/tex; mode=display">R_{i}=\max _{j}\left\{x_{i j}\right\}-\min _{j}\left\{x_{i j}\right\}</script><p>极差标准化两种方式:</p><script type="math/tex; mode=display">X_{i j}=\left( \begin{array}{l}{x_{i j}-\overline{x}_{i} ) /_{R_{i}}}\end{array}\right.</script><script type="math/tex; mode=display">X_{i j}=\frac{x_{i j}-\min _{j}\left\{x_{i j}\right\}}{R_{i}}</script></li><li><p>方差标准化</p><p>${S_i}$为方差:</p><script type="math/tex; mode=display">\overline{x}_{i}=\frac{1}{n} \sum_{j=1}^{n} x_{i j}</script><script type="math/tex; mode=display">S_{i}=\sqrt{\frac{1}{n-1} \sum_{j=1}^{n}\left(x_{i j}-\overline{x}_{i}\right)^{2}}</script><p>方差标准化:</p><script type="math/tex; mode=display">X_{i j}=\left(x_{i j}-\overline{x}_{i}\right) / S_{i}</script></li><li><p>归一化标准化</p><script type="math/tex; mode=display">X_{i j}=^{x_{i j}} / \sum_{j=1}^{n} x_{i j}</script></li></ol><p>标准化的方法很多,原始数据是否应该标准化,应采用什么方法标准化,都要根据具体情况来定.</p><p>Reference: 西北工业大学自动化学院张绍武教授《模式识别》课程PPT</p>]]></content>
    
    
    <categories>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pattern-Recognition</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模式识别入门（零） - 线性代数与概率论数学基础</title>
    <link href="/2019/05/02/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%85%A5%E9%97%A8-%E9%9B%B6-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A6%82%E7%8E%87%E8%AE%BA%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
    <url>/2019/05/02/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%85%A5%E9%97%A8-%E9%9B%B6-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A6%82%E7%8E%87%E8%AE%BA%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<p>本学期选修了学校自动化学院的张绍武教授《模式识别》课程。此为期末总结时候的复习总结。此篇主要包含了一些数学常识与基础。</p><a id="more"></a><h1 id="Chapter-0-数学基础"><a href="#Chapter-0-数学基础" class="headerlink" title="Chapter 0 数学基础"></a>Chapter 0 数学基础</h1><h2 id="0-1-行列式与线性方程组"><a href="#0-1-行列式与线性方程组" class="headerlink" title="0.1 行列式与线性方程组"></a>0.1 行列式与线性方程组</h2><p>解行列式: </p><ol><li>按行 (列) 展开计算</li><li>化为三角行列式计算</li></ol><pre><code class="hljs matlab">det(A)</code></pre><p>解线性方程组:</p><script type="math/tex; mode=display">\left\{\begin{array}{c}{a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}=b_{1}} \\ {a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}=b_{2}} \\ {\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots} \\ {a_{n 1} x_{1}+a_{n 2} x_{2}+\cdots+a_{n n} x_{n}=b_{n}}\end{array}\right.</script><ol><li><p>若系数行列式$\mathrm{D}=\left|a_{i j}\right| \neq 0$, 则方程组存在唯一解</p><script type="math/tex; mode=display">x_{1}=\frac{D_{1}}{D}, x_{2}=\frac{D_{2}}{D}, \cdots, x_{n}=\frac{D_{n}}{D}</script></li><li><p>若为齐次方程组($b=0$), 有非零解的充要条件是$\mathrm{D}=\left|a_{i j}\right|=0$</p></li></ol><h2 id="0-2-矩阵"><a href="#0-2-矩阵" class="headerlink" title="0.2 矩阵"></a>0.2 矩阵</h2><script type="math/tex; mode=display">A_{m \times n}=\left[a_{i j}\right]_{m} \times_{\mathrm{n}}​</script><p>方阵:</p><script type="math/tex; mode=display">m=n</script><p>对角阵:</p><script type="math/tex; mode=display">\wedge=\operatorname{diag}\left(a_{11}, a_{22}, \ldots, a_{n n}\right)</script><pre><code class="hljs matlab"><span class="hljs-built_in">diag</span>(A)</code></pre><p>单位阵:</p><script type="math/tex; mode=display">E=\operatorname{diag}(1,1, \dots, 1)</script><pre><code class="hljs matlab"><span class="hljs-built_in">eye</span>(n)</code></pre><p>上三角与下三角阵: (TODO)</p><pre><code class="hljs matlab"><span class="hljs-built_in">triu</span>(A)<span class="hljs-built_in">tril</span>(A)</code></pre><h2 id="0-3-矩阵运算"><a href="#0-3-矩阵运算" class="headerlink" title="0.3 矩阵运算"></a>0.3 矩阵运算</h2><p>矩阵乘法 C=AB:</p><script type="math/tex; mode=display">c_{i j}=\sum_{k} a_{i k} b_{k j}</script><p>矩阵的转置:</p><script type="math/tex; mode=display">\mathrm{A}=\left(\mathrm{a}_{i j}\right), \mathrm{A}^{\prime}=\mathrm{A}^{\mathrm{T}}=\left(\mathrm{a}_{j i}\right)</script><p>对称方阵:</p><script type="math/tex; mode=display">\mathrm{A}^{\prime}=\mathrm{A}, 即 \mathrm{a}_{i j}=\mathrm{a}_{j i}</script><p>方阵的行列式性质:</p><ol><li>如果$|\mathrm{A}| \neq 0$, $A$ 称为非奇异阵, 否则为奇异阵.</li><li>$\left|A^{\prime}\right|=|A|, \quad|A B|=|A||B|$</li></ol><p>逆矩阵:</p><p>如果$AB = BA = E$, 则称$A$可逆, $B$为$A$的逆.</p><p>方阵$A$可逆的充要条件为</p><script type="math/tex; mode=display">|\mathrm{A}| \neq 0</script><pre><code class="hljs matlab">B = inv(A)</code></pre><h2 id="0-4-分块矩阵及其运算"><a href="#0-4-分块矩阵及其运算" class="headerlink" title="0.4 分块矩阵及其运算"></a>0.4 分块矩阵及其运算</h2><p>用横线和竖线把矩阵分成若干小块,每个小块为一个矩阵,它可以作为一个元素参加运算。</p><script type="math/tex; mode=display">\left[ \begin{array}{ccccc}{1} & {0} & {\vdots} & {1} & {0} \\ {0} & {1} & {\vdots} & {0} & {1} \\ {\cdots} & {\cdots} & {\vdots} & {\cdots} & {\cdots} \\ {0} & {0} & {\vdots} & {1} & {2} \\ {0} & {0} & {\vdots} & {2} & {1}\end{array}\right] = \left[ \begin{array}{ll}{E_{2}} & {E_{2}} \\ {O} & {B_{22}}\end{array}\right]</script><p>分块对角阵(TODO):</p><script type="math/tex; mode=display">|\mathrm{A}|=\left|\mathrm{A}_{11}\right|\left|\mathrm{A}_{22}\right| \ldots\left|\mathrm{A}_{\mathrm{rr}}\right|</script><pre><code class="hljs matlab">A = <span class="hljs-built_in">blkdiag</span>(A11, A22, ..., Arr)A11 = A(<span class="hljs-number">1</span>:m, <span class="hljs-number">1</span>:n)</code></pre><h2 id="0-5-向量"><a href="#0-5-向量" class="headerlink" title="0.5 向量"></a>0.5 向量</h2><p>n维向量:</p><script type="math/tex; mode=display">\mathbf{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{\mathrm{T}}</script><p>线性相关与线性无关:</p><p>设有n维向量组: $\mathbf{x}<em>{1}, \mathbf{x}</em>{2}, \dots, \mathbf{x}<em>{\mathrm{m}}$, 如果只有当$k</em>{1}=k<em>{2}=\ldots=k</em>{\mathrm{m}}=0$时, 才能使得下式成立, 则称该向量组线性无关, 否则则称线性相关.</p><script type="math/tex; mode=display">k_{1} \mathbf{x}_{1}+k_{2} \mathbf{x}_{2}+\cdots+k_{m} \mathbf{x}_{m}=0</script><p>m个n维的向量的矩阵表示:</p><script type="math/tex; mode=display">\mathrm{A}=\left(\mathbf{a}_{1}, \mathbf{a}_{2}, \ldots, \mathbf{a}_{\mathrm{m}}\right)</script><p>n个n维向量: $\boldsymbol{a}<em>{i}=\left(\mathbf{a}</em>{\mathbf{i} 1}, \mathbf{a}<em>{\mathbf{i} 2}, \ldots, \mathbf{a}</em>{\mathbf{i n}}\right)^{\mathrm{T}}$线性无关的充要条件是</p><script type="math/tex; mode=display">| \mathrm{A}\mathrm  | \neq 0</script><h2 id="0-6-向量-二"><a href="#0-6-向量-二" class="headerlink" title="0.6 向量(二)"></a>0.6 向量(二)</h2><p>若满足下式, 则称A可以由向量组B线性表示:</p><script type="math/tex; mode=display">A=\left(a_{1}, a_{2}, \dots, a_{m}\right)=\left(b_{1}, b_{2}, \dots, b_{m}\right) C= BC</script><p>向量组的秩:(TODO)</p><script type="math/tex; mode=display">\operatorname{rank}(\mathrm{A})=\mathrm{nor}(A的最大线性无关组)</script><ol><li>向量组$\alpha_1, \alpha_2 … \alpha_s$线性无关等价于$\operatorname{rank}(\mathrm{\alpha_1, \alpha_2 … \alpha_s}) = s$</li><li>等价的向量组具有相同的秩</li><li>任意n+1个n维向量线性相关</li></ol><p>向量的内积:</p><script type="math/tex; mode=display">(\mathbf{x}, \mathbf{y})=\sum_{i=1}^{n} x_{i} y_{i}=\mathbf{x}^{T} \mathbf{y}</script><p>向量的模(范数/长度):</p><script type="math/tex; mode=display">|\mathbf{x}|=\sqrt{\mathbf{x}^{T} \mathbf{x}}</script><p>两点的距离:</p><script type="math/tex; mode=display">d\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)=\left|\mathbf{x}_{2}-\mathbf{x}_{1}\right|=\sqrt{\left(\mathbf{x}_{2}-\mathbf{x}_{1}\right)^{T}\left(\mathbf{x}_{2}-\mathbf{x}_{1}\right)}</script><p>两个向量的夹角:</p><script type="math/tex; mode=display">\theta=<\mathbf{x}_{1}, \mathbf{x}_{2}>=\arccos \frac{\mathbf{x}_{1}^{T} \mathbf{x}_{2}}{\left|\mathbf{x}_{1}\right|\left|\mathbf{x}_{2}\right|}</script><h2 id="0-7-向量-三"><a href="#0-7-向量-三" class="headerlink" title="0.7 向量(三)"></a>0.7 向量(三)</h2><p>两个向量正交:</p><script type="math/tex; mode=display">(\mathbf{x}, \mathbf{y})=0, \cos (\theta)=0</script><p>若非零的n维向量$\mathbf{x}<em>{1}, \mathbf{x}</em>{2}, \dots, \mathbf{x}_{\mathrm{m}}$两两正交, 则称为<strong>正交向量组</strong>. 正交向量组队的性质:</p><ol><li><p>正交向量组线性无关</p></li><li><p>若n维向量y可以由正交向量组$\mathbf{x}<em>{1}, \mathbf{x}</em>{2}, \dots, \mathbf{x}_{\mathrm{m}}$线性表示, 则</p><script type="math/tex; mode=display">\mathbf{y}=\frac{\mathbf{y}^{T} \mathbf{x}_{1}}{\mathbf{x}_{1}^{T} \mathbf{x}_{1}} \mathbf{x}_{1}+\frac{\mathbf{y}^{T} \mathbf{x}_{2}}{\mathbf{x}_{2}^{T} \mathbf{x}_{2}} \mathbf{x}_{2}+\cdots+\frac{\mathbf{y}^{T} \mathbf{X}_{m}}{\mathbf{x}_{m}^{T} \mathbf{x}_{m}} \mathbf{x}_{m}</script></li></ol><h2 id="0-8-向量-四"><a href="#0-8-向量-四" class="headerlink" title="0.8 向量(四)"></a>0.8 向量(四)</h2><p>向量空间:</p><p>对加法和乘法运算均封闭的非空向量集合称为一个向量空间.</p><p>向量空间$V$的基: 向量空间的任一向量都可以由线性无关的向量组$a<em>{1}, a</em>{2}, \dots, a<em>{r}$线性表示, 则称向量组$a</em>{1}, a<em>{2}, \dots, a</em>{r}$为$V$的基, $\operatorname{dim} \mathrm{V}=\mathrm{r}$</p><p>向量空间$V$中的任意一个向量$z$可由它的基唯一线性表示, 有序组$\left(x<em>{1}, x</em>{2}, \ldots, x_{r}\right)$称为向量$z$在该基下的坐标.</p><script type="math/tex; mode=display">z=x_{1} \mathbf{a}_{1}+x_{2} \mathbf{a}_{2}+\ldots+x_{r} \mathbf{a}_{r}</script><p>基变换与坐标变换:</p><script type="math/tex; mode=display">\left(\beta_{1} \beta_{2} \ldots \beta_{r}\right)=\left(\alpha_{1} \alpha_{2} \ldots a_{r}\right) C</script><h2 id="0-9-矩阵的特征值与特征向量"><a href="#0-9-矩阵的特征值与特征向量" class="headerlink" title="0.9 矩阵的特征值与特征向量"></a>0.9 矩阵的特征值与特征向量</h2><p>方阵A的特征值$\lambda$与特征向量$\alpha$:</p><script type="math/tex; mode=display">\mathrm{A}{\alpha}=\lambda {\alpha}</script><ol><li>设α是方阵A的属于特征值λ的特征向量,则$kα$也是A的属于特征值λ的特征向量.</li><li>方阵A的两个不同特征值所对应的特征向量是线性无关的</li></ol><p>方阵$A$的特征矩阵$A-λE$和特征多项式$|A-λE|$和特征多项式$|A-λE|$.</p><p>仿真$A$的特征方程:</p><script type="math/tex; mode=display">|A-\lambda E|=0</script><p>特征方程$|A-\lambda E|=0$的解$\lambda$为方阵$A$的特征值, 方程$(\mathrm{A}-\lambda \mathrm{E}) \mathrm{x}=0$的非零解向量就是方阵$A$的属于特征值$\lambda$的特征向量.</p><h2 id="0-10-相似矩阵"><a href="#0-10-相似矩阵" class="headerlink" title="0.10 相似矩阵"></a>0.10 相似矩阵</h2><p>如果存在可逆方阵$P$,使$P^{-1} A P=B$,则称A与B相似,记作$A \sim B$</p><ol><li>相似关系具有反身, 对称, 传递性.</li><li>相似矩阵有相同的行列式, 即$|\mathrm{A}|=|\mathrm{B}|$</li><li>相似矩阵有相同的特征多项式及特征值</li></ol><p>n阶方阵$A$与对角矩阵$\wedge$相似的充要条件是$A$有n个线性无关的特征向量.</p><p>如果$\mathrm{A} \sim \wedge$, 即有</p><script type="math/tex; mode=display">\mathrm{P}^{-1} \mathrm{AP}=\wedge=\operatorname{diag}\left(\mathrm{d}_{1}, \mathrm{d}_{2}, \ldots, \mathrm{d}_{\mathrm{n}}\right)</script><p>, 则$\mathrm{d}<em>{1}, \mathrm{d}</em>{2}, \ldots, \mathrm{d}_{\mathrm{n}}$是$A$的$n$个特征值.</p><p><strong>实对称矩阵</strong>:(TODO)</p><p>如果有n阶矩阵A, 其矩阵的元素都为实数,且矩阵A的转置等于其本身($a<em>{ij} = a</em>{ji}$)(i,j为元素的脚标), 则称A为实对称矩阵</p><ol><li>特征值为实数, 特征向量为实向量</li><li>两个相异的特征值对应的特征向量正交</li><li>n阶实对称方阵$A$有n个线性无关的特征向量</li><li>n阶实对称方阵$A$与对角矩阵相似, 即n阶实对称矩阵A必可对角化,且相似对角阵上的元素即为矩阵本身特征值</li></ol><h2 id="0-11-正交矩阵"><a href="#0-11-正交矩阵" class="headerlink" title="0.11 正交矩阵"></a>0.11 正交矩阵</h2><p>正交矩阵$A$, 有$A A^{\prime}=E$, 即$A^{-1}=A^{\prime}$</p><ol><li>正交矩阵$A, B$的乘积$AB$仍为正交矩阵</li><li>正交矩阵$A$的行列式$|\mathrm{A}|=1$</li></ol><p>正交矩阵$A$的行(列)向量组为正交单位向量组, 即:</p><script type="math/tex; mode=display">\left( \begin{array}{c}{\boldsymbol{a}_{1}} \\ {\boldsymbol{a}_{2}} \\ {\vdots} \\ {\boldsymbol{a}_{n}}\end{array}\right)\left(\boldsymbol{a}_{1}^{\prime}, \boldsymbol{a}_{2}^{\prime}, \cdots, \boldsymbol{a}_{n}^{\prime}\right)=E</script><script type="math/tex; mode=display">\boldsymbol{a}_{i} \boldsymbol{a}_{j}^{T}=\delta_{i j}</script><p>若$A$为实对称矩阵, 则一定存在<strong>正交矩阵$P$</strong>, 使得$P^{-1} A P=\wedge$, $\wedge$是以<strong>$A$的特征值为对角元素的对角矩阵</strong>.</p><h2 id="0-12-二次型"><a href="#0-12-二次型" class="headerlink" title="0.12 二次型"></a>0.12 二次型</h2><p>二次齐次函数：</p><script type="math/tex; mode=display">f\left(x_{1}, x_{2}, \cdots, x_{n}\right)=\sum_{i=1}^{n} a_{i j} x_{i} x_{j}, a_{i j}=a_{j i}</script><p>记$\mathbf{x}=\left(x<em>{1}, x</em>{2}, \ldots x<em>{n}\right)^{\top}$, $A=\left(a</em>{i j}\right)_{n^{*} n}$, 则有</p><script type="math/tex; mode=display">f\left(x_{1}, x_{2}, \cdots, x_{n}\right)=\mathbf{x}^{\prime} A \mathbf{x}</script><p>二次型$f$与对称矩阵$A$存在一一对应: $A$为二次型$f$的矩阵, $f$为矩阵$A$的二次型.</p><blockquote><p>$A=\wedge$时为标准二次型(只含平方项)</p></blockquote><p>对于任何二次型, 总可以找到正交变换将$f$化为标准型</p><script type="math/tex; mode=display">f\left(x_{1}, x_{2}, \cdots, x_{n}\right)=\mathbf{x}^{\prime} \mathbf{A} \mathbf{x}</script><script type="math/tex; mode=display">\mathbf{x}=C \mathbf{y}</script><script type="math/tex; mode=display">\wedge=C^{\prime} A C</script><script type="math/tex; mode=display">f=\mathbf{y}^{\prime} \wedge \mathbf{y}=\lambda_{1} y_{1}^{2}+\lambda_{2} y_{2}^{2}+\cdots \lambda_{n} y_{n}^{2}</script><h2 id="0-13-正定二次型和正定矩阵"><a href="#0-13-正定二次型和正定矩阵" class="headerlink" title="0.13 正定二次型和正定矩阵"></a>0.13 正定二次型和正定矩阵</h2><p>二次型$f\left(\mathrm{x}<em>{1}, \mathrm{x}</em>{2}, \dots, \mathrm{x}<em>{n}\right)$, 如果对于任何$\mathrm{x}</em>{1}^{2}+\mathrm{x}<em>{2}^{2}+\ldots+\mathrm{x}</em>{n}^{2} \neq 0$, 都有$f&gt;0$, 则称$f$为正定二次型.其矩阵$A$为正定矩阵($A&gt;0$).</p><p>n阶方阵$A$正定的充要条件是: A的n个特征值全为正数.</p><p>n阶方阵$A$, 若存在可逆矩阵$B$, 使得$A=B^{\prime} B$, 则$A$为正定矩阵.</p><p>意义(TODO)</p><h2 id="0-14-多元随机变量的统计特征"><a href="#0-14-多元随机变量的统计特征" class="headerlink" title="0.14 多元随机变量的统计特征"></a>0.14 多元随机变量的统计特征</h2><p>ｎ维随机变量: </p><script type="math/tex; mode=display">\mathbf{x}=\left[x_{1}, x_{2}, \ldots, x_{n}\right]^{\mathrm{T}}</script><p>n维随机变量的(总体)均值:</p><script type="math/tex; mode=display">\boldsymbol{\mu}=\mathrm{E}(\mathbf{x})=\int_{\mathbf{x}} \mathbf{x} p(\mathbf{x}) d \mathbf{x}</script><p>n维随机变量的(样本)均值:</p><script type="math/tex; mode=display">\hat{\boldsymbol{\mu}}=\frac{1}{N} \sum_{i=1}^{N} \mathbf{x}_{i}</script><p>n维随机变量的(总体)相关函数矩阵:(TODO)</p><script type="math/tex; mode=display">\mathrm{R}(\mathbf{x})=\left[r_{i j}\right]=\left[\mathrm{E}\left\{x_{i} x_{j}\right\}\right]=\mathrm{E}\left\{\mathbf{x} \mathbf{x}^{T}\right\}</script><p>n维随机变量的(样本)相关函数矩阵:</p><script type="math/tex; mode=display">\hat{\mathrm{R}}(\mathbf{x})=\frac{1}{N} \sum_{i=1}^{N} \mathbf{x}_{i} \mathbf{x}_{i}^{T}</script><p>n维随机变量的(总体)协方差矩阵:</p><script type="math/tex; mode=display">\mathrm{C}(\mathbf{x})=\left[c_{i j}\right]=\left[\mathrm{E}\left\{\left(x_{i}-\mu_{i}\right)\left(x_{j}-\mu_{j}\right)\right\}\right]=\mathrm{E}\left\{(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{T}\right\}</script><p>n维随机变量的(样本)协方差矩阵:</p><script type="math/tex; mode=display">\hat{\mathrm{C}}(\mathbf{x})=\frac{1}{N} \sum_{i=1}^{N}\left(\mathbf{x}_{i}-\boldsymbol{\mu}_{i}\right)\left(\mathbf{x}_{i}-\boldsymbol{\mu}_{i}\right)^{T}</script><h2 id="0-15-n维随机变量协方差矩阵的性质"><a href="#0-15-n维随机变量协方差矩阵的性质" class="headerlink" title="0.15 n维随机变量协方差矩阵的性质"></a>0.15 n维随机变量协方差矩阵的性质</h2><p>ｎ维随机变量的协方差矩阵$C$是实对称矩阵</p><ol><li><p>协方差矩阵$C$的特征值为实数</p></li><li><p>$C$有$n$个线性无关的特征向量</p></li><li><p>存在正交矩阵$U$, 使得$\mathrm{U}^{-1} \mathrm{CU}=\mathrm{U}^{\mathrm{T}} \mathrm{CU}=\wedge$, $\wedge$是以$C$的特征值为对角元素的对角矩阵, </p><script type="math/tex; mode=display">\mathrm{U}=\left[\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{\mathrm{n}}\right], \quad \mathrm{C} \mathbf{u}_{\mathrm{i}}=\lambda_{\mathrm{i}} \mathbf{u}_{\mathrm{i}}</script></li></ol><h2 id="0-16-梯度下降法"><a href="#0-16-梯度下降法" class="headerlink" title="0.16 梯度下降法"></a>0.16 梯度下降法</h2><p>准则函数: $J(\mathbf{a})$</p><p>最优化问题: $\mathbf{a}^{*}=\underset{\mathbf{a}}{\operatorname{argmin}} J(\mathbf{a})$</p><p>求解方法: $\mathbf{a}^{*}$应满足方程:</p><script type="math/tex; mode=display">\nabla J(\mathbf{a})=\left[ \begin{array}{cc}{\frac{\partial J}{\partial a_{1}}} & {\frac{\partial J}{\partial a_{2}}} & {\cdots} & {\frac{\partial J}{\partial a_{n}}}\end{array}\right]^{T}=0</script><p>沿梯度的负方向改变$\mathbf{a}$, 函数会很快达到极小点, 梯度趋于0.故迭代算法为:</p><script type="math/tex; mode=display">\mathbf{a}_{k+1}=\mathbf{a}_{k}-\eta \nabla J(\mathbf{a})</script><p>流程图:</p><pre><code class="hljs mermaid">graph TB;A(&quot;选择初始点a_0,给定容许误差ε,设定学习率η.设k&#x3D;0&quot;)--&gt;B[&quot;计算梯度▽J(a_k)&quot;];B--&gt;C[&quot;修改a_k: a_k+1 &#x3D; a_k - η▽J(a_k)&quot;];C--&gt;D&#123;&quot;计算J(a_k+1),并检验|J(a_k+1)-J(a_k)|&lt;ε&quot;&#125;;D--No--&gt;E&gt;&quot;k&#x3D;k+1&quot;];E--&gt;B;D--Yes--&gt;F[&quot;输出结果&quot;]F--&gt;G(&quot;结束&quot;)</code></pre><p>Reference: 西北工业大学自动化学院张绍武教授《模式识别》课程PPT</p>]]></content>
    
    
    <categories>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pattern-Recognition</tag>
      
      <tag>Math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模式识别实践 - K-L变换实现人脸识别</title>
    <link href="/2019/05/02/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%AE%9E%E8%B7%B5-KL%E5%8F%98%E6%8D%A2%E5%AE%9E%E7%8E%B0%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    <url>/2019/05/02/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%AE%9E%E8%B7%B5-KL%E5%8F%98%E6%8D%A2%E5%AE%9E%E7%8E%B0%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<p>本篇介绍使用K-L变换实现一个简单的人脸识别算法。完整的代码、报告和数据见Github：<a href="https://github.com/kehanXue/pca-face-recognition" target="_blank" rel="noopener">https://github.com/kehanXue/pca-face-recognition</a> 。K-L变换也常称为主成分变换(PCA)，是一种基于图像统计特性的变换，它的协方差矩阵除对角线以外的元素都是零(所以大家也叫它最佳变换)，消除了数据之间的相关性。</p><a id="more"></a><h1 id="应用K-L变换在OCL库中进行人脸识别"><a href="#应用K-L变换在OCL库中进行人脸识别" class="headerlink" title="应用K-L变换在OCL库中进行人脸识别"></a>应用K-L变换在OCL库中进行人脸识别</h1><h2 id="一-原理简述与程序框图"><a href="#一-原理简述与程序框图" class="headerlink" title="一. 原理简述与程序框图"></a>一. 原理简述与程序框图</h2><h3 id="1-原理"><a href="#1-原理" class="headerlink" title="1. 原理"></a>1. 原理</h3><h4 id="1-1-K-L变换"><a href="#1-1-K-L变换" class="headerlink" title="1.1 K-L变换"></a>1.1 K-L变换</h4><blockquote><p>K-L变换也常称为主成分变换(PCA)，是一种基于图像统计特性的变换，它的协方差矩阵除对角线以外的元素都是零(所以大家也叫它最佳变换)，消除了数据之间的相关性，从而在信息压缩方面起着重要作用。</p></blockquote><p>在模式识别和图像处理中一个主要的问题就是降维，在实际的模式识别问题中，我们选择的特征经常彼此相关，在识别这些特征时，数量很多，大部分都是无用的。如果我们能减少特征的数量，即减少特征空间的维数，那么我们将以更少的存储和计算复杂度获得更好的准确性。如何寻找一种合理的综合性方法，使得：</p><ol><li>减少特征量的个数。</li><li>尽量不损失或者稍损失原特征中所包含的信息。</li><li>使得原本相关的特征转化为彼此不相关(用相关系数阵衡量)。</li></ol><p><strong>K-L变换</strong>即主成分分析就可以简化大维数的数据集合。K-L 变换以原始数据的<strong>协方差矩阵的归一化正交特征矢量构成的正交矩阵</strong>作为变换矩阵,对原始数据进行正交变换,在变换域上实现数据压缩。它具有去相关性、能量集中等特性,属于均方误差测度下,失真最小的一种变换,是最能去除原始数据之间相关性的一种变换。它还可以用于许多图像的处理应用中，例如：压缩、分类、特征选择等。</p><p>K-L变换的实质就是去除各维度之间的相关性。就是建立新的坐标系，将原本高度相关的数据在新坐标系下的协方差矩阵除对角线以外的元素都是零。从坐标系的角度来看，图像的矩阵可以看作为二维平面上一组像素点坐标的集合，变换你结果Y可以看作是图像矩阵X在一个新的坐标系下的相同像素点的集合。该新的坐标系为原坐标系的旋转，旋转矩阵即为K-L变换矩阵。</p><blockquote><p>在原坐标系下的x和y具有非常强的相关性，而变换后两者之间的相关性被去除。</p></blockquote><p>K-L变换的目的，即在于找出使得X矢量中的各个分量相关性降低或去除的方向，对图像进行旋转，使其在新空间的坐标轴指向各个主分量方向——主成分分析或者主成分变换。扩展至多维空间，K-L变换可实现多维空间中的去相关，并将能量集中在少数主分量上。</p><p>构建新坐标系的过程就是主成分变换的过程。y代表新坐标系的坐标点，x代表原来坐标系的点。</p><script type="math/tex; mode=display">y=W * x</script><p>其中$W$就是变换矩阵, $W$矩阵就是$x$的协方差矩阵的特征向量矩阵的转置矩阵。</p><h4 id="1-2-主成分选取"><a href="#1-2-主成分选取" class="headerlink" title="1.2 主成分选取"></a>1.2 主成分选取</h4><p>PCA 则是选取协方差矩阵前 k 个最大的特征值的特征向量构成 K-L 变换矩阵。保留多少个主成分取决于保留部分的累积方差在方差总和中所占百分比（即累计贡献率），它标志着前几个主成分概括信息之多寡。实践中,粗略规定一个百分比便可决定保留几个主成分;如果多留一个主成分,累积方差增加无几,便不再多留。</p><h4 id="1-3-人脸空间的建立"><a href="#1-3-人脸空间的建立" class="headerlink" title="1.3 人脸空间的建立"></a>1.3 人脸空间的建立</h4><p>假设一幅人脸图像包含 $N$ 个像素点,它可以用一个 $N$ 维向量 $x$ 表示,这样,训练样本库就可以使用 ${x_i(i=1,…,M)}$ 来表示. 协方差矩阵 $C$ 的正交特征向量就是组成人脸空间的基向量,即特征脸. 将特征值由小到大排列:</p><script type="math/tex; mode=display">\lambda_{1} \geqslant \lambda_{2} \geqslant \ldots \geqslant \lambda_{\mathrm{r}}</script><p>其对应的特征向量为 $\mu_{\mathrm{k}}$. 这样每一幅人脸图像对应于子空间中的一点。同样,子空间的任意一点也对应于一幅图像.</p><h4 id="1-4-人脸识别"><a href="#1-4-人脸识别" class="headerlink" title="1.4 人脸识别"></a>1.4 人脸识别</h4><p>有了这样一个由”特征脸”张成的降维子空间,任何一幅人脸图像都可以向其投影得到一组坐标系数,这组系数表明了该图像在子空间中的位置,从而可以作为人脸识别的依据。</p><ol><li><p>计算数据库中每张图片在子空间中的坐标,得到一组坐标,作为下一步识别匹配的搜索空间。</p></li><li><p>计算新输入图片在子空间中的坐标,采用最小距离法,遍历搜索空间,得到与其距离最小的坐标向量,该向量对应的人脸图像即为识别匹配的结果。</p></li></ol><h3 id="2-实验步骤"><a href="#2-实验步骤" class="headerlink" title="2. 实验步骤"></a>2. 实验步骤</h3><ol><li><p>选取数据集，并将其读入．将每一张图像均reshape为列向量$X_i$,并组合成数据矩阵.</p><script type="math/tex; mode=display">\mathbf{X}=\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}\right)</script></li><li><p>求均值向量</p><script type="math/tex; mode=display">\boldsymbol{\mu}=\frac{1}{n} \sum_{i}^{n} \boldsymbol{X}_{i}</script></li><li><p>求中心化后的数据矩阵</p><script type="math/tex; mode=display">\mathrm{C}=\left(\mathbf{X}_{1}-\boldsymbol{\mu}, \mathbf{X}_{2}-\boldsymbol{\mu}, \ldots, \mathbf{X}_{n}-\boldsymbol{\mu}\right)</script></li><li><p>求$C^{\top} \mathrm{C}$的协方差矩阵的特征值, 选取出$k$个使得它的总能量达到设定值, 求出特征脸(特征向量)$\mathbf{e}_{\mathbf{i}}$, 将$k$个这样的向量按列排列成变换矩阵$W$</p><script type="math/tex; mode=display">W=\left(\mathbf{e}_{1}, \mathbf{e}_{2}, \ldots, \mathbf{e}_{k}\right)</script></li><li><p>计算每一幅图像的投影($k$维列向量)</p><script type="math/tex; mode=display">Y_{i}=W^{\top}\left(X_{i}-\mu\right)</script></li><li><p>计算待识别的人脸的投影(k维列向量), 设待识别的人脸为$Z$</p><script type="math/tex; mode=display">\operatorname{ch} \mathbf{Z}=\mathbf{W}^{\top}(\mathbf{Z}-\boldsymbol{\mu})</script></li><li><p>遍历搜索进行匹配,找到最近邻的人脸图像,根据标签得到$Z$属于第几个人.</p><script type="math/tex; mode=display">Y_{j}=\min | | Y_{i}-\operatorname{ch} z| |</script></li></ol><h3 id="3-程序框图"><a href="#3-程序框图" class="headerlink" title="3. 程序框图"></a>3. 程序框图</h3><pre><code class="hljs mermaid">graph TB;A(&quot;读入训练集数据，每个人10张照片中选取8张作为训练集&quot;)--&gt;B(&quot;求均值向量avgX&quot;);B--&gt;C(&quot;训练数据中心化&quot;);C--&gt;D(&quot;进行K-L变换，求出特征矩阵与所有特征值&quot;);D--&gt;E(&quot;进行主成分分析，选取构成能力95%的特征值&quot;);E--&gt;F(&quot;得到变换矩阵W，计算出特征脸&quot;);F--&gt;G(&quot;将训练数据投影到该新的特征空间中&quot;);G--&gt;H(&quot;取每个人的两张照片构成测试集，计算准确率&quot;);</code></pre><h2 id="二-实验结果分析"><a href="#二-实验结果分析" class="headerlink" title="二. 实验结果分析"></a>二. 实验结果分析</h2><h3 id="1-训练数据中心化样例"><a href="#1-训练数据中心化样例" class="headerlink" title="1. 训练数据中心化样例"></a>1. 训练数据中心化样例</h3><p>Fig.1.2.1训练数据中心化后的图片.</p><p><img src="/assets/avgFace-1556715073400.jpg" srcset="/img/loading.gif" alt=""></p><p>这里展示其中20张训练数据中心化后的人脸样例.</p><h3 id="2-计算出的特征脸样例"><a href="#2-计算出的特征脸样例" class="headerlink" title="2. 计算出的特征脸样例"></a>2. 计算出的特征脸样例</h3><p>Fig.1.2.2 特征脸样例</p><p><img src="/assets/featureFace.jpg" srcset="/img/loading.gif" alt="特征脸"></p><p>这里展示其中20张特征脸样例. </p><h3 id="3-训练数据投影到新的特征空间"><a href="#3-训练数据投影到新的特征空间" class="headerlink" title="3. 训练数据投影到新的特征空间"></a>3. 训练数据投影到新的特征空间</h3><p>Fig.1.2.3 训练数据投影到新的特征空间后的数据分布图</p><p><img src="/assets/refs-1556715136687.jpg" srcset="/img/loading.gif" alt=""></p><p>​    训练数据投影到新的特征空间后对应的新的数据矩阵,横轴表示维度(选取的主分量的数量),纵轴代表投影后的各个分量的大小,可以看出靠前的分量占得总能量更大.</p><h3 id="4-训练数据的数量对测试分类精确度的影响"><a href="#4-训练数据的数量对测试分类精确度的影响" class="headerlink" title="4. 训练数据的数量对测试分类精确度的影响"></a>4. 训练数据的数量对测试分类精确度的影响</h3><p>我们的训练数据集中一共有40个人的照片,每个人的照片各十张.我们将每个人的后两张图片作为测试样本集.然后根据前面通过改变训练样本的数量来观察其对分类精确度的影响.我们通过改变每个人选取的照片数量$n$来控制训练数据集的大小.我们分别取了$n=3,4,…,8$,绘制出了其精确度变化的曲线.</p><p>Fig.1.2.4 模型的精确度随训练数据集的数量的变化.</p><p><img src="/assets/acc-1555498077760.jpg" srcset="/img/loading.gif" alt=""></p><p>可见,随着训练样本数的增加,模型的精确度也在上升,其中当每个人的照片至少取4张时才可以开始进行比较好的判别.</p><h2 id="三-程序代码"><a href="#三-程序代码" class="headerlink" title="三. 程序代码"></a>三. 程序代码</h2><p>matlab程序代码:</p><pre><code class="hljs matlab">clearclose all;clc<span class="hljs-comment">% 按文件夹读取数据,数据保存到allData中</span>db = dir(<span class="hljs-string">'orl_faces'</span>);face = cell(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>); allData = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>);labelList = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>);acc_index = <span class="hljs-number">0</span>;<span class="hljs-keyword">for</span> train_data_nums = <span class="hljs-number">6</span>:<span class="hljs-number">-1</span>:<span class="hljs-number">2</span>    num = <span class="hljs-number">0</span>;    <span class="hljs-built_in">type</span> = <span class="hljs-number">0</span>;    labelMap = containers.Map(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>);    <span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">4</span> : <span class="hljs-built_in">length</span>(db)        fi = dir([db(<span class="hljs-built_in">i</span>).folder <span class="hljs-string">'/'</span> db(<span class="hljs-built_in">i</span>).name]);        <span class="hljs-built_in">type</span> = <span class="hljs-built_in">type</span> + <span class="hljs-number">1</span>;        <span class="hljs-keyword">for</span> <span class="hljs-built_in">j</span> = <span class="hljs-number">3</span> : <span class="hljs-built_in">length</span>(fi)-train_data_nums            num = num + <span class="hljs-number">1</span>;            face&#123;<span class="hljs-built_in">i</span> - <span class="hljs-number">2</span>, <span class="hljs-built_in">j</span> - <span class="hljs-number">2</span>&#125; = imread([fi(<span class="hljs-built_in">j</span>).folder <span class="hljs-string">'/'</span> fi(<span class="hljs-built_in">j</span>).name]);            labelMap(num) = <span class="hljs-built_in">type</span>;            <span class="hljs-keyword">if</span> num == <span class="hljs-number">1</span>                [imageLen, imageWid] = <span class="hljs-built_in">size</span>(face&#123;<span class="hljs-built_in">i</span> - <span class="hljs-number">2</span>, <span class="hljs-built_in">j</span> - <span class="hljs-number">2</span>&#125;);            <span class="hljs-keyword">end</span>            allData(<span class="hljs-number">1</span>: imageLen * imageWid, num) = double(<span class="hljs-built_in">reshape</span>(face&#123;<span class="hljs-built_in">i</span> - <span class="hljs-number">2</span>, <span class="hljs-built_in">j</span> - <span class="hljs-number">2</span>&#125;, [imageLen * imageWid, <span class="hljs-number">1</span>]));            <span class="hljs-comment">% imshow(face&#123;i - 2, j - 2&#125;);</span>        <span class="hljs-keyword">end</span>        labelList(<span class="hljs-number">1</span>, <span class="hljs-built_in">type</span>) = <span class="hljs-built_in">i</span><span class="hljs-number">-3</span>;    <span class="hljs-keyword">end</span>    <span class="hljs-comment">% 求均值向量</span>    [allDataRows, allDataCols] = <span class="hljs-built_in">size</span>(allData);    <span class="hljs-comment">% avgX = 1.0/allDataCols * sum(allData, 2);</span>    avgX = <span class="hljs-built_in">mean</span>(allData, <span class="hljs-number">2</span>);    <span class="hljs-comment">% plot(avgX);</span>    <span class="hljs-comment">% imshow(reshape(avgX, [imageLen, imageWid]));</span>    <span class="hljs-keyword">for</span> num = <span class="hljs-number">1</span>:allDataCols        allData(:, num) = allData(:, num)-avgX;    <span class="hljs-keyword">end</span>    <span class="hljs-comment">% for num = 1:20</span>    <span class="hljs-comment">%     subplot(4, 5, num);imshow(reshape(allData(:, num), [imageLen, imageWid]));</span>    <span class="hljs-comment">% end</span>    <span class="hljs-comment">% 求协方差矩阵</span>    <span class="hljs-comment">% covC = 1.0/allDataCols * C * C';</span>    covC = allData' * allData;    <span class="hljs-comment">% covC = cov(allData);</span>    [coeffC, latentC, explainedC] = pcacov(covC);    <span class="hljs-comment">% 选取构成能量95%的特征值</span>    numOfLatent = <span class="hljs-number">1</span>;    proportion = <span class="hljs-number">0</span>;    <span class="hljs-keyword">while</span>(proportion &lt; <span class="hljs-number">95</span>)        proportion = proportion + explainedC(numOfLatent);        numOfLatent = numOfLatent+<span class="hljs-number">1</span>;    <span class="hljs-keyword">end</span>    numOfLatent = numOfLatent - <span class="hljs-number">1</span>;    <span class="hljs-comment">% 求特征脸</span>    W = allData*coeffC;    W = W(:, <span class="hljs-number">1</span>:numOfLatent);    <span class="hljs-comment">% plot(W(:), 1);</span>    <span class="hljs-comment">% for k = 1:20</span>    <span class="hljs-comment">%     subplot(4, 5, k);imshow(reshape(W(:, k), [imageLen, imageWid]));</span>    <span class="hljs-comment">% end</span>    <span class="hljs-comment">% 将训练数据投影到该新的特征空间中</span>    reference = W' * allData;    <span class="hljs-comment">% plot(reference);</span>    <span class="hljs-comment">% 测试训练结果准确率</span>    distances = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>);    testNum = <span class="hljs-number">0</span>;MNIST 数据集来自美国国家标准与技术研究所, National Institute of Standards and Technology (NIST). 训练集 (training set) 由来自 <span class="hljs-number">250</span> 个不同人手写的数字构成, 其中 <span class="hljs-number">50</span><span class="hljs-comment">% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据.</span>    type_index = <span class="hljs-number">0</span>;    accCnt = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">4</span> : <span class="hljs-built_in">length</span>(db)        fi = dir([db(<span class="hljs-built_in">i</span>).folder <span class="hljs-string">'/'</span> db(<span class="hljs-built_in">i</span>).name]);        type_index = type_index + <span class="hljs-number">1</span>;        <span class="hljs-keyword">for</span> <span class="hljs-built_in">j</span> = <span class="hljs-built_in">length</span>(fi)<span class="hljs-number">-1</span> : <span class="hljs-built_in">length</span>(fi)            testNum = testNum + <span class="hljs-number">1</span>;            imgTest = imread([fi(<span class="hljs-built_in">j</span>).folder <span class="hljs-string">'/'</span> fi(<span class="hljs-built_in">j</span>).name]);            imgTest = <span class="hljs-built_in">reshape</span>(imgTest, [imageLen * imageWid, <span class="hljs-number">1</span>]);            imgTest = double(imgTest(:));            imgTest = W' * (imgTest - avgX);            dis = <span class="hljs-built_in">realmax</span>(<span class="hljs-string">'double'</span>);            <span class="hljs-keyword">for</span> k = <span class="hljs-number">1</span>:allDataCols                temp = norm(imgTest - reference(:, k));                distances(:, k) = norm(imgTest - reference(:, k));                <span class="hljs-keyword">if</span>(dis &gt; temp)                    aimOne = k;                    dis = temp;                <span class="hljs-keyword">end</span>            <span class="hljs-keyword">end</span>            <span class="hljs-keyword">if</span> labelMap(aimOne) == type_index                accCnt = accCnt + <span class="hljs-number">1</span>;            <span class="hljs-keyword">end</span>        <span class="hljs-keyword">end</span>    <span class="hljs-keyword">end</span>    <span class="hljs-comment">% plot(distances);</span>    acc_index = acc_index + <span class="hljs-number">1</span>;    accuray(acc_index+<span class="hljs-number">3</span>) = accCnt/testNum;<span class="hljs-keyword">end</span><span class="hljs-comment">% plot(accuray);</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pattern-Recognition</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用 Autoware 进行双目相机与激光雷达的联合标定</title>
    <link href="/2019/04/02/%E4%BD%BF%E7%94%A8Autoware%E8%BF%9B%E8%A1%8C-%E5%8F%8C%E7%9B%AE-%E7%9B%B8%E6%9C%BA%E4%B8%8E%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%9A%84%E8%81%94%E5%90%88%E6%A0%87%E5%AE%9A/"/>
    <url>/2019/04/02/%E4%BD%BF%E7%94%A8Autoware%E8%BF%9B%E8%A1%8C-%E5%8F%8C%E7%9B%AE-%E7%9B%B8%E6%9C%BA%E4%B8%8E%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%9A%84%E8%81%94%E5%90%88%E6%A0%87%E5%AE%9A/</url>
    
    <content type="html"><![CDATA[<p>使用 Autoware 提供的 autoware_camera_lidar_calibrator 工具进行进行 VLP-16 与 ZED Camera 两者之间的标定。</p><a id="more"></a><p>在用Autoware提供的工具进行标定之前,搜索了很多的标定工具,但是看其他的方法或多或少都有一点点的麻烦,比如还要制作比较大的标定板等等,而使用 Autoware 则比较简单, 可以直接通过手动对齐图像中的像素点与激光雷达的 3D points 来进行标定. 标定结束后感觉该方式对于 VLP-16 这种较为稀疏的多线激光雷达来说,标定的精度可能不是很高.</p><h1 id="安装Autoware"><a href="#安装Autoware" class="headerlink" title="安装Autoware"></a>安装Autoware</h1><p>官方推荐使用 Docker 安装，我选择直接源码装了。<a href="https://github.com/autowarefoundation/autoware/wiki/Demo" target="_blank" rel="noopener">跑Demo的地址</a>中有一个 Build Sources 的<a href="https://github.com/CPFL/Autoware/wiki/Source-Build" target="_blank" rel="noopener">链接</a>是源码编译的步骤。源码编译其实也不麻烦，Autoware 是基于 ROS 搞的，rosdep 依赖一装然后跑编译脚本即可。中间没出什么问题，反而Docker的时候因为要装 NVIDIA Docker（因为要用 GPU），笔记本 ubuntu 端一牵扯显卡啥的就老出一堆问题。后面懒得折腾了就直接源码编译了一遍 Autoware。</p><h1 id="开始标定工作"><a href="#开始标定工作" class="headerlink" title="开始标定工作"></a>开始标定工作</h1><p>Autoware装好后，source 一下它的setup.zsh，就可以正常使用了。它提供了 <code>autoware_camera_lidar_calibrator</code> 可以用来联合标定相机与激光雷达。联合标定分为两步走:</p><ol><li>获取相机的内参</li><li>获得相机-Lidar的外参</li></ol><h1 id="标定相机内参"><a href="#标定相机内参" class="headerlink" title="标定相机内参"></a>标定相机内参</h1><p>单目和双目相机均可以用 <code>autoware_camera_lidar_calibrator</code> 来标定。还需准备一个棋盘格。</p><p>标定单目:</p><pre><code class="hljs bash">rosrun autoware_camera_lidar_calibrator cameracalibrator.py --square SQUARE_SIZE --size MxN image:=/image_topic</code></pre><p>标定双目:</p><pre><code class="hljs bash">rosrun autoware_camera_lidar_calibrator cameracalibrator.py --square SQUARE_SIZE  --size MxN right:=/image_topic left:=/image_topic</code></pre><p>参数说明:</p><p><code>--square</code> :棋盘格中的每个方格的边长大小。单位为m</p><p><code>--size</code> :棋盘格的尺寸是几乘几。注意是 <strong>inner</strong> ，也就是出去边长最外圈方格数-1。如果这个参数设置不对的话会在下面的标定步骤中发现标定程序毫无反应</p><p><code>image</code> :发布图像的话题名</p><p><code>right</code>,<code>left</code> : 左右眼图像的话题名。</p><p>最后发现它的参数其实还有一个 <code>camera_info</code>，但是好像是zed的包的小bug或者zed launch时未加标定文件吧，zed的节点发布的这个话题中是没有消息的。标定的时候也没有加这个topic，目前看起来没啥影响。</p><p><img src="/assets/autoware-1.png" srcset="/img/loading.gif" alt="相机内参标定"></p><p>上述节点启动起来后，会弹出一个图像框，右边有几个按钮，通过晃动棋盘格使得右边的 <code>Calibration</code> 按钮变绿。把棋盘格拿到相机前，看到棋盘格上有了mark并且右上方出现了四个滑动条类似的东西，分别表示X、Y、尺度与俯仰，根据提示哪一个自由度完成度不足来移动棋盘格。等 <code>Calibration</code> 按钮变绿了之后就可以点击一下，命令行窗口会给出计算出的标定结果，再点击Save按钮即可保存成在home目录下的命名类似于20190401_1133_autoware_camera_calibration.yaml文件。</p><blockquote><p>我Save的时候报了个错说没有cv2有关的那个对象没write方法，然后pip重新装一下opencv-python即可</p></blockquote><p>标定双目相机的流程与上图类似。结果被打包成为一个压缩包，里面分别有左右眼的参数和ost。但这个结果在后面的相机雷达联合标定中是无法使用的。后面的相机雷达联合标定需要用到的是单目标定出的那个 yaml 参数文件.</p><h1 id="相机雷达联合标定"><a href="#相机雷达联合标定" class="headerlink" title="相机雷达联合标定"></a>相机雷达联合标定</h1><p>相机与雷达的联合标定是要使用上一步的相机内参标定结果的。运行命令如下:</p><pre><code class="hljs bash">roslaunch autoware_camera_lidar_calibrator camera_lidar_calibration.launch intrinsics_file:=/PATH/TO/YYYYmmdd_HHMM_autoware_camera_calibration.yaml image_src:=/image</code></pre><p>参数说明:</p><p><code>intrinsics_file</code> :相机内参标定结果的yaml文件</p><p><code>image_src</code> :为发布图像信息的话题</p><p>启动后会出现一个显示已经纠正过的相机图像的弹框（该弹框需要ROS的image-view2组件，报错找不到这个node的话apt装一下就好了）。</p><p>然后启动雷达开启rviz调出雷达扫描的点云图。然后通过寻找图像窗口中图像的像素点与雷达点云数据的对应关系，先点击图像上的像素点，然后在rviz中通过 <code>publish point</code> 工具点击雷达点云中对应的3D点，看命令行是会有反应/输出信息的。寻找9组后会把生成的标定结果文件生成到home目录下，命名格式为20190401_115333_autoware_lidar_camera_calibration.yaml。</p><p><img src="/assets/autoware-2.png" srcset="/img/loading.gif" alt="相机雷达外参标定"></p><p>标定完9组后自动在home目录下生成结果文件:</p><p><img src="/assets/autoware-3.png" srcset="/img/loading.gif" alt="相机雷达外参标定2"></p>]]></content>
    
    
    <categories>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Robotics</tag>
      
      <tag>Autoware</tag>
      
      <tag>ROS</tag>
      
      <tag>Calibration</tag>
      
      <tag>Multi-Sensors</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
