<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>EMVS: Event-Based Multi-View Stereo 论文解读</title>
    <link href="/2020/06/10/EMVS%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <url>/2020/06/10/EMVS%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<p>《EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time》 主要讲述了一种利用 Event Camera 的高速特性，提出的一种 MVS 方法。将传统 MVS 的 Space-Sweep 方法应用到 Event-based MVS 中。<br><a id="more"></a></p><h1 id="EMVS论文解读"><a href="#EMVS论文解读" class="headerlink" title="EMVS论文解读"></a>EMVS论文解读</h1><blockquote><p>Rebecq, H., Gallego, G., Mueggler, E., Scaramuzza, D.,<br><em><a href="https://doi.org/10.1007/s11263-017-1050-6" target="_blank" rel="noopener">EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time</a></em>,<br>Int. J. of Computer Vision (IJCV), 126(12):1394-1414, 2018. <a href="http://rpg.ifi.uzh.ch/docs/IJCV17_Rebecq.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://youtu.be/EFpZcpd9XJ0" target="_blank" rel="noopener">YouTube</a>, <a href="https://github.com/uzh-rpg/rpg_emvs" target="_blank" rel="noopener">Code</a>.</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇 paper 主要讲述了一种利用 Event Camera 的高速特性，提出的一种 MVS 方法。将传统 MVS 的 Space-Sweep 方法应用到 Event-based MVS 中；在 Event-based 场景运用 Space-Sweep 具有天然的优势：</p><ol><li><p>Event Camera 本来就是检测的场景中的 edge。</p></li><li><p>Event Camera 相比于固定帧率的传统 Camera，viewpoints 更加的稠密。</p><p><img src="/assets/image-20200614164507222.png" srcset="/img/loading.gif" alt="image-20200614164507222" style="zoom:50%;" /></p></li></ol><p>文章的整体思路简而言之，就是将“每一帧”的 Events 进行反投影得到了一束光线，通过计算经过 DSI 空间中每一个 voxel 的光线的数量，高于一个阈值即可确定为一个空间的 3D 点。具体实施起来，分为下文中的五步。</p><p>完整的公式推导见原论文。</p><h2 id="主要过程"><a href="#主要过程" class="headerlink" title="主要过程"></a>主要过程</h2><h3 id="Feature-Viewing-Rays-by-Event-Back-Projection"><a href="#Feature-Viewing-Rays-by-Event-Back-Projection" class="headerlink" title="Feature-Viewing Rays by Event Back-Projection"></a>Feature-Viewing Rays by Event Back-Projection</h3><p>将每一个 Event 进行反投影，得到一条光线。</p><blockquote><p>This higher abundance of measurements and viewpoints in the event-based setting generates many more viewing rays than in frame-based MVS, and therefore, it facilitates the detection of scene points by analyzing the regions of high ray density.</p><p>A major advantage of our method is that no explicit data association is needed.</p></blockquote><p>不需要 Event 之间的数据关联、不需要 intensity 信息。</p><h3 id="Volumetric-Ray-Counting-Creating-the-Disparity-Space-Image-DSI"><a href="#Volumetric-Ray-Counting-Creating-the-Disparity-Space-Image-DSI" class="headerlink" title="Volumetric Ray Counting. Creating the Disparity Space Image (DSI)"></a>Volumetric Ray Counting. Creating the Disparity Space Image (DSI)</h3><p>在 DSI 中进行计数。这个时候问题就来了，如何确定这个 DSI 空间呢？</p><p>作者考虑到 <code>the reconstruction of large scenes in a scalable way</code>，采用了一个分批的方式，将一组 Event 归位一批，选取一个虚拟帧 RV，基于它做一个 <code>local 3D reconstruction</code>。</p><p><img src="/assets/image-20200614172203728.png" srcset="/img/loading.gif" alt="image-20200614172203728" style="zoom:50%;" /></p><h3 id="Detection-of-Scene-Structure-by-Maximization-of-Ray-Density"><a href="#Detection-of-Scene-Structure-by-Maximization-of-Ray-Density" class="headerlink" title="Detection of Scene Structure by Maximization of Ray Density"></a>Detection of Scene Structure by Maximization of Ray Density</h3><p>Local DSI 建立好之后，就卡一个阈值，是一个 <code>local maxima</code> 的操作。一个DSI的实例以及使用 <code>local maxima</code> 而不是 <code>global maxima</code> 的原因如下：</p><p><img src="/assets/image-20200614172541662.png" srcset="/img/loading.gif" alt="image-20200614172541662" style="zoom:50%;" /></p><p>作者使用了一个 Adaptive Gaussian 阈值函数：</p><blockquote><p>a pixel (<em>x</em>, <em>y</em>) is selected if <em>c</em>(<em>x</em>, <em>y</em>) &gt; <em>T</em> (<em>x</em>, <em>y</em>), with <em>T</em>(<em>x</em>,<em>y</em>) = <em>c</em>(<em>x</em>,<em>y</em>) ∗ <em>G</em>σ(<em>x</em>,<em>y</em>) − <em>C</em>. In practice, we use a 5×5 neighborhood in <em>G</em>σ and <em>C</em> = −10</p></blockquote><p>从 DSI 中提取结构：</p><p><img src="/assets/image-20200614174118964.png" srcset="/img/loading.gif" alt="image-20200614174118964" style="zoom:50%;" /></p><h3 id="Merging-Depth-Maps-from-Multiple-Reference-Viewpoints"><a href="#Merging-Depth-Maps-from-Multiple-Reference-Viewpoints" class="headerlink" title="Merging Depth Maps from Multiple Reference Viewpoints"></a>Merging Depth Maps from Multiple Reference Viewpoints</h3><p>既然有了上面提到的 Local reconstruction，那么就肯定有 key frame 的概念和一次 local 的大小的确定。</p><blockquote><p>we select a new <em>key</em> reference view as soon as the distance to the previous <em>key</em> reference view exceeds a certain percent- age of the mean scene depth (typically a number between 15 and 40%), and use the subset of events until the next <em>key</em> ref- erence view to estimate the corresponding semi-dense depth map of the scene.</p></blockquote><p>然后再做滤波去除一些噪声。融合的话，有了每组 local 点云的 pose ，有很多的方法就可以选了。</p><h3 id="Map-Cleaning"><a href="#Map-Cleaning" class="headerlink" title="Map Cleaning"></a>Map Cleaning</h3><p>为了得到更好的效果，又进一步做了滤波。</p><blockquote><p>we use a median filter on the semi-dense depth maps</p><p>we also apply a radius filter (Rusu and Cousins 2011) to the final point cloud, which discards the points whose number of neighbors within a given radius is less than a threshold.</p></blockquote><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><h3 id="实现实时、高效的-Event-反投影到-DSI-中"><a href="#实现实时、高效的-Event-反投影到-DSI-中" class="headerlink" title="实现实时、高效的 Event 反投影到 DSI 中"></a>实现实时、高效的 Event 反投影到 DSI 中</h3><p>主要思路：DSI 是由不同 depth 的平面们组成的，也就是说明只有 translation，无 rotation。那么，求出一个相机的当前帧与一个深度为 $Z_0$ 的 plane 之间的 homography，与其他 DSI 中的深度为 $Z_i$ 的 plane 的 homography 就可以很简单的来计算，可以推导为一步公式。</p><p>相机平面、EV 虚拟相机平面、不同深度 $Zi$ DSI 平面之间的关系：</p><script type="math/tex; mode=display">\left(x\left(Z_{0}\right), y\left(Z_{0}\right), 1\right)^{\top} \sim_{\mathrm{H} Z_{0}}(u, v, 1)^{\top}</script><script type="math/tex; mode=display">\left(x\left(Z_{i}\right), y\left(Z_{i}\right), 1\right)^{\top} \sim H_{Z_{i}} H_{Z_{0}}^{-1}\left(x\left(Z_{0}\right), y\left(Z_{0}\right), 1\right)^{\top}</script><script type="math/tex; mode=display">\mathrm{H}_{Z_{i}} \mathrm{H}_{Z_{0}}^{-1} \stackrel{(8)}{=}\left(\mathrm{R}+\frac{1}{Z_{i}} \mathbf{e}_{3}^{\top}\right)^{-1}\left(\mathrm{R}+\frac{1}{Z_{0}} \mathbf{e}_{3}^{\top}\right)</script><p>其中，$\mathbf{e}_{3}$为平面法向量，因为与 EV 相机平面垂直，所以为$(0, 0, 1)$，在后面的公式化简中有用到。</p><p>令$\left(C<em>{x}, C</em>{y}, C<em>{z}\right)^{\top} \doteq \mathbf{C}=-R^{\top} \mathbf{t}$，通过展开、以及利用$\mathbf{e}</em>{3}$性质，</p><script type="math/tex; mode=display">\mathrm{H}_{Z_{i}} \mathrm{H}_{Z_{0}}^{-1} \sim \mathrm{I}+\frac{Z_{0}-Z_{i}}{Z_{0}\left(Z_{i}-C_{z}\right)} \mathbf{C e}_{3}^{\top}</script><p>进一步代入上面几个平面的关系，化简得：</p><script type="math/tex; mode=display">\begin{array}{l}x\left(Z_{i}\right)=\frac{Z_{0}}{Z_{i}} \delta x\left(Z_{0}\right)+\frac{1}{Z_{i}}(1-\delta) C_{x} \\y\left(Z_{i}\right)=\frac{Z_{0}}{Z_{i}} \delta y\left(Z_{0}\right)+\frac{1}{Z_{i}}(1-\delta) C_{y}\end{array}</script><p>其中$\delta=\left(Z<em>{i}-Z</em>{0}\right) /\left(Z<em>{0}-C</em>{z}\right)$。</p><p>至此，计算效率就提高上来了。</p><p>算法流程：</p><p><img src="/assets/image-20200614203043203.png" srcset="/img/loading.gif" alt="image-20200614203043203" style="zoom:50%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MVS</tag>
      
      <tag>3D-Vision</tag>
      
      <tag>Event-Camera</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
