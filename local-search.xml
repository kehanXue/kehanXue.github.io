<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文阅读 - EMVS: Event-Based Multi-View Stereo</title>
    <link href="/2020/06/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-EMVS/"/>
    <url>/2020/06/10/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-EMVS/</url>
    
    <content type="html"><![CDATA[<p>EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time 描述了一种利用 Event Camera 的高速特性，提出的一种 MVS 方法。将传统 MVS 的 Space-Sweep 方法应用到 Event-based MVS 中。<br><a id="more"></a></p><h1 id="EMVS-Event-Based-Multi-View-Stereo—3D-Reconstruction-with-an-Event-Camera-in-Real-Time"><a href="#EMVS-Event-Based-Multi-View-Stereo—3D-Reconstruction-with-an-Event-Camera-in-Real-Time" class="headerlink" title="EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time"></a>EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time</h1><blockquote><p>Rebecq, H., Gallego, G., Mueggler, E., Scaramuzza, D.,<br><em><a href="https://doi.org/10.1007/s11263-017-1050-6" target="_blank" rel="noopener">EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time</a></em>,<br>Int. J. of Computer Vision (IJCV), 126(12):1394-1414, 2018. <a href="http://rpg.ifi.uzh.ch/docs/IJCV17_Rebecq.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://youtu.be/EFpZcpd9XJ0" target="_blank" rel="noopener">YouTube</a>, <a href="https://github.com/uzh-rpg/rpg_emvs" target="_blank" rel="noopener">Code</a>.</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇 paper 主要描述了一种利用 Event Camera 的高速特性，提出的一种 MVS 方法。将传统 MVS 的 Space-Sweep 方法应用到 Event-based MVS 中；在 Event-based 场景运用 Space-Sweep 具有天然的优势：</p><ol><li><p>Event Camera 本来就是检测的场景中的 edge。</p></li><li><p>Event Camera 相比于固定帧率的传统 Camera，viewpoints 更加的稠密。</p><p><img src="/assets/image-20200614164507222.jpeg" srcset="/img/loading.gif" alt="image-20200614164507222" style="zoom:50%;" /></p></li></ol><p>文章的整体思路简而言之，就是将“每一帧”的 Events 进行反投影得到了一束光线，通过计算经过 DSI 空间中每一个 voxel 的光线的数量，高于一个阈值即可确定为一个空间的 3D 点。具体实施起来，分为下文中的五步。</p><p>完整的公式推导见原论文。</p><h2 id="主要过程"><a href="#主要过程" class="headerlink" title="主要过程"></a>主要过程</h2><h3 id="Feature-Viewing-Rays-by-Event-Back-Projection"><a href="#Feature-Viewing-Rays-by-Event-Back-Projection" class="headerlink" title="Feature-Viewing Rays by Event Back-Projection"></a>Feature-Viewing Rays by Event Back-Projection</h3><p>将每一个 Event 进行反投影，得到一条光线。</p><blockquote><p>This higher abundance of measurements and viewpoints in the event-based setting generates many more viewing rays than in frame-based MVS, and therefore, it facilitates the detection of scene points by analyzing the regions of high ray density.</p><p>A major advantage of our method is that no explicit data association is needed.</p></blockquote><p>不需要 Event 之间的数据关联、不需要 intensity 信息。</p><h3 id="Volumetric-Ray-Counting-Creating-the-Disparity-Space-Image-DSI"><a href="#Volumetric-Ray-Counting-Creating-the-Disparity-Space-Image-DSI" class="headerlink" title="Volumetric Ray Counting. Creating the Disparity Space Image (DSI)"></a>Volumetric Ray Counting. Creating the Disparity Space Image (DSI)</h3><p>在 DSI 中进行计数。这个时候问题就来了，如何确定这个 DSI 空间呢？</p><p>作者考虑到 <code>the reconstruction of large scenes in a scalable way</code>，采用了一个分批的方式，将一组 Event 归位一批，选取一个虚拟帧 RV，基于它做一个 <code>local 3D reconstruction</code>。</p><p><img src="/assets/image-20200614172203728.jpeg" srcset="/img/loading.gif" alt="image-20200614172203728" style="zoom:50%;" /></p><h3 id="Detection-of-Scene-Structure-by-Maximization-of-Ray-Density"><a href="#Detection-of-Scene-Structure-by-Maximization-of-Ray-Density" class="headerlink" title="Detection of Scene Structure by Maximization of Ray Density"></a>Detection of Scene Structure by Maximization of Ray Density</h3><p>Local DSI 建立好之后，就卡一个阈值，是一个 <code>local maxima</code> 的操作。一个DSI的实例以及使用 <code>local maxima</code> 而不是 <code>global maxima</code> 的原因如下：</p><p><img src="/assets/image-20200614172541662.jpeg" srcset="/img/loading.gif" alt="image-20200614172541662" style="zoom:50%;" /></p><p>作者使用了一个 Adaptive Gaussian 阈值函数：</p><blockquote><p>a pixel $(x, y)$ is selected if $c(x, y) &gt; T(x, y)$, with $T(x, y)=c(x, y) * G \sigma(x, y)-C$. In practice, we use a 5×5 neighborhood in $G \sigma(x, y)$ and $C$ = −10</p></blockquote><p>从 DSI 中提取结构：</p><p><img src="/assets/image-20200614174118964.jpeg" srcset="/img/loading.gif" alt="image-20200614174118964" style="zoom:50%;" /></p><h3 id="Merging-Depth-Maps-from-Multiple-Reference-Viewpoints"><a href="#Merging-Depth-Maps-from-Multiple-Reference-Viewpoints" class="headerlink" title="Merging Depth Maps from Multiple Reference Viewpoints"></a>Merging Depth Maps from Multiple Reference Viewpoints</h3><p>既然有了上面提到的 Local reconstruction，那么就肯定有 key frame 的概念和一次 local 的大小的确定。</p><blockquote><p>we select a new <em>key</em> reference view as soon as the distance to the previous <em>key</em> reference view exceeds a certain percent- age of the mean scene depth (typically a number between 15 and 40%), and use the subset of events until the next <em>key</em> ref- erence view to estimate the corresponding semi-dense depth map of the scene.</p></blockquote><p>然后再做滤波去除一些噪声。融合的话，有了每组 local 点云的 pose ，有很多的方法就可以选了。</p><h3 id="Map-Cleaning"><a href="#Map-Cleaning" class="headerlink" title="Map Cleaning"></a>Map Cleaning</h3><p>为了得到更好的效果，又进一步做了滤波。</p><blockquote><p>we use a median filter on the semi-dense depth maps</p><p>we also apply a radius filter (Rusu and Cousins 2011) to the final point cloud, which discards the points whose number of neighbors within a given radius is less than a threshold.</p></blockquote><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><h3 id="实现实时、高效的-Event-反投影到-DSI-中"><a href="#实现实时、高效的-Event-反投影到-DSI-中" class="headerlink" title="实现实时、高效的 Event 反投影到 DSI 中"></a>实现实时、高效的 Event 反投影到 DSI 中</h3><p>主要思路：DSI 是由不同 depth 的平面们组成的，也就是说明只有 translation，无 rotation。那么，求出一个相机的当前帧与一个深度为 $Z_0$ 的 plane 之间的 homography，与其他 DSI 中的深度为 $Z_i$ 的 plane 的 homography 就可以很简单的来计算，可以推导为一步公式。</p><p>相机平面、EV 虚拟相机平面、不同深度 $Zi$ DSI 平面之间的关系：</p><script type="math/tex; mode=display">\left(x\left(Z_{0}\right), y\left(Z_{0}\right), 1\right)^{\top} \sim_{\mathrm{H} Z_{0}}(u, v, 1)^{\top}</script><script type="math/tex; mode=display">\left(x\left(Z_{i}\right), y\left(Z_{i}\right), 1\right)^{\top} \sim H_{Z_{i}} H_{Z_{0}}^{-1}\left(x\left(Z_{0}\right), y\left(Z_{0}\right), 1\right)^{\top}</script><script type="math/tex; mode=display">\mathrm{H}_{Z_{i}} \mathrm{H}_{Z_{0}}^{-1} \stackrel{(8)}{=}\left(\mathrm{R}+\frac{1}{Z_{i}} \mathbf{e}_{3}^{\top}\right)^{-1}\left(\mathrm{R}+\frac{1}{Z_{0}} \mathbf{e}_{3}^{\top}\right)</script><p>其中，$\mathbf{e}_{3}$为平面法向量，因为与 EV 相机平面垂直，所以为$(0, 0, 1)$，在后面的公式化简中有用到。</p><p>令$\left(C<em>{x}, C</em>{y}, C<em>{z}\right)^{\top} \doteq \mathbf{C}=-R^{\top} \mathbf{t}$，通过展开、以及利用$\mathbf{e}</em>{3}$性质，</p><script type="math/tex; mode=display">\mathrm{H}_{Z_{i}} \mathrm{H}_{Z_{0}}^{-1} \sim \mathrm{I}+\frac{Z_{0}-Z_{i}}{Z_{0}\left(Z_{i}-C_{z}\right)} \mathbf{C e}_{3}^{\top}</script><p>进一步代入上面几个平面的关系，化简得：</p><script type="math/tex; mode=display">\begin{array}{l}x\left(Z_{i}\right)=\frac{Z_{0}}{Z_{i}} \delta x\left(Z_{0}\right)+\frac{1}{Z_{i}}(1-\delta) C_{x} \\y\left(Z_{i}\right)=\frac{Z_{0}}{Z_{i}} \delta y\left(Z_{0}\right)+\frac{1}{Z_{i}}(1-\delta) C_{y}\end{array}</script><p>其中$\delta=\left(Z<em>{i}-Z</em>{0}\right) /\left(Z<em>{0}-C</em>{z}\right)$。</p><p>至此，计算效率就提高上来了。</p><p>算法流程：</p><p><img src="/assets/image-20200614203043203.jpeg" srcset="/img/loading.gif" alt="image-20200614203043203" style="zoom:50%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>3D-Vision</tag>
      
      <tag>MVS</tag>
      
      <tag>3D-Reconstruction</tag>
      
      <tag>Event-Camera</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CeleX5-ROS: CeleX5-MIPI 事件相机的一个完善的ROS工具包</title>
    <link href="/2020/03/10/CeleX5-ROS-CeleX5-MIPI-%E4%BA%8B%E4%BB%B6%E7%9B%B8%E6%9C%BA%E7%9A%84%E4%B8%80%E4%B8%AA%E5%AE%8C%E5%96%84%E7%9A%84ROS%E5%B7%A5%E5%85%B7%E5%8C%85/"/>
    <url>/2020/03/10/CeleX5-ROS-CeleX5-MIPI-%E4%BA%8B%E4%BB%B6%E7%9B%B8%E6%9C%BA%E7%9A%84%E4%B8%80%E4%B8%AA%E5%AE%8C%E5%96%84%E7%9A%84ROS%E5%B7%A5%E5%85%B7%E5%8C%85/</url>
    
    <content type="html"><![CDATA[<p>我在前段时间为 CeleX5-MIPI 系列相机开发了一套完善的 ROS 下的驱动。CeleX5-MIPI是一款国产的事件相机，具有1280x800的分辨率。项目已开源在：<a href="https://github.com/kehanXue/CeleX5-ROS" target="_blank" rel="noopener">https://github.com/kehanXue/CeleX5-ROS</a> 。</p><a id="more"></a><blockquote><p>！本篇是项目在 Github 的中文版 README.md，所以本文中的链接是相对链接，这将导致它们在本文中不可用。请移步项目的<a href="https://github.com/kehanXue/CeleX5-ROS" target="_blank" rel="noopener">地址</a>，非常感谢！</p></blockquote><p><em><a href="https://github.com/kehanXue/CeleX5-ROS" target="_blank" rel="noopener">English Version</a></em></p><h1 id="CeleX5-ROS"><a href="#CeleX5-ROS" class="headerlink" title="CeleX5-ROS"></a>CeleX5-ROS</h1><blockquote><p> The ROS packages for CeleX™ CeleX5-MIPI Dynamic Vision Sensor.</p></blockquote><p>本仓库提供了CeleX5-MIPI相机在ROS下多个功能包和示例，包括有：</p><ul><li><code>celex5_ros</code>：提供了CeleX5-MIPI相机的ROS下的较为完善驱动。</li><li><code>celex5_msgs</code>：为CeleX5-MIPI相机定制的ROS消息类型。</li><li><code>celex5_calibration</code>：提供了有关于<code>celex5_calibration</code>基于Events数据进行标定相关的工具和教程。</li></ul><p><strong><em>本仓库代码主要遵从<a href="https://google.github.io/styleguide/cppguide.html" target="_blank" rel="noopener">Google C++ 编程风格</a></em></strong></p><p><a href="#介绍">介绍</a></p><p><a href="#概要">概要</a></p><p><a href="#编译与运行">编译与运行</a></p><p><a href="#仍存在的问题">仍存在的问题</a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><img src="assets/242069421.jpg" srcset="/img/loading.gif" height="300" alt="CeleX5-MIPI"/></p><p>CeleX™ 是针对机器视觉而设计的智能图像传感器系列。传感器中的每一像素点能够独立自主地监测相对光强的变化，并在到达阈值时被激发发出被读出信号。行和列仲裁电路实时处理像素激发信号，并确保即使同时接收到多个请求时能够按有序的方式逐一处理。传感器依据被激发的事件，输出连续的异步数据流,而不是图像帧。CeleX™ 传感器监测的运动物体速度不再受传统的曝光时间和帧速率限制。它可以侦测高达万帧/秒昂贵高速相机才能获取到的高速物体运动信息，而且还能大幅降低后端处理量。</p><p>CeleX-5 是一款多功能智能图像传感器，具有一百万像素（分辨率为：1280*800）和片上集成的一些附加功能（如光流）。传感器支持几种不同的输出格式:纯二进制地址事件,具有像素强度信息或定时信息的地址事件。此外,传感器的读出方案可以是异步数据流或同步全帧。输出格式和读出方案的不同组合使该传感器具有很大的灵活性,总共支持 6 种独立的操作模式（但是其中有一种未在sdk中提供相应接口）。为了进一步满足不同应用的要求,传感器还可以配置为 Loop 模式,可以在三种不同模式之间自动切换。</p><p><a href="https://www.ros.org/" target="_blank" rel="noopener">ROS</a>是目前很主流的实验平台，提供了丰富的开发接口和资源。但在<a href="https://github.com/CelePixel/CeleX5-MIPI" target="_blank" rel="noopener">CeleX™官方开源仓库</a>中，关于ROS下的示例非常的不完善：</p><ul><li><p>官方的SDK版本已经更新到了v2.0版本，然而其ROS-Sample支持的版本仍停留在v1.6版本。</p></li><li><p>官方的ROS-Sample仅为一个简单的示例，只输出一个工作模式下的一种图像，未提供全面且方便的调参功能和界面。</p></li></ul><p><strong><em>因此，为了更为方便的利用ROS提供的资源，我开发了这个仓库中的内容。</em></strong></p><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>本仓库提供了CeleX5-MIPI相机在ROS下多个功能包和示例，包括有：</p><ul><li><p><code>celex5_ros</code>：提供了CeleX5-MIPI相机的ROS下的较为完善驱动，可根据用户需求自由配置输出多路数据（原始Event数据、IMU数据、灰度帧、光流信息等），并提供了<code>rqt_reconfigure</code>调参面板以支持动态调参。<a href="celex5_ros">更多细节</a>。</p><p><img src="assets/Screenshot from 2020-01-22 21-16-52.png" srcset="/img/loading.gif" width="800" /></p><p><em>目前仅仅在CeleX5-MIPI设备上测试通过，由于手里没有CeleX5系列的其他产品所以无法测试。</em></p></li><li><p><code>celex5_msgs</code>：为CeleX5-MIPI相机定制的ROS消息类型。</p></li><li><p><code>celex5_calibration</code>：提供了有关于<code>celex5_calibration</code>基于Events数据进行标定相关的工具和教程，包括标定板生成（目前仅支持闪烁的棋盘格，其他类型待加入）、内参标定、与传统相机进行外参标定（使用Kalibr）、同步收集与定制发布标定所需的图像数据、与传统相机进行时间戳对齐（TODO，目前存在问题）等工具。<a href="calibration">更多细节</a></p><p><img src="assets/Screenshot from 2020-02-12 21-02-32.png" srcset="/img/loading.gif" width="600" /></p></li></ul><p><em>建议在使用前，请首先认真阅读<a href="https://github.com/CelePixel/CeleX5-MIPI/tree/master/Documentation" target="_blank" rel="noopener">CeleX官方提供的</a>快速入门手册和API文档的概念部分，对基本名词术语有大概认识。</em></p><h2 id="编译与运行"><a href="#编译与运行" class="headerlink" title="编译与运行"></a>编译与运行</h2><ol><li><p>编译</p><pre><code class="hljs bash">mkdir -p ~/celex_ws/src<span class="hljs-built_in">cd</span> ~/celex_ws/srcgit <span class="hljs-built_in">clone</span> git@github.com:kehanXue/CeleX5-ROS.gitgit submodule update --init --recursive<span class="hljs-comment"># Or with http: `git clone https://github.com/kehanXue/CeleX5-ROS.git`</span><span class="hljs-built_in">cd</span> ..rosdep install -y --from-paths src --ignore-src --rosdistro <span class="hljs-variable">$ROS_DISTRO</span>catkin_make <span class="hljs-comment"># Or use `catkin build`</span></code></pre><p>如果过程中报少哪些依赖库的错误，安装即可。</p></li><li><p>运行<code>celex5_ros</code> <a href="celex5_ros">更多细节</a></p><p>首先将相机连接到电脑上。</p><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/celex_ws/devel/setup.bash <span class="hljs-comment"># Or source setup.zsh when you use zsh</span>roslaunch celex5_ros celex5_ros_node.launch<span class="hljs-comment"># In a new Terminal</span>rosrun rqt_reconfigure rqt_reconfigure <span class="hljs-comment"># Open rqt_reconfigure to config</span></code></pre><p>无报错即正常运行。使用<code>rostopic list</code>即可看到发布的话题，使用<code>rivz</code>或者<code>image_view</code>订阅相对应图像话题即可看到发布的图像。某图像话题中是否发布数据由参数配置、上表的相机工作模式所共同决定。</p><pre><code class="hljs bash">$ rostopic list/celex5_mipi/display/accumulated_img/camera_info/celex5_mipi/display/accumulated_img/raw_image/celex5_mipi/display/binary_img/camera_info/celex5_mipi/display/binary_img/raw_image/celex5_mipi/display/count_img/camera_info/celex5_mipi/display/count_img/raw_image/celex5_mipi/display/denoised_binary_img/camera_info/celex5_mipi/display/denoised_binary_img/raw_image/celex5_mipi/display/full_frame_img/camera_info/celex5_mipi/display/full_frame_img/raw_image/celex5_mipi/display/gray_img/camera_info/celex5_mipi/display/gray_img/raw_image/celex5_mipi/display/in_pixel_img/camera_info/celex5_mipi/display/in_pixel_img/raw_image/celex5_mipi/display/optical_flow_direction_img/camera_info/celex5_mipi/display/optical_flow_direction_img/raw_image/celex5_mipi/display/optical_flow_img/camera_info/celex5_mipi/display/optical_flow_img/raw_image/celex5_mipi/display/optical_flow_speed_img/camera_info/celex5_mipi/display/optical_flow_speed_img/raw_image/celex5_mipi/display/parameter_descriptions/celex5_mipi/display/parameter_updates/celex5_mipi/display/superimposed_img/camera_info/celex5_mipi/display/superimposed_img/raw_image/celex5_mipi/events/celex5_mipi/imu_data/celex5_mipi/polarity_img/camera_info/celex5_mipi/polarity_img/raw_image/celex5_mipi/sensor/parameter_descriptions/celex5_mipi/sensor/parameter_updates/rosout/rosout_agg</code></pre><p>其中原始event数据的话题：<code>/celex5_mipi/events</code></p><p>imu数据的话题（注意由于CeleX5原始imu数据获取的方式，导致imu话题与ROS中标准的IMU话题的消息类型不同）：<code>/celex5_mipi/imu_data</code></p><p>通过<code>image_view</code>查看某一图像：</p><pre><code class="hljs bash">rosrun image_view image_view image:=/celex5_mipi/display/binary_img/raw_image</code></pre></li></ol><ol><li><p>使用<code>celex5_calibration</code></p><p>更多细节：<a href="celex5_calibration">链接</a></p><p>提供了一系列基于Events数据进行相机参数标定的办法。</p><ul><li><p>标定工具的生成</p><p>运行：</p><pre><code class="hljs bash">rosrun celex5_ros pattern_generator_node<span class="hljs-comment"># In a new Terminal</span>rosrun rqt_reconfigure rqt_reconfigure <span class="hljs-comment">#  Open rqt_reconfigure to config</span></code></pre><p>更多细节：<a href="celex5_calibration/src/pattern">链接</a></p></li><li><p>基于ROS中的<a href="http://wiki.ros.org/camera_calibration/Tutorials" target="_blank" rel="noopener">camera_calibration</a>工具包进行的内参标定。（如果要标定与其他相机的外参，ROS中的这个工具仅支持分辨率一样的…）</p><p>安装：</p><pre><code class="hljs bash">sudo apt install ros-<span class="hljs-variable">$ROS_DISTRO</span>-camera-calibration</code></pre><p>CeleX5-MIPI进行基于该方法进行标定的具体过程：<a href="celex5_calibration/src/intrinsics_extrinsics/pkg_camera_calibration">链接</a></p></li><li><p>基于<a href="https://github.com/ethz-asl/kalibr" target="_blank" rel="noopener">Kalibr</a>的标定，支持同时进行内参、与传统相机的外参标定。基于Event数据。</p><p>更多细节：<a href="celex5_calibration/src/intrinsics_extrinsics/kalibr">链接</a></p><p>同时提供了收集标定数据的一系列工具。</p></li><li><p>与其他相机进行时间戳的标定。</p><p><em>TODO</em>，目前仍存在问题。<a href="celex5_calibration/src/temporal">链接</a></p></li></ul></li></ol><h2 id="仍存在问题"><a href="#仍存在问题" class="headerlink" title="仍存在问题"></a>仍存在问题</h2><p><strong>celex5_ros</strong></p><ul><li>实现的<a href="http://wiki.ros.org/nodelet" target="_blank" rel="noopener">Nodelet</a>接口仍存在问题（不过ros node版本是完全正常工作的）。在通过nodelet加载相机的参数文件的过程中，解析xml文件会出现乱码。调了好久都没找到问题，如果大家提供解决建议的话我会非常感谢。</li><li>关于文档中的<code>Multi_Read_Optical_Flow_Mode</code>模式，未在SDK中找到相关接口。// TODO</li><li>暂未添加生成FPN的功能，请仍使用CeleX官方提供的GUI Demo进行FPN的生成（其Linux下的Demo运行可能会直接报段错误，Windows下的较为稳定一些）。</li><li>暂未提供CeleX SDK里的有关录制bin文件的功能，但ROS下我们可以使用ROS bag来进行录制。</li></ul><p><strong>celex5_calibration</strong></p><ul><li>关于与其他相机进行时间戳标定的相关功能还未完成。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Event-Camera</tag>
      
      <tag>ROS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读 - Low-Latency Visual Odometry using Event-based Feature Tracks</title>
    <link href="/2020/03/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Low-Latency-Visual-Odometry-using-Event-based-Feature-Tracks/"/>
    <url>/2020/03/04/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Low-Latency-Visual-Odometry-using-Event-based-Feature-Tracks/</url>
    
    <content type="html"><![CDATA[<p>Low-Latency Visual Odometry using Event-based Feature Tracks 通过手工设计的feature和feature tracking的方法，将问题转化为传统VO，但又不失Event带来的高速和异步的特性。</p><a id="more"></a><h1 id="Low-Latency-Visual-Odometry-using-Event-based-Feature-Tracks"><a href="#Low-Latency-Visual-Odometry-using-Event-based-Feature-Tracks" class="headerlink" title="Low-Latency Visual Odometry using Event-based Feature Tracks"></a>Low-Latency Visual Odometry using Event-based Feature Tracks</h1><blockquote><p>Kueng, B., Mueggler, E., Gallego, G., Scaramuzza, D.,<br><a href="https://doi.org/10.1109/IROS.2016.7758089" target="_blank" rel="noopener">Low-Latency Visual Odometry using Event-based Feature Tracks</a>,<br>IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS), 2016, pp. 16-23. <a href="https://www.doc.ic.ac.uk/~ajd/Publications/kim_etal_eccv2016.pdf" target="_blank" rel="noopener">PDF</a>. <a href="https://youtu.be/RDu5eldW8i8" target="_blank" rel="noopener">YouTube</a></p></blockquote><h2 id="本篇概要"><a href="#本篇概要" class="headerlink" title="本篇概要"></a>本篇概要</h2><p>通过手工设计的feature和feature tracking的方法，将问题转化为传统VO，但又不失Event带来的高速和异步的特性。</p><p><img src="assets/Screenshot from 2020-02-26 16-48-57.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>使用DAVIS，跟踪event-based feature从而实现一个低延迟的VO。</p><p>A lowlatency visual odometry algorithm for the DAVIS sensor using event-based feature tracks. </p><p><img src="assets/Screenshot from 2020-03-02 17-21-27.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><h2 id="主要过程"><a href="#主要过程" class="headerlink" title="主要过程"></a>主要过程</h2><p>首先在DAVIS的灰度帧中检测feature，然后利用event流进行异步的跟踪。然后这些特征被输入到VO算法中，通过最小化投影误差计算场景的local probabilistic 3D map以及做6-DoF的位姿估计。位姿估计是event-based的，所以具有低延迟的特性。</p><p><img src="assets/Screenshot from 2020-02-26 17-51-41.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><h2 id="对比已有工作"><a href="#对比已有工作" class="headerlink" title="对比已有工作"></a>对比已有工作</h2><p>None of the previous <strong>event-based motion estimation</strong> methods is <strong>based on tracking complex, natural features in the event stream</strong>. </p><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><ul><li><p><strong>特征提取与跟踪部分</strong></p><p><em>算法整体流程：</em></p><p><img src="assets/Screenshot from 2020-03-02 17-43-08.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p><em>详细说明：</em></p><ul><li>通过灰度Frames进行特征提取。算法流程：</li></ul><ol><li>通过Canny边缘提取和Harris角点检测。<ol><li>在最突出、分布均匀的角点周围，再其edge图上选取一个一定大小的patch。Frame的帧率并不需要固定，这些相当于是提供初始的特征；并且在特征点丢失的情况下再补充特征点。如Fig.4的(a)-&gt;(b)图示意。这个patch中的边缘点的集合称为<strong>model point set</strong>。</li></ol></li></ol><ul><li><p>通过Events数据进行feature tracking：</p><ol><li><p>将新产生的Event归到上一步划分的patch中。每组Events patch的里面的Events的数量和上一步检测到的边缘点的数量一致，每个patch集合中的Events称为<strong>data point set</strong>。然后维护一个队列，先进先出。每次每个patch中的Event发生更新后，就单独对更新的patch重新运行一次registration。</p></li><li><p>registration是通过将上述的model point set和data point set通过ICP（3D点推广到2D点）进行配准，从而找出匹配点。这一步相当于是让Events去适配目前的feature。</p><p>一个典型的ICP问题：</p><script type="math/tex; mode=display">\arg \min _{\mathbf{R}, \mathbf{t}}=\sum_{\left(\mathbf{p}_{i}, \mathbf{m}_{i}\right) \in \text { Matches }} b_{i}\left\|\mathbf{Rp}_{i}+\mathbf{t}-\mathbf{m}_{i}\right\|^{2}</script><p>这样来找出Event和Edge点的对应的匹配关系。</p><p><img src="assets/Screenshot from 2020-03-02 17-43-40.png" srcset="/img/loading.gif" style="zoom:100%;" /></p></li><li><p>针对Tracking的一些改进</p><ol><li><p>由于这些Event其实都是边缘特征触发的，所以相当于是有一个结构的约束的。所以看上面的那个求解ICP的公式，那个$b_i$其实相当于加了一个约束，它的大小与data point set中后1/4的Event中落在当前Event周围3*3像素中的个数成正比。</p></li><li><p>为了更好的长期跟踪feature，还通过基于Event累计触发量直方图的方式，通过调整feature的位置来增加长期跟踪的鲁棒性。因为时间长的话，偏移比较大的时候，Events就无法很好的和目前的feature进行ICP了，这个时候就再去调整feature在图片上的二维坐标就OK了。比如针对这个特征，刚开始选择M1数量个Events根据触发数量合成直方图，然后用当前最近的M2个Event合成的直方图，然后定义一个$\mathbf{s}=\left(o<em>{x}, o</em>{y}\right)^{\top}$表示一个范围在$\pm 3$个像素的偏移。然后枚举所有$\mathbf{s}$的值，找出使得下面值最小的$\mathbf{s}$：</p><script type="math/tex; mode=display">d\left(H_{1}, H_{2}, \mathbf{s}\right)=\sum_{\mathbf{x}} \min \left(H_{1}(\mathbf{x}), H_{2}(\mathbf{x}+\mathbf{s})\right)</script><p>如果$\mathbf{s}$大于一个阈值，就把他应用到feature中。M1和M2是大于N（也就是data point set的大小的），并且为了得到一个较好的初始值，M1是可以大于M2的。下图是当M2=5N的时候的直方图情况：</p><p><img src="assets/Screenshot from 2020-03-03 23-59-12.png" srcset="/img/loading.gif" style="zoom:100%;" /></p></li></ol></li></ol></li></ul></li><li><p><strong>VO部分</strong></p><p>有了上面的特征点的部分，那么接下的VO部分其实就是传统的VO了：</p><ol><li><p>通过深度滤波器（depth-filters），恢复出3D场景的结构。</p></li><li><p>通过最小化重投影误差求解出相机的位姿</p><p>有个细节，这一步通过高斯牛顿法(G-N)优化、初值选用上一帧的pose就可以了，因为两帧event之间的运动是非常非常小的，使用G-N的优化速度很快，不需要使用L-M了。</p><script type="math/tex; mode=display">\mathbf{T}_{k}=\underset{\mathbf{T}}{\operatorname{argmin}} \frac{1}{2} \sum_{i} w_{i}\left\|\mathbf{u}_{i}-\pi\left(\mathbf{T}, \mathbf{p}_{i}\right)\right\|^{2}</script><p>$w_i$是鲁棒核函数：</p><script type="math/tex; mode=display">w_{i}=\left\{\begin{array}{ll}\left(1-\frac{x^{2}}{b^{2}}\right)^{2} & |x| \leq|b| \\0 & \text { otherwise }\end{array}\right.</script><p>其中$x=\left|\mathbf{u}<em>{i}-\pi\left(\mathbf{T}, \mathbf{p}</em>{i}\right)\right|$，$b=5$ pixels。</p></li></ol><p>和SVO中的做法是一样的。</p><p>具体细节就不再详述了。</p></li></ul><h2 id="对比与实验"><a href="#对比与实验" class="headerlink" title="对比与实验"></a>对比与实验</h2><p>作者做了Feature跟踪性能的分析、VO精度的对比分析、系统运行时间消耗的分析。详细的实验数据见原论文。</p><ul><li><p>Feature跟踪性能的分析：</p><p>patch大小为19*19时，</p><ul><li><p>在checkerboard-like的场景中跟踪8秒，跟踪错误的平均误差为1.5pixels；在自然场景中跟踪6秒，跟踪错误的平均误差为2.5pixels。</p></li><li><p>一个特征被跟踪的生命周期，经过直方图refine之后：</p><p><img src="assets/Screenshot from 2020-03-04 00-55-19.png" srcset="/img/loading.gif" style="zoom:80%;" /></p></li></ul></li><li><p>VO性能分析：</p><p><em>细节：当前跟踪的feature数量维持在120个左右的时候比较合适；当小于100的时候效果较差。</em></p><p>Event-based VO与Ground Truth、Frame-based VO (SVO)三者的对比。可以看出，在精度上还是比传统VO（SVO）稍差。</p><p><img src="assets/Screenshot from 2020-03-04 00-58-17.png" srcset="/img/loading.gif" style="zoom:100%;" /></p></li><li><p>运行时间分析：</p><p>在Core i7-4710MQ CPU @ 2.50GHz with 8GB RAM配置的机器上、C++单线程实现的情况下，events的处理速度平均为160kevents/s。</p><p><img src="assets/Screenshot from 2020-03-04 01-01-53.png" srcset="/img/loading.gif" style="zoom:80%;" /></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>3D-Vision</tag>
      
      <tag>Event-Camera</tag>
      
      <tag>Odomerty</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LOAM（A-LOAM）Laser-SLAM 算法流程</title>
    <link href="/2020/02/26/LOAM%EF%BC%88A-LOAM%EF%BC%89Laser-SLAM-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B/"/>
    <url>/2020/02/26/LOAM%EF%BC%88A-LOAM%EF%BC%89Laser-SLAM-%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p>loam 是一个三维激光 SLAM 中很重要的一个方案，俗称 “loam 神器”。在这篇文章中我梳理了 loam 的算法流程，代码部分参考的是港科大 HKUST-Aerial-Robotics 组的实现版本：A-LOAM。</p><a id="more"></a><h1 id="LOAM简介"><a href="#LOAM简介" class="headerlink" title="LOAM简介"></a>LOAM简介</h1><p>Paper:</p><blockquote><p>J. Zhang and S. Singh. LOAM: Lidar Odometry and Mapping in Real-time.<br>Robotics: Science and Systems Conference (RSS). Berkeley, CA, July 2014. <a href="">PDF</a></p></blockquote><p>loam的开源代码版本有很多…作者自己开源出来的代码版本说实话有点惨不忍睹…但是坊间也相应就有了很多修改版本。比如 <a href="https://github.com/HKUST-Aerial-Robotics/A-LOAM" target="_blank" rel="noopener">1</a>、<a href="https://github.com/daobilige-su/loam_velodyne" target="_blank" rel="noopener">2</a>等等，相对好了一些但还是不甚符合软件设计的思想…</p><p>本篇文章对应的loam源码采用的是港科大<a href="https://github.com/HKUST-Aerial-Robotics" target="_blank" rel="noopener">HKUST-Aerial-Robotics</a>组的实现版本：<a href="https://github.com/HKUST-Aerial-Robotics/A-LOAM" target="_blank" rel="noopener">A-LOAM</a>。该版本采用的是通过Ceres-solver的自动求导来做非线性优化的，所以从优化部分来看相对于源代码省去了优化过程中实现的公式推导的流程，相对简介了一些；但是另一方面来看代码中还是存在不少冗余的部分…</p><p>贴一张<strong>A-LOAM</strong>在kitti数据集上的效果图 (来源A-LOAM github主页)：</p><p><img src="assets/kitti.png" srcset="/img/loading.gif" alt="img"></p><p>参考博客：<a href="https://zhuanlan.zhihu.com/p/57351961" target="_blank" rel="noopener">1</a> <a href="https://blog.csdn.net/Nksjc/article/details/76401092" target="_blank" rel="noopener">2</a></p><h1 id="LOAM流程"><a href="#LOAM流程" class="headerlink" title="LOAM流程"></a>LOAM流程</h1><p>loam的一个很重要的思想，就是激光雷达获取到的一帧点云，由于高速运动产生的畸变，会对激光里程计的结果准确性造成很大的影响（所以代码中存在很多的空间3D点位姿变换的部分）；</p><ul><li>这里的畸变是指一帧点云数据的产生点不在同一个位置；而一般算法默认一帧点云作为一次的观测数据，是在同一个点观测到的。目前大多的激光雷达还都是机械式的激光，扫描一圈是需要时间的，虽然一帧的时间很短，但是在高速运动过程中，这个畸变的问题就凸显出来了。</li></ul><p>loam的主要流程就是，首先对点云提取特征；然后通过当前帧与上一帧进行特征匹配得到高频低精度的odometry信息，这个过程中加入运动补偿来修复畸变；通过间隔n帧与当前地图进行特征匹配得到低频高精度的odometry的信息（这一步主要是计算量大的限制而选择低频）；然后通过上述两者之间的相互插值最后得到一个不错的激光里程计结果。而建图是通过估计出的每一帧的位姿，通过将多帧的激光特征点云基于位姿拼接，形成特征点云地图。</p><p>首先，LOAM的代码流程主要由下面四个部分组成：</p><pre><code class="hljs mermaid">graph TDA(Scan Registration)--&gt;B(Low-precision, High-frequency Laser Odometry)B--&gt;C(High-precision, Low-frequency Laser Mapping)C--&gt;D(Frequency boost-up and more accurate Odometry)</code></pre><h2 id="Scan-Registration"><a href="#Scan-Registration" class="headerlink" title="Scan Registration"></a>Scan Registration</h2><ul><li><p>首先根据激光雷达的扫描模式将点云进行归档，比如若采用vlp-16雷达的话，将一帧的点云(Sweep)所有点对应到16条线(scan)上。</p><p>这部分的代码对应<code>scanRegistration.cpp</code>的 第<code>160-241</code>行for循环的内容。其中有一些实现的小细节：</p><ul><li><p>关于判断某一点是位于哪一条线上，这个是根据雷达的硬件扫描模式而定的（所以如果要实现基于不同雷达硬件的loam的话，这部分是要替换一下的哈，loam目前也有实现的很多的不同的版本，比如loam_back_and_forth、loam_continuous、loam_velodyne、loam_livox等等）。比如vlp16的话就要去查看vlp16的用户手册里面的参数（垂直分辨率、水平分辨率、扫描周期等参数）。所以说代码中的第<code>169-205</code>行里的参数就是根据vlp16雷达的用户手册得到的。关于这个，这个知乎<a href="https://zhuanlan.zhihu.com/p/57351961" target="_blank" rel="noopener">链接</a>的 <strong>5.1节线束模型</strong> 里讲得比较详细。</p></li><li><p>里面还有一个关于处理点云顺序的小细节，就是看到的那些加减2$\pi$的处理。</p><p>首先，默认每次雷达扫描的开始都是从0°开始的，对于水平角度，每个角度对应一个扫描的相对时间， 这在后续的运动补偿中用来去掉点云的畸变。但是雷达里的电机一直处于高速旋转中，没有一个复位的过程，雷达只在电机旋转到接近0°时记下当前对应激光点的精确坐标。同样，结束的位置，也不一定时0°，都是在0°周围。所谓这时候要进行归一化到一个2$\pi$的区间。对应代码<code>208-236</code>。</p></li></ul></li><li><p>第二步进行曲率计算，为下一步特征提取做准备。这一步的做法是选取了前后各5个点分别与自己计算距离并累加。这部分的对应代码<code>scanRegistration.cpp</code>的 第<code>256-266</code>行for循环的内容。</p></li><li><p>第三步开始选取特征点云并合成特征点云。为了使得约束均匀分布，沿水平方向平均分为几块点云区域。将块内的点按曲率大小排列，设置一个曲率阈值t， 比如0.1，来区分边缘点和平面点。我们设定一个每块的最大点数N。</p><p>边缘点sharp选择条件:</p><ul><li><p>从曲率最大的点开始,最多选择N个, 只有曲率大于t的点才能被选取</p></li><li><p>若一个点周围五个点中已有点被选为边缘点,跳过这个点, 从曲率更小的点中选取</p></li></ul><p>平面点flat选择条件:</p><ul><li><p>从曲率最小的点开始,最多选择N个, 只有曲率小于t的点才能被选取</p></li><li><p>若一个点周围五个点中已有点被选为平面点,跳过这个点, 从曲率更大的点中选取</p></li></ul><p>上述的具体参数设置见代码和论文。</p><p>特征选取的过程中还处理了下面两种情况：</p><p><img src="assets/Screenshot from 2020-02-25 23-39-46.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p>最后将选出的特征集合成特征点云。在实现的过程中，针对sharp和flat分别提出了两组点云cornerPointsSharp/cornerPointsLessSharp、surfPointsFlat/surfPointsLessFlat。其实less就相当于是N值取的较大一些，也就是多了一些sharp/flat程度低一些的点。然后在匹配的过程中，是通过当前的cornerPointsSharp/surfPointsFlat去匹配less的，也就是少数去匹配多数。这个考虑其实特别好，因为在不同位置观测到的曲率是不一样的，增加了系统的鲁棒性。</p><p>上述的代码实现在<code>scanRegistration.cpp</code>的第<code>282-399</code>。代码中的设置即为分为了6块区域，曲率阈值为0.1。cornerPointsSharp选了3个，cornerPointsLessSharp选了21个。所以一帧16线的点云共提取$16<em>6</em>3=288$个角点。flat的选择有点不太一样，见代码<code>346-398</code>行，其实surfPointsFlat选取了$16<em>6</em>5=480$个角点，然后surfPointsLessFlat把所有曲率满足flat（小于设定的那个阈值）、不是corner的点给塞了进去，然后又加了一个下采样的处理。</p><p>一帧特征点云的效果图（白色为sharp，彩色为flat）：</p><p>cornerPointsSharp和surfPointsFlat：</p><p><img src="assets/Screenshot from 2020-02-26 00-04-20.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p>cornerPointsLessSharp和surfPointsLessFlat：</p><p><img src="assets/Screenshot from 2020-02-26 00-03-38.png" srcset="/img/loading.gif" style="zoom:80%;" /></p></li></ul><p>至此，得到的四组点云将作为高频低精度的laserOdometry的输入。</p><h2 id="Low-precision-High-frequency-Laser-Odometry"><a href="#Low-precision-High-frequency-Laser-Odometry" class="headerlink" title="Low-precision, High-frequency Laser Odometry"></a>Low-precision, High-frequency Laser Odometry</h2><p>这一部分的主要功能有两个：</p><ul><li><p>特征点云的去运动畸变</p><p>根据点云中每个点的扫描时间（存储在intensity中，为SCANID整数部分+time小数部分）、其他方式获取的里程计信息（有imu的话用imu，没的话用匀速运动模型，当然其他的也可以比如轮子里程计），然后将这一帧中所有点转换到起始点的坐标系下。</p></li><li><p>高频低精确度的odometry</p><p>这一步通过当前帧点云与上一帧点云的对齐的实现的。具体算法：</p><ul><li><p>首先进行特征匹配和构建loss。根据论文中的算法：</p><ul><li>找出本帧corner特征与上一帧的最近邻且不在一条扫描线上的两个特征点，然后上一帧的两个点组成直线，求出本帧特征点在空间中对于这条直线的距离，构成一项loss。</li><li>找出本帧flat特征，然后找出上一帧最近邻的一个、何其同线最近邻的一个和不在一条线上最近邻的一个，三点确定出一个平面，然后求本帧特征点与其的距离，此项也构成一个loss。</li></ul><p>下图是论文中的示意图（前面的一根线就是一根线SCAN；由于论文采用的是2D雷达+摆动的方式，所以前面的线束长这个样子，对应vlp16就是平行的）。</p><p><img src="assets/Screenshot from 2020-02-25 23-40-40.png" srcset="/img/loading.gif" style="zoom:80%;" /></p></li><li><p>有了这些loss，就可以构建最小二乘优化了。将这些距离对位姿进行求导。loam论文中给出了导数形式，并提出通过LM法进行优化；loam原代码中作者手写了优化过程但是使用的是高斯牛顿…并且论文和代码的被求导对象有点不一样…这个暂就不说了。</p><p>A-loam是拿Ceres-solver的自动求导进行优化的，而没有使用解析导数。所以这些求导过程就可以先略过了。在Ceres-solver中构建基于上面的loss构造方式来构建观测误差即可。</p></li></ul></li></ul><p>至此，由于两帧之间的计算量其实还是比较小的，所以可以做到一个比较高频的odometry（10hz）。但由于只有当前帧与上一帧的联系，所以这个一是精度比较偏低，并且长时间了会有drift。所以单纯由这个计算得到的odometry信息实用性(aloam代码中的<code>/laser_odom_to_init</code>话题)并不是很强，并且我们通常选择使用根据map对齐得到的低频高精度odometry与这个高频低精度odometry两者插值得到的odometry信息，即A-loam代码中的<code>/aft_mapped_to_init_high_frec</code>话题。</p><p>所以，这一步更大的意义是将每一帧内的点云进行去运动畸变，这个给高速运动下精度的提高带来了很大的好处。当然，这些odometry的作用还有如下：</p><ul><li>在laserMapping中用于位姿的预测。</li><li>为laserMapping输出的低频odometry提供插值，以获得高频(10HZ)的odometry。</li></ul><blockquote><p>Ps: less点云中好像未参与去畸变。然后A-LOAM的代码中，后面map匹配的时候，采用的是less点云，map建图过程中，插入的也是less！</p></blockquote><h2 id="High-precision-Low-frequency-Laser-Mapping"><a href="#High-precision-Low-frequency-Laser-Mapping" class="headerlink" title="High-precision, Low-frequency Laser Mapping"></a>High-precision, Low-frequency Laser Mapping</h2><p>这一步就是，如果当前帧被选择用来与已经构建在map中的特征进行匹配的话（由于计算实时性的影响，在计算力达不到的场景下不可能每一帧都进行map的匹配，所以会对这一过程进行一个抽样，比如5帧一匹配），通过这一帧在map中的配准（与上面laserOdometry方法一样，只是去匹配的目标不一样了，这次是匹配的是已经构建在map中的、且在视域范围内（laserCloudValidInd）的立方体中的点。</p><p>这次的匹配得到更加精准的位姿，然后继续根据这一位姿，将后面帧的特征点云继续给加入到map中。</p><p>上面的这两步骤反复进行即可。这样的话就完成了建图（特征地图）和一个低频精准的odometry信息了。</p><p>对于Mapping这一步具体的实现，</p><ul><li>map其实分为corner map(sharp)和surface map(flat)这两种特征的地图。</li><li>有一个空间划分的过程，将新获得的corner和surf激光点分别映射到每个立方体laserCloudCornerArray和laserCloudSurfArray。具体划分的步骤代码写的也太啰嗦了吧…代码<code>laserMapping.cpp</code>文件第<code>323-529</code>行，然后选出用于这一帧匹配的map中的视域内的(按照立方体，其实圆柱形、球形等都可以)划分的点，代码<code>531-537</code>行。</li><li>有了两堆用于匹配的点云，就按照之前步骤匹配就可以了。计算量大的原因是从map中选取的比较多(也就是匹配的target)。</li></ul><h2 id="Frequency-boost-up-and-more-accurate-Odometry"><a href="#Frequency-boost-up-and-more-accurate-Odometry" class="headerlink" title="Frequency boost-up and more accurate Odometry"></a>Frequency boost-up and more accurate Odometry</h2><p>最后一步是将两种高低频率结合起来。这一步是实现其实非常简单，插值就可以了。代码实现在<code>laserMapping.cpp</code>文件第<code>197-229</code>行，即<code>laserOdometryHandler</code>函数。</p>]]></content>
    
    
    <categories>
      
      <category>SLAM 原理与实践</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Robotics</tag>
      
      <tag>SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Event-based Camera 标定工作整理大全</title>
    <link href="/2019/12/15/Event-based-Camera-%E6%A0%87%E5%AE%9A%E5%B7%A5%E4%BD%9C%E6%95%B4%E7%90%86%E5%A4%A7%E5%85%A8/"/>
    <url>/2019/12/15/Event-based-Camera-%E6%A0%87%E5%AE%9A%E5%B7%A5%E4%BD%9C%E6%95%B4%E7%90%86%E5%A4%A7%E5%85%A8/</url>
    
    <content type="html"><![CDATA[<p>本篇内容整理有：</p><ul><li>有关Event Camera Calibration一些直接相关的工作和工具包</li><li>目前为止的主流的Event-Camera的数据集，整理了其中涉及多传感器外参标定的工作</li><li>在Event Camera的VO/SLAM、目标检测的工作中，涉及到的多传感器、与传统CMOS相机标定的工作（<em>未完，仍在继续整理</em>）</li><li>目前为止的一些基于Event的Video Synthesis相关主流工作，去模糊、提高帧率等</li></ul><a id="more"></a><p>主要总结如下：</p><ul><li>大多数同时采集/使用图像帧与event数据的，大多数都是直接利用DAVIS做的</li><li>有两篇利用RGB-D的depth信息与dvs做标定 <a href="#RGBD-DVS">1</a>  <a href="#rgb-dvs-2">2</a></li><li><p>关于event-camera与传统相机做外参标定的文章，目前发现了三篇（两种做法，有两篇的做法是一样的）</p><ul><li>基于一个分束器装置和估计单应矩阵来做外参标定 <a href="#beam">1</a> <a href="#beam2">2</a></li><li><strong>event与CMOS灰度摄像头的pixel进行对齐。uzh-rpg组14年发表在ICRA上的<a href="#vo-ex-cal">一篇</a>做VO工作，涉及到dvs与灰度摄像头的外参标定，且是一种自标定算法。这一篇感觉挺重要…但目前具体算法流程还未看完</strong></li></ul></li><li><p>目前做Video Synthesis的已有工作中，多数还是基于DAVIS的灰度帧，关于彩色视频有：</p><ul><li>从彩色的、前景高速运动、背景静止的视频中恢复模糊帧。它的dvs和rgb相机是上面提到的利用分束器做的结合 <a href="#beam2">2</a></li><li>基于学习的方法直接根据起始帧、固定周期出现的中间彩色帧和过程中的event，直接恢复。它的对应彩色帧的event数据是模仿event的产生方式仿真生成的</li></ul></li></ul><h1 id="Event-Camera外参标定相关整理"><a href="#Event-Camera外参标定相关整理" class="headerlink" title="Event Camera外参标定相关整理"></a>Event Camera外参标定相关整理</h1><h2 id="有关Event-Camera-Calibration直接相关的工作"><a href="#有关Event-Camera-Calibration直接相关的工作" class="headerlink" title="有关Event Camera Calibration直接相关的工作"></a>有关<a href="https://github.com/uzh-rpg/event-based_vision_resources#calibration" target="_blank" rel="noopener">Event Camera Calibration</a>直接相关的工作</h2><p><a href="https://github.com/uzh-rpg/event-based_vision_resources" target="_blank" rel="noopener">event-based_vision_resources</a>项目中直接提到的与DVS标定相关的一些工作如下。然而这里面目前还没有提到RGB相机与DVS的基于event的外参标定工作。这里提出的工作有<strong>DVS的内参标定、DVS和IMU的外参标定、DVS与激光雷达的外参标定、双目dvs基于event的标定、以及DAVIS基于灰度帧的与传统相机的外参标定工作</strong>。</p><ul><li><p><a href="https://github.com/uzh-rpg/rpg_dvs_ros/tree/master/dvs_calibration#focus-adjustment" target="_blank" rel="noopener">Lens focus adjustment</a> or <a href="https://github.com/ethz-asl/kalibr/wiki/calibrating-the-vi-sensor#2-setting-the-focus" target="_blank" rel="noopener">this other source</a>.</p></li><li><p>For the DAVIS: <strong>use the grayscale frames to calibrate the optics of both frames and events.</strong></p><ul><li>ROS camera calibrator (<a href="http://wiki.ros.org/camera_calibration/Tutorials/MonocularCalibration" target="_blank" rel="noopener">monocular</a> or <a href="http://wiki.ros.org/camera_calibration/Tutorials/StereoCalibration" target="_blank" rel="noopener">stereo</a>)</li><li><a href="https://github.com/ethz-asl/kalibr/wiki/multiple-camera-calibration" target="_blank" rel="noopener">kalibr software</a> by ASL - ETH.</li></ul><blockquote><p>这些就是利用DAVIS的灰度帧与传统相机进行标定，之前的双目标定的常用工具均可以用。</p></blockquote></li><li><p>For the DAVIS camera and IMU calibration: <a href="https://github.com/ethz-asl/kalibr/wiki/camera-imu-calibration" target="_blank" rel="noopener">kalibr software</a> by ASL - ETH, using the grayscale frames.</p><blockquote><p>与IMU的外参标定，还是用的DAVIS的灰度帧。</p></blockquote></li><li><p>For the DVS (events-only):</p><ul><li><p><a href="https://github.com/uzh-rpg/rpg_dvs_ros/tree/master/dvs_calibration" target="_blank" rel="noopener">Calibration using blinking LEDs or computer screens</a> by RPG - UZH.</p><blockquote><p>用会闪烁的屏幕或者LED灯来进行dvs内参和stereo dvs外参的标定，基于events。</p></blockquote></li><li><p><a href="https://github.com/gorchard/DVScalibration" target="_blank" rel="noopener">DVS camera calibration</a> by G. Orchard.</p></li><li><p><a href="https://github.com/VLOGroup/dvs-calibration" target="_blank" rel="noopener">DVS camera calibration</a> by VLOGroup at TU Graz.</p><blockquote><p>这两个都是标定内参的。通过在屏幕上生成特定的图案。基于MATLAB的Caltech Camera Calibration Toolbox。</p></blockquote></li></ul></li><li><p>Song, R., Jiang, Z., Li, Y., Shan, Y., Huang, K.,<br><em><a href="https://doi.org/10.1109/WRC-SARA.2018.8584215" target="_blank" rel="noopener">Calibration of Event-based Camera and 3D LiDAR</a></em>,<br>WRC Symposium on Advanced Robotics and Automation (WRC SARA), 2018.</p><blockquote><h3 id="标定Event-Camera和VLP-16激光雷达之间的外参"><a href="#标定Event-Camera和VLP-16激光雷达之间的外参" class="headerlink" title="标定Event Camera和VLP-16激光雷达之间的外参"></a><strong>标定Event Camera和VLP-16激光雷达之间的外参</strong></h3><p>做了一个标定装置（后面带了一个闪烁的屏幕），Event camera可以稳定的输出，雷达的3D点云也能扫出来明显的效果。作者用的设备是VLP-16和<strong>Celex</strong>04-S。</p><div align="center">    <img src="assets/Screenshot from 2020-01-04 16-09-15.png" srcset="/img/loading.gif" style="zoom:80%;" />    <img src="assets/Screenshot from 2020-01-04 16-09-38.png" srcset="/img/loading.gif" style="zoom:50%;" /></div><p>标定方法也就是传统的模型：</p><p><img src="assets/Screenshot from 2020-01-04 16-26-56.png" srcset="/img/loading.gif" style="zoom:57%;" /></p><p><strong>但是看了下公式发现它只标定了外参的$T$而没标定$R$…</strong></p><div align="center">    <img src="assets/Screenshot from 2020-01-04 16-28-58.png" srcset="/img/loading.gif" style="zoom:37%;" />    <img src="assets/Screenshot from 2020-01-04 16-29-11.png" srcset="/img/loading.gif" style="zoom:37%;" />    <img src="assets/Screenshot from 2020-01-04 16-29-21.png" srcset="/img/loading.gif" style="zoom:43%;" /></div></blockquote></li><li><p>Dominguez-Morales, M. J., Jimenez-Fernandez, A., Jimenez-Moreno, G., Conde, C., Cabello, E., Linares-Barranco, A.,<br><em><a href="https://doi.org/10.1109/ACCESS.2019.2943160" target="_blank" rel="noopener">Bio-Inspired Stereo Vision Calibration for Dynamic Vision Sensors</a></em>,<br>IEEE Access, vol. 7, pp. 138415-138425, 2019.</p><blockquote><p>这一篇提到了关于双目的dvs之间基于event的外参标定工作。做了一个挺麻烦的装置…他这应该是为了做一个固定的系统。也是通过LED灯、基于events进行的两者之间的外参标定</p><p><img src="assets/Screenshot from 2020-01-04 17-23-52.png" srcset="/img/loading.gif" style="zoom:49%;" /></p></blockquote></li></ul><h2 id="Event-Camera相关的数据集中关于多设备和外参标定做法"><a href="#Event-Camera相关的数据集中关于多设备和外参标定做法" class="headerlink" title="Event Camera相关的数据集中关于多设备和外参标定做法"></a>Event Camera相关的数据集中关于多设备和外参标定做法</h2><h3 id="MVSEC-Dataset"><a href="#MVSEC-Dataset" class="headerlink" title="MVSEC Dataset"></a><a href="https://daniilidis-group.github.io/mvsec/" target="_blank" rel="noopener">MVSEC Dataset</a></h3><blockquote><p>Zhu, A. Z., Thakur, D., Ozaslan, T., Pfrommer, B., Kumar, V., &amp; Daniilidis, K. (2018). <em>The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception. IEEE Robotics and Automation Letters, 3(3), 2032–2039.</em> doi:10.1109/lra.2018.2800793 </p></blockquote><p>该数据集中多传感器的配置如下：</p><p><img src="assets/Screenshot from 2020-01-04 21-32-46.png" srcset="/img/loading.gif" style="zoom:100%;" /></p><p>对于这些传感器配置的参数的确定：</p><p>相机的内参、双目的外参和相机-IMU的外参通过Kalibr工具进行标定。左DAVIS和VLP16的标定通过Camera and Range Calibration Toolbox工具进行标定，<strong>用的是DAVIS的灰度帧</strong>，并且作者根据实际情况和CAD模型对其进行了手动微调。</p><div align="center">    <img src="assets/Screenshot from 2020-01-04 21-40-27.png" srcset="/img/loading.gif" style="zoom:47%;" />    <img src="assets/Screenshot from 2020-01-04 21-41-13.png" srcset="/img/loading.gif" style="zoom:50%;" /></div><hr><h3 id="Combined-Dynamic-Vision-RGB-D-Dataset"><a href="#Combined-Dynamic-Vision-RGB-D-Dataset" class="headerlink" title="Combined Dynamic Vision / RGB-D Dataset"></a><a href="http://ebvds.neurocomputing.systems/EBSLAM3D/index.html" target="_blank" rel="noopener">Combined Dynamic Vision / RGB-D Dataset</a></h3><blockquote><p>David Weikersdorfer, David B. Adrian, Daniel Cremers, Jörg Conradt: <em>Event-based 3D SLAM with a depth-augmented dynamic vision sensor. International Conference on Robotics and Automation, Hong-Kong, 2014</em></p></blockquote><p><a name="RGBD-DVS"></a>这一篇的原文章是拿RGB-D Camera和Event Camera做3D SLAM的。顺便提出了这个数据集。</p><p>作者的硬件配置长这个样子：</p><p><img src="assets/Screenshot from 2020-01-04 22-08-19.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><strong>这一篇做的标定的工作感觉和咱们想要做的有点接近。已经做了通过RGB frame与dvs的对应pixel，从而将深度相机捕获的深度图中的每一个深度值与每一个event做pixel级的对齐</strong></p><p>要解决这个问题，作者分了两步。首先先利用一个标定板，找出RGBD相机的彩图和DVS对应的pixel。然后利用这些匹配好的pixel对，用针孔相机模型建模（利用Depth的投影），最后将求解两个相机的内参和两者之间的外参建模成一个最小二乘问题进行求解。</p><p>找出RGBD相机和DVS对应pixel的标定板：</p><p><img src="assets/Screenshot from 2020-01-04 22-23-51.png" srcset="/img/loading.gif" alt=""></p><p>问题求解的建模过程：</p><p>畸变矩阵这里被建模成了这个简单的模型。作者提到这个简单模型对于dvs的建模已经够用了。RGBD相机在硬件上的补偿已经挺好，也够用了。</p><script type="math/tex; mode=display">L(u)=u\left(1+\kappa_{1} r+\kappa_{2} r^{2}\right), \quad r=\|u\|</script><p>depth像素到event像素平面的模型：</p><script type="math/tex; mode=display">u_{e}=L^{-1}\left(\mathrm{K}_{e} \mathrm{T} \mathrm{K}_{d}^{-1} u_{d}\right)</script><p>然后根据像素间的对应关系，建模最小二乘问题：</p><script type="math/tex; mode=display">\operatorname{argmin} \sum_{i=1}^{N}\left\|u_{e, i}-L^{-1}\left(\mathrm{K}_{e} \mathrm{T} \mathrm{K}_{d}^{-1} u_{k, i}\right)\right\|^{2}</script><p>求解出$f<em>{e}, c</em>{e}, \kappa<em>{1}, \kappa</em>{2}, \mathrm{T}$即可。RGBD相机的内参可以另标定得到。</p><hr><h3 id="ViViD-Vision-for-Visibility-Dataset"><a href="#ViViD-Vision-for-Visibility-Dataset" class="headerlink" title="ViViD : Vision for Visibility Dataset"></a><a href="https://sites.google.com/view/dgbicra2019-vivid/" target="_blank" rel="noopener">ViViD : Vision for Visibility Dataset</a></h3><p><img src="assets/Screenshot from 2020-01-04 22-52-17-1578149580736.png" srcset="/img/loading.gif" style="zoom:100%;" /></p><p>他这个用的也是DAVIS，用的灰度帧进行的外参标定。</p><p><img src="assets/Screenshot from 2020-01-04 22-53-39.png" srcset="/img/loading.gif" alt=""></p><p><img src="assets/Screenshot from 2020-01-04 22-54-11.png" srcset="/img/loading.gif" alt=""></p><hr><h3 id="The-UZH-FPV-Drone-Racing-Dataset"><a href="#The-UZH-FPV-Drone-Racing-Dataset" class="headerlink" title="The UZH-FPV Drone Racing Dataset"></a><a href="http://rpg.ifi.uzh.ch/uzh-fpv.html" target="_blank" rel="noopener">The UZH-FPV Drone Racing Dataset</a></h3><p>直接用的是DAVIS，frame和event之间的标定是做好的。其他的直接用DAVIS的frame和event的还有很多数据集，如<a href="http://rpg.ifi.uzh.ch/direct_event_camera_tracking/" target="_blank" rel="noopener">direct_event_camera_tracking dataset</a>等。</p><hr><h3 id="A-Dataset-for-Visual-Navigation-with-Neuromorphic-Methods"><a href="#A-Dataset-for-Visual-Navigation-with-Neuromorphic-Methods" class="headerlink" title="A Dataset for Visual Navigation with Neuromorphic Methods"></a><a href="http://atcproyectos.ugr.es/realtimeasoc/protected/evbench.html" target="_blank" rel="noopener">A Dataset for Visual Navigation with Neuromorphic Methods</a></h3><blockquote><p>Barranco, F., Fermuller, C., Aloimonos, Y., &amp; Delbruck, T. (2016). <em>A Dataset for Visual Navigation with Neuromorphic Methods. Frontiers in Neuroscience, 10.</em> doi:10.3389/fnins.2016.00049 </p></blockquote><p><a name="rgb-dvs-2">&lt;/a&gt;这也是一个结合RGB-D（MS Kinect）和DAVIS数据的数据集。它的一个主要的工作是要将rgbd的depth图与DAVIS的灰度帧给对应起来（和<a href="#RGBD-DVS">上面那个RGBD的工作</a>有点像）。分为两步，<strong>首先获取到两个传感器的内外参的参数，然后通过转换rgbd生成的点云的坐标系并在DAVIS的成像屏幕进行投影，得到它对应的深度图。</strong></p><p>关于DAVIS和RGBD的外参标定，作者论文中并没有明确提及…看描述应该是拿RGBD的RGB和DAVIS的灰度帧做的传统的外参标定。关于得到DAVIS坐标系下的深度图，作者将RGBD的点云通过两者的外参转换到DAVIS的坐标系下，然后再做个投影即可。</p><div align="center">    <img src="assets/Screenshot from 2020-01-06 18-38-57.png" srcset="/img/loading.gif" style="zoom:50%;" />    <img src="assets/Screenshot from 2020-01-06 18-33-17.png" srcset="/img/loading.gif" style="zoom:60%;" />    <img src="assets/Screenshot from 2020-01-06 18-33-27.png" srcset="/img/loading.gif" style="zoom:80%;" /></div><hr><h3 id="Color-DAVIS346-Datasets"><a href="#Color-DAVIS346-Datasets" class="headerlink" title="Color-DAVIS346 Datasets"></a><a href="http://rpg.ifi.uzh.ch/CED.html" target="_blank" rel="noopener">Color-DAVIS346 Datasets</a></h3><blockquote><p>C. Scheerlinck<em>, H. Rebecq</em>, T. Stoffregen, N. Barnes, R. Mahony, D. Scaramuzza, <em>CED: Color Event Camera Dataset, IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019.</em></p></blockquote><p>这是一个带有彩色fame的Event数据集，但它直接拿DAVIS346相机录制的…它可以直接输出彩色图像与event。</p><p><img src="assets/Screenshot from 2020-01-07 01-11-43.png" srcset="/img/loading.gif" style="zoom:60%;" /></p><p><img src="assets/Screenshot from 2020-01-07 01-11-33-1578330731656.png" srcset="/img/loading.gif" style="zoom:37%;" /></p><p><em>到此数据集方面整理的也差不多了，其他的Event数据集获取图像也基本都是通过DAVIS直接读取的，而不牵扯多传感器和外参。</em></p><hr><h3 id="DDD17"><a href="#DDD17" class="headerlink" title="DDD17"></a><a href="http://sensors.ini.uzh.ch/news_page/DDD17.html" target="_blank" rel="noopener">DDD17</a></h3><blockquote><p>Binas, J., Neil, D., Liu, S.-C., and Delbruck, T. (2017). DDD17: End-To-End DAVIS Driving Dataset. <em>arXiv:1711.01458 [cs]</em>. Available at: <a href="http://arxiv.org/abs/1711.01458" target="_blank" rel="noopener">http://arxiv.org/abs/1711.01458</a>. <em>(available online)</em></p></blockquote><p>一个包含了DAVIS的灰度帧和event数据、和车辆的一些运动信息比如速度等，场景大多为街区场景，为构建端到端的无人驾驶数据集做准备。</p><hr><h3 id="Large-scale-multimodal-event-based-dataset"><a href="#Large-scale-multimodal-event-based-dataset" class="headerlink" title="Large-scale multimodal event-based dataset"></a><a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10639/2305504/Toward-a-large-scale-multimodal-event-based-dataset-for-neuromorphic/10.1117/12.2305504.short?SSO=1&amp;tab=ArticleLink" target="_blank" rel="noopener">Large-scale multimodal event-based dataset</a></h3><blockquote><p>Leung, S., Shamwell, J., Maxey, C., Nothwang, W. D.,<br><a href="https://doi.org/10.1117/12.2305504" target="_blank" rel="noopener">Toward a large-scale multimodal event-based dataset for neuromorphic deep learning applications</a>,<br>Proc. SPIE 10639, Micro- and Nanotechnology Sensors, Systems, and Applications X, 106391T. <a href="https://www.researchgate.net/publication/325939343" target="_blank" rel="noopener">PDF</a></p></blockquote><p>这一篇论文下载不下来…看不了，sci-hub里也没有。但看摘要，它是通过两个DAVIS240C和一个深度相机录制的。里面可能会有关于几者之间外参标定。</p><hr><h2 id="Event-Camera-一些应用中标定相关的工作"><a href="#Event-Camera-一些应用中标定相关的工作" class="headerlink" title="Event Camera 一些应用中标定相关的工作"></a>Event Camera 一些应用中标定相关的工作</h2><h3 id="Object-Tracking"><a href="#Object-Tracking" class="headerlink" title="Object Tracking"></a>Object Tracking</h3><blockquote><p>D. Saner, O. Wang, S. Heinzle, Y. Pritch, A. Smolic, A. Sorkine-Hornung, M. Gross,<br><em><a href="http://dx.doi.org/10.2312/vmv.20141280" target="_blank" rel="noopener">High-Speed Object Tracking Using an Asynchronous Temporal Contrast Sensor</a></em>,<br>Vision, Modeling and Visualization (Darmstadt, Germany, October 8-10, 2014), pp. 87-94. <a href="http://ahornung.net/files/pub/2014-vmv-siliconretina-saner.pdf" target="_blank" rel="noopener">PDF</a></p></blockquote><p><a name="beam"></a> 这是一篇做Object Tracking的工作。它利用event数据在彩色帧中跟踪快速运动的物体，所以需要将彩色图与event进行Image registration。它利用了一个分束器装置使得在在两个相机中看到的场景尽可能保持一致<em>（分束器应该是这个作用吧？那不然他后面算单应矩阵问题就会很大）</em>，然后通过一个标定板然后来求单应矩阵，将DVS坐标系映射到相机坐标系中去了。</p><div align="center">    <img src="assets/Screenshot from 2020-01-08 10-39-27.png" srcset="/img/loading.gif" style="zoom:42%;" />    <img src="assets/Screenshot from 2020-01-08 10-39-54.png" srcset="/img/loading.gif" style="zoom:42%;" /></div><p>分束器（beamsplitter）：</p><p><img src="assets/v2-387dac22909d5485371329dfcc5f3d28_r-1578475989581-1578475992057.jpg" srcset="/img/loading.gif" alt="preview" style="zoom:67%;" /></p><p><em>图片来自<a href="https://zhuanlan.zhihu.com/p/24434788" target="_blank" rel="noopener">知乎</a></em></p><p>为了将单应性对应的更准确，作者手动尽可能的将两个相机的光轴对齐。与传统相机相比，DVS的分辨率较低，DVS像素将始终映射到视频图像中直径为几个像素的区域。因此，作者发现单应性映射对于作者的目的而言足够精确。</p><p><img src="assets/Screenshot from 2020-01-08 10-54-45.png" srcset="/img/loading.gif" style="zoom:40%;" /></p><hr><h3 id="Localization-and-Ego-Motion-Estimation"><a href="#Localization-and-Ego-Motion-Estimation" class="headerlink" title="Localization and Ego-Motion Estimation"></a>Localization and Ego-Motion Estimation</h3><blockquote><p>Mueggler, E., Huber, B., Scaramuzza, D.,<br><em><a href="https://doi.org/10.1109/IROS.2014.6942940" target="_blank" rel="noopener">Event-based, 6-DOF Pose Tracking for High-Speed Maneuvers</a></em>,<br>IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS), Chicago, IL, 2014, pp. 2761-2768. <a href="http://rpg.ifi.uzh.ch/docs/IROS14_Mueggler.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://youtu.be/LauQ6LWTkxM" target="_blank" rel="noopener">YouTube</a></p></blockquote><p>涉及到DVS内参的标定</p><p><img src="assets/Screenshot from 2020-01-09 21-11-05.png" srcset="/img/loading.gif" style="zoom:60%;" /></p><p><img src="assets/Screenshot from 2020-01-09 21-10-39.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="assets/Screenshot from 2020-01-09 21-11-18.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="VO-SLAM-VIO-VI-SLAM"><a href="#VO-SLAM-VIO-VI-SLAM" class="headerlink" title="VO/SLAM (VIO/VI-SLAM)"></a>VO/SLAM (VIO/VI-SLAM)</h3><blockquote><p>Censi, A. and Scaramuzza, D.,<br><em><a href="https://doi.org/10.1109/ICRA.2014.6906931" target="_blank" rel="noopener">Low-latency Event-based Visual Odometry</a></em>,<br>IEEE Int. Conf. Robotics and Automation (ICRA), 2014, pp. 703-710. <a href="http://rpg.ifi.uzh.ch/docs/ICRA14_Censi.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://censi.science/pub/research/2013-dvsd/201405-icra15-dvsd.pdf" target="_blank" rel="noopener">Slides</a></p></blockquote><p><a name="vo-ex-cal"></a>这一篇是同时通过一个灰度的camera和DVS来同时做的VO。需要camera提供的绝对光强，所以需要将camera和DVS进行Pixel级别的对应。</p><p><img src="assets/Screenshot from 2020-01-08 22-34-03.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p>该标定算法涉及时间同步和pixel(空间)的对齐。主要思路都是放置一段时间，然后利用两者捕获的光强变化作对比从而进行时间与空间的标定。</p><ul><li><p>时间标定：</p><p>要将dvs与camera的数据流的时间戳对齐，首先先将两者放置使其可以看到差不多的场景（see the same “changes”）。然后定义出一个可以表征两者观察到change而触发的信号，累积一段时间后将两者进行对比，从而判断出时移。由于累计的时间足够长，所以即使两者看到的场景并不完全一致，结果仍比较鲁棒。</p><p>由于这两个数据流的频率不一致，因此需要将它们重新采样为相同的采样率$\Delta$。</p><p>定义dvs的信号$f_t$为在时间间隔$[t-\Delta, t+\Delta]$内检测到的event数量：</p><p><img src="assets/Screenshot from 2020-01-09 16-16-28.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p>定义表征camera图像光强变化的信号$g_t$：</p><p>来自camera的数据可以被看做一个二元组$\left\langle t<em>{i}, y</em>{i}\right\rangle$，$t<em>{i}$是时间戳，$y</em>{i}$代表整张图像（$y<em>{i}: \mathbb{S}^{2} \rightarrow \mathbb{R}$  is the image<br>, here defined as a function on the visual sphere）。计算其离散的导数$\dot{y}</em>{i}=\left(y<em>{i+1}-y</em>{i-1} /\left(t<em>{i+1}-t</em>{i-1}\right)\right.$，从而得出总的光强变化$c<em>i$，$c_i$是$\dot{y}</em>{i}$的一范数$c<em>{i}=\int</em>{\mathbb{S}^{2}}\left|\dot{y}_{i}(s)\right| \mathrm{d} s$。从而定义表征camera图像在时间间隔$[t-\Delta, t+\Delta]$的光强变化的信号$g_t$：</p><p><img src="assets/Screenshot from 2020-01-09 16-28-46.png" srcset="/img/loading.gif" style="zoom:78%;" /></p><p>然后分析一段时间的两者信号，找出时移$\tau$:</p><p><img src="assets/Screenshot from 2020-01-09 16-31-12.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>同样，这种方法可以推广到只要能找到相互关系的、需要时间同步的两个系统中。</p><p><img src="assets/Screenshot from 2020-01-09 16-32-28.png" srcset="/img/loading.gif" style="zoom:77%;" /></p></li><li><p>pixel对齐（空间标定）的主要流程是：</p><ol><li><p><strong>将两个相机组成的装置放在环境中静止，环境中会出现动态物体</strong></p></li><li><p><strong>统计一段时间内，dvs的每个pixel的触发的event的数量，以及每个camera pixel的累计光强变化</strong></p><p>用下标 a ∈ A 来表示每个DVS pixel, 用下标 b ∈ B 表示每个 camera pixel。与上面时间标定类似($f_t$表示整张图像的触发event数量)，定义$f_t(a)$表示每个dvs像素在时间间隔$[t-\Delta, t+\Delta]$的触发event数量。$g_t(b)$同理，表示某个camera像素的。</p></li><li><p><strong>将每一个dvs pixel分别与每一个camera pixel根据在一段时间内的触发的情况，进行相似性度量</strong></p><p>定义$S(a,b)$表示$f_t(a)$和$g_t(b)$的相关性：</p><script type="math/tex; mode=display">S(a, b)=\operatorname{corr}\left(f_{t}(a), g_{t}(b)\right)</script><p><img src="assets/Screenshot from 2020-01-09 16-51-17.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p>对每个dvs pixel，将其与每个camera pixel遍历与对比。所以数据量的量级是$\mathcal{O}\left(\rho<em>{1}^{2} \rho</em>{2}^{2}\right)$($\rho<em>{1}$, $\rho</em>{2}$是两个相机的分辨率)。这是一个很恐怖的量级，甚至可能都不能完整加载到内存中…所以作者在实验的时候对dvs的分辨率进行了下采样，从128×128降到了16*16…</p></li><li><p><strong>选出每一个dvs pixel与其相似度最高的相机pixel，并通过计算置信度去除外点。</strong></p><p>对于每个a（dvs中的pixel），对应于camera frame中的pixel b：</p><script type="math/tex; mode=display">\hat{b}(a)=\arg \max _{b \in B} S(a, b)</script><p>如果触发的event在camera的视野外，则按道理是无法在camera中对应上的。所以为了解决这个情况，又定义了一个简单的置信度计算公式：</p><script type="math/tex; mode=display">w^{a}=S(a, \hat{b}(a)) / \sum_{b} S(a, b)</script><p>卡一个阈值即可区分是否真的有对应关系，从而去除在camera中观测不到的点。</p><p>至此得到dvs和camera的初步的对应关系。</p></li><li><p><strong>根据几何约束（正方形网格），对dvs pixel在相机像素平面上的排列情况进行再调整</strong></p><p>由于是每个dvs pixel都是单独进行对应的，所以受到噪声对结果的影响还蛮大的。所以再对两者pixel的对应关系进行微调，加入dvs的pixel是网格几何分布的约束。下图是微调后的效果：</p><p><img src="assets/Screenshot from 2020-01-09 17-12-02.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p>定义了一个能量函数：</p><script type="math/tex; mode=display">E=E_{\text {data }}+\alpha E_{\text {model }}</script><p>其中$E<em>{\text {data }}$是上一步中找到的对应关系；$E</em>{\text {model}}$是指dvs pixel尽可能的符合正方形网格模型。$\alpha &gt; 0$是对模型的置信度。</p><p>设$p^a$为优化后的、dvs pixel应该对应的camera pixel的位置。</p><p>$E_{\text {data }}$：设$p_0^{a}$为上一步估算出的dvs pixel的对应camera pixel的位置。$w^a$为上一步计算出的置信度。</p><script type="math/tex; mode=display">E_{\text {data }}\left(\left\{\boldsymbol{p}^{a}\right\}\right)=\sum_{a \in A} w^{a}\left\|\boldsymbol{p}^{a}-\boldsymbol{p}_{0}^{a}\right\|^{2}</script><p>$E_{\text {model}}$：dvs pixel的分布应该为正方形栅格的形状。为了表达这一约束可以有很多种形式来表述。作者用了一个dvs pixel与其前邻居dvs pixel的距离应该是相等的这一约束。</p><p>$a<em>{1}, a</em>{2} \in \operatorname{neig}(a)$，有$\left|\boldsymbol{p}^{a<em>{1}}-\boldsymbol{p}^{a}\right|=\left|\boldsymbol{p}^{a</em>{2}}-\boldsymbol{p}^{a}\right|$</p><p>得出</p><script type="math/tex; mode=display">E_{\text {model }}\left(\left\{\boldsymbol{p}^{a}\right\}\right)=\sum_{a \in A} \sum_{a_{1}, a_{2} \in \operatorname{neig}(a)}\left\|\boldsymbol{p}^{a_{1}}-\boldsymbol{p}^{a}\right\|^{2}-\left\|\boldsymbol{p}^{a_{2}}-\boldsymbol{p}^{a}\right\|^{2} |</script><p>将E最小化即可。使用quasi-Newton BFGS算法来优化它。</p><p>作者实验中选择的是$\alpha$ = 0.1，25次迭代。</p></li><li><p><strong>对dvs降采样后的pixel进行插值，得到全部的dvs pixel对应到camera frame中</strong></p><p><img src="assets/Screenshot from 2020-01-09 19-08-26.png" srcset="/img/loading.gif" style="zoom:70%;" /></p></li></ol></li></ul><p>讨论：</p><p>它提出的这个算法的缺陷：</p><ol><li><p>不能适用于高分辨率Event Camera的场景下。128×128分辨率的DVS都需要进行下采样到16×16，对齐结束后然后再进行插值恢复，这可能会丧失不少信息。</p><blockquote><p>如果把这个标定问题的思路放到三维求解、比如小规模SfM、解出Rt、进行重投影的方式的话，应该不受分辨率的限制。可能会得到更高的标定精度。</p></blockquote></li><li><p>需要将整个装置放置很长一段的时间来收集数据，效率较低，且算法的时空复杂度都较高。作者在论文中提到标定过程中收集数据用了35分钟。</p><blockquote><p>如果能像现在标定两个普通相机外参那样的效率来标定出Event Camera和RGB Camera之间的外参，感觉会更好。</p></blockquote></li><li><p>虽然这个算法可以自标定，也就是标定过程是全自动的，但是对场景也有一定的限制：</p><p><img src="assets/Screenshot from 2020-01-09 20-43-07.png" srcset="/img/loading.gif" style="zoom:80%;" /></p></li></ol><hr><blockquote><p>Rosinol Vidal, A., Rebecq, H., Horstschaefer, T., Scaramuzza, D.,<br><em><a href="https://doi.org/10.1109/LRA.2018.2793357" target="_blank" rel="noopener">Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios</a></em>,<br>IEEE Robotics and Automation Letters (RA-L), 3(2):994-1001, Apr. 2018. <a href="http://rpg.ifi.uzh.ch/docs/RAL18_VidalRebecq.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://youtu.be/jIvJuWdmemE" target="_blank" rel="noopener">YouTube</a>, <a href="http://rpg.ifi.uzh.ch/docs/RAL18_VidalRebecq_poster.pdf" target="_blank" rel="noopener">Poster</a>, <a href="http://rpg.ifi.uzh.ch/ultimateslam.html" target="_blank" rel="noopener">Project page</a>, <a href="https://youtu.be/0hDGFFJQfmA" target="_blank" rel="noopener">ICRA18 video pitch</a>.</p></blockquote><p>这一篇是结合Event Frame、Standard Frame和IMU三者联合做的SLAM。</p><p><img src="assets/Screenshot from 2020-01-08 23-05-10.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p>作者做实验直接用了DAVIS，所以也无需考虑灰度帧与dvs的外参标定。</p><div align="center">    <img src="assets/Screenshot from 2020-01-08 23-10-00.png" srcset="/img/loading.gif" style="zoom:50%;" />    <img src="assets/Screenshot from 2020-01-08 23-06-08.png" srcset="/img/loading.gif" style="zoom:55%;" /></div>---> Kueng, B., Mueggler, E., Gallego, G., Scaramuzza, D.,> *[Low-Latency Visual Odometry using Event-based Feature Tracks](https://doi.org/10.1109/IROS.2016.7758089)*,> IEEE/RSJ Int. Conf. Intelligent Robots and Systems (IROS), 2016, pp. 16-23. [PDF](http://rpg.ifi.uzh.ch/docs/IROS16_Kueng.pdf). [YouTube](https://youtu.be/RDu5eldW8i8)作者做实验直接用了DAVIS，所以也无需考虑灰度帧与dvs的外参标定。<div><img src="assets/Screenshot from 2020-01-09 21-22-47.png" srcset="/img/loading.gif" style="zoom:90%;" /><img src="assets/Screenshot from 2020-01-09 21-14-58.png" srcset="/img/loading.gif" style="zoom:80%;" /><img src="assets/Screenshot from 2020-01-09 21-14-48.png" srcset="/img/loading.gif" style="zoom:67%;" /></div><h2 id="Video-Synthesis-的一些目前相关工作工作"><a href="#Video-Synthesis-的一些目前相关工作工作" class="headerlink" title="Video Synthesis 的一些目前相关工作工作"></a>Video Synthesis 的一些目前相关工作工作</h2><p><em>以下仅从实验设置方面整理</em></p><p>Event Camera 的标定中有很多工作是与传统 Camera 相结合的。那么直接从单 Event Camera 中恢复出 frame 信息也是一个重要的工作领域。</p><hr><blockquote><p>Brandli, C., Muller, L., Delbruck, T.,<br><em><a href="https://doi.org/10.1109/ISCAS.2014.6865228" target="_blank" rel="noopener">Real-time, high-speed video decompression using a frame- and event-based DAVIS sensor</a></em>,<br>IEEE Int. Symp. on Circuits and Systems (ISCAS), 2014, pp. 686-689.</p></blockquote><p>这一篇是第一篇从event流中恢复视频帧的工作。<strong>直接利用DAVIS获取灰度帧和event数据，重建出灰度视频序列</strong>。</p><div align="center">    <img src="assets/Screenshot from 2020-01-07 11-57-16.png" srcset="/img/loading.gif" style="zoom:55%;" />    <img src="assets/Screenshot from 2020-01-07 11-57-23.png" srcset="/img/loading.gif" style="zoom:53%;" /></div><p><img src="assets/Screenshot from 2020-01-07 12-00-22.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p>算法主要步骤（待深究）：</p><p><img src="assets/Screenshot from 2020-01-07 16-40-05.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="assets/Screenshot from 2020-01-07 16-40-21.png" srcset="/img/loading.gif" style="zoom:60%;" /></p><hr><blockquote><p>Liu HC., Zhang FL., Marshall D., Shi L., Hu SM.,<br><em><a href="https://link.springer.com/article/10.1007/s00371-017-1372-y" target="_blank" rel="noopener">High-speed Video Generation with an Event Camera</a></em>,<br>The Visual Computer, 2017. <a href="https://cg.cs.tsinghua.edu.cn/papers/TVC-2017-HS-Video.pdf" target="_blank" rel="noopener">PDF</a>.</p></blockquote><p><a name="beam2">&lt;/a&gt;这一篇是从”<strong>前景高速运动、背景静止(应用场景限制)</strong>”的彩色视频序列中恢复超高帧率的视频。同时采集彩色视频与event流的装置与<a href="#beam">上面的做目标识别的一篇文章</a>相同，都是采用一个光学分束器。</p><p><img src="assets/Screenshot from 2020-01-08 11-21-27.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="assets/Screenshot from 2020-01-08 11-28-32.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><hr><blockquote><p>Shedligeri, P.A., Shah, K., Kumar, D., Mitra, K.,<br><em><a href="https://arxiv.org/pdf/1805.06140.pdf" target="_blank" rel="noopener">Photorealistic Image Reconstruction from Hybrid Intensity and Event based Sensor</a></em>,<br>arXiv:1805.06140, 2018.</p></blockquote><p>仅仅使用DAVIS的灰度帧和event进行的图像重建。考虑了event camera的突然运动和噪声的情况，效果更鲁棒。</p><p><img src="assets/Screenshot from 2020-01-08 11-45-33.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p><div align="center"><br>    <img src="assets/Screenshot from 2020-01-08 11-45-53.png" srcset="/img/loading.gif" style="zoom:52%;" /><br>    <img src="assets/Screenshot from 2020-01-08 11-46-02.png" srcset="/img/loading.gif" style="zoom:28%;" /></p><h2 id="lt-div-gt"><a href="#lt-div-gt" class="headerlink" title="&lt;/div&gt;"></a>&lt;/div&gt;</h2><blockquote><p>Wang, Z., Jiang, W., Katsaggelos, A., Cossairt, O.,<br><em><a href="https://arxiv.org/abs/1902.09680" target="_blank" rel="noopener">Event-driven Video Frame Synthesis</a></em>,<br>arXiv:1902.09680, 2019.</p></blockquote><p>基于学习的方法来做的。实验是直接选择的DAVIS数据集。</p><p><div align="center"><br>    <img src="assets/Screenshot from 2020-01-08 12-03-59.png" srcset="/img/loading.gif" style="zoom:80%;" /><br>    <img src="assets/Screenshot from 2020-01-08 12-04-06.png" srcset="/img/loading.gif" style="zoom:53%;" /><br>    <img src="assets/Screenshot from 2020-01-08 12-04-20.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="lt-div-gt-1"><a href="#lt-div-gt-1" class="headerlink" title="&lt;/div&gt;"></a>&lt;/div&gt;</h2><blockquote><p>Pan, L., Scheerlinck, C., Yu, X., Hartley, R., Liu, M., Dai, Y.,<br><em><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Pan_Bringing_a_Blurry_Frame_Alive_at_High_Frame-Rate_With_an_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Bringing a Blurry Frame Alive at High Frame-Rate with an Event Camera</a></em>,<br>IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2019. <a href="https://arxiv.org/pdf/1811.10180.pdf" target="_blank" rel="noopener">PDF</a>. <a href="http://rpg.ifi.uzh.ch/docs/CVPR19workshop/CVPRW19_Yuchao_Dai.pdf" target="_blank" rel="noopener">Slides</a>, <a href="https://drive.google.com/file/d/1NscnUF2QxK0of4ZW7T8kneJTH1X76l2u/view" target="_blank" rel="noopener">Video CVPR</a>, <a href="https://youtu.be/JcgboJ_7JAE" target="_blank" rel="noopener">Video CVPRW</a>, <a href="https://github.com/panpanfei/Bringing-a-Blurry-Frame-Alive-at-High-Frame-Rate-with-an-Event-Camera" target="_blank" rel="noopener">Code</a></p></blockquote><p>实验设置：利用的是DAVIS的灰度帧和event，灰度帧的去模糊。</p><p><img src="assets/Screenshot from 2020-01-08 16-58-30.png" srcset="/img/loading.gif" style="zoom:70%;" /></p><p>EDI在今年还有其扩展版本mEDI，arXiv:1903.06531v2。</p><p><img src="assets/Screenshot from 2020-01-08 17-00-27.png" srcset="/img/loading.gif" style="zoom:55%;" /></p><hr><blockquote><p>Pini, S., Borghi, G., Vezzani, R., Cucchiara, R.,<br><em><a href="https://doi.org/10.1007/978-3-030-30642-7_28" target="_blank" rel="noopener">Video synthesis from Intensity and Event Frames</a></em>,<br>Int. Conf. Image Analysis and Processing (ICIAP), 2019. Lecture Notes in Computer Science, vol 11751. <a href="https://iris.unimore.it/retrieve/handle/11380/1178955/224434/ICIAP19_Event_Cameras.pdf" target="_blank" rel="noopener">PDF</a></p></blockquote><p>这是一篇基于学习的方法来做的。使用DDD17数据集，基于<strong>灰度帧和积分后的event frame</strong>。做的是通过初始帧和后续的event数据流来估计接下来的帧。</p><p><img src="assets/Screenshot from 2020-01-08 17-11-38.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><hr><blockquote><p>Pini S., Borghi G., Vezzani R.,<br><em><a href="http://hdl.handle.net/11380/1185831" target="_blank" rel="noopener">Learn to See by Events: Color Frame Synthesis from Event and RGB Cameras</a></em>,<br>Int. Joint Conf. on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP) 2020. <a href="https://arxiv.org/pdf/1812.02041" target="_blank" rel="noopener">PDF</a></p></blockquote><p>这也是一篇基于学习的方法来做的。这一篇通过event恢复彩色的视频流。通过初始帧和一定周期内出现的彩色帧，以及过程中的event，恢复出彩色帧。</p><p>作者提出的模型可以同时恢复彩色帧和灰度帧。作者通过一些通用数据集比如KITTI，根据event的产生方式，通过彩色图，生成仿真的event数据流来进行对模型的训练；此外还用的两个Event Camera的数据集，都是采集的DAVIS的灰度帧。</p><p><img src="assets/Screenshot from 2020-01-08 20-01-16.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p><img src="assets/Screenshot from 2020-01-08 20-01-28.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="assets/Screenshot from 2020-01-08 20-02-44.png" srcset="/img/loading.gif" alt=""></p><p><img src="assets/Screenshot from 2020-01-08 20-02-20.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p>@author 薛轲翰</p><p>@e-mail kehan.xue@gmail.com</p>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Event-Camera</tag>
      
      <tag>Calibration</tag>
      
      <tag>Multi-Sensors</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cartographer Laser-SLAM 算法流程解析与调优</title>
    <link href="/2019/12/07/Cartographer%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%B0%83%E4%BC%98/"/>
    <url>/2019/12/07/Cartographer%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90%E4%B8%8E%E8%B0%83%E4%BC%98/</url>
    
    <content type="html"><![CDATA[<p>本篇博客的目的是记录自己在实物机器人上调试 Cartographer 算法的过程中，参考的资料和结合部分代码，一些自己总结的内容。Cartographer 是 Google 开源的一个激光 SLAM 项目，有着非常惊艳的工程实现。<br><a id="more"></a></p><p>本文参考cartographer<a href="https://google-cartographer-ros.readthedocs.io/en/latest/" target="_blank" rel="noopener">官方文档</a>。</p><p>首先要明确的一点，Cartographer的官方文档也提到，这个<strong>简介</strong>仅仅给出直觉概述层面上的Cartographer的不同子系统的介绍和配置说明。如果为了更详细的描述和更严谨的表达，应该去看Cartographer的Paper。虽然Paper中仅仅给出了2D SLAM的重要概念的严格表达，但这些概念可以非常自然的推广到3D。</p><p>Paper：</p><blockquote><p>W. Hess, D. Kohler, H. Rapp, and D. Andor, <a href="https://research.google.com/pubs/pub45466.html" target="_blank" rel="noopener">Real-Time Loop Closure in 2D LIDAR SLAM</a>, in <em>Robotics and Automation (ICRA), 2016 IEEE International Conference on</em>. IEEE, 2016. pp. 1271–1278.</p></blockquote><h1 id="Cartographer-Algorithm-walkthrough-for-tuning"><a href="#Cartographer-Algorithm-walkthrough-for-tuning" class="headerlink" title="Cartographer Algorithm walkthrough for tuning"></a>Cartographer Algorithm walkthrough for tuning</h1><h2 id="cartographer基本思路简介"><a href="#cartographer基本思路简介" class="headerlink" title="cartographer基本思路简介"></a>cartographer基本思路简介</h2><p><a href="https://blog.csdn.net/jsgaobiao/article/details/53116042" target="_blank" rel="noopener">这篇</a>CSDN博客说的挺简洁，但是现在必须成VIP才能看的文章了，有点醉…主要两张图如下：</p><ol><li><p>算法的基本思路</p><p><img src="assets/20161110144436386.png" srcset="/img/loading.gif" alt="img"></p></li><li><p>基本的ROS框架如下，当然可以有更多类型的输入，比如里程计信息等。</p><p><img src="assets/20161110145019043.png" srcset="/img/loading.gif" alt="img"></p></li></ol><p>Cartographer它的设计目标是实现低计算资源消耗，实时优化，不追求高精度（可以达到r=5cm级别的精度） Paper中说了，<strong>它主要的贡献并不在算法层面，而是提供了工程上高效率、Robust的代码实现。</strong></p><p><img src="assets/high_level_system_overview.png" srcset="/img/loading.gif" alt="Cartographer系统结构图"></p><p>Cartographer可以看做两个独立但有关系的子系统，Local SLAM 和 Global SLAM。</p><ul><li><p>Local SLAM可以被看做是前端(local trajectory builder)，任务是建立一个个的submap。各个submap是本地一致的，但是会慢慢的漂移。Local SLAM相关的配置文件是<a href="https://github.com/googlecartographer/cartographer/blob/df337194e21f98f8c7b0b88dab33f878066d4b56/configuration_files/trajectory_builder_2d.lua" target="_blank" rel="noopener">trajectory_builder_2d.lua</a>或<a href="https://github.com/googlecartographer/cartographer/blob/df337194e21f98f8c7b0b88dab33f878066d4b56/configuration_files/trajectory_builder_3d.lua" target="_blank" rel="noopener">trajectory_builder_3d.lua</a>。后面用<em>TRAJECTORY_BUILDER_nD</em>表示相通的配置options。</p></li><li><p>Golbal SLAM是后端，单开了一个线程运行在后端，主要作用是寻找 loop closure constraints(回环约束)。通过<strong>scans</strong>(gathered in <strong>nodes</strong>) 与submaps进行matching的方法来工作。并且可以结合其他传感器的数据来进一步提高精度，来确定一致性最强的Global optimization 方案。3D SLAM中还会尝试去估计重力方向。相关配置文件是<a href="https://github.com/googlecartographer/cartographer/blob/df337194e21f98f8c7b0b88dab33f878066d4b56/configuration_files/pose_graph.lua" target="_blank" rel="noopener">pose_graph.lua</a>。</p></li></ul><blockquote><p>TODO:</p><ul><li>.lua修改为本地gitlab仓库链接</li><li>Global中的scans和nodes的关系？<strong>scans</strong>(gathered in <strong>nodes</strong>)</li></ul></blockquote><p>总的来说，Local SLAM 的主要工作是生成更好的 submaps，Global SLAM的工作是将这些 submaps 更好的结合起来。</p><h2 id="Input-Process"><a href="#Input-Process" class="headerlink" title="Input Process"></a>Input Process</h2><h3 id="Laser-Data"><a href="#Laser-Data" class="headerlink" title="Laser Data"></a>Laser Data</h3><blockquote><p>注意！Cartographer中关于距离的参数的单位均为<strong>米/m</strong></p></blockquote><p>由于在实际运用的过程中，雷达在机器人身上的安装位置导致雷达被机器人身上其他部件挡了、或者雷达自身落灰了，或者一些从不期望的来源得到的最远的测量值（比如反射、或者自身传感器噪声）等，都是SLAM过程中所不期望的。对于这些噪声，Cartographer starts by applying a bandpass filter and only keeps range values between a certain min and max range。下面的这俩值根据你机器人和雷达的实际情况来确定。</p><p>这两个参数就是雷达扫描数据的距离范围。参数为下面两个。</p><pre><code class="hljs lua">TRAJECTORY_BUILDER_nD.min_rangeTRAJECTORY_BUILDER_nD.max_range</code></pre><p>Cartographer会用<code>TRAJECTORY_BUILDER_2D.missing_data_ray_length</code>的值来替换实际大于 <code>TRAJECTORY_BUILDER_nD.max_range</code> 的ranges。</p><blockquote><p>TODO:</p><ul><li><p>考虑根据具体的自己机器人的形状和雷达的实际效果，确定更好的卡值的方法？但又仔细一想好像又没这必要。</p></li><li><p><code>TRAJECTORY_BUILDER_2D.missing_data_ray_length</code>参数的说法还是有点迷……代码中是这样写的:</p><p><code>cartographer/mapping/internal/2d/local_trajectory_builder_2d.h</code> line: 170-189</p><pre><code class="hljs c++"><span class="hljs-comment">// Drop any returns below the minimum range and convert returns beyond the</span><span class="hljs-comment">// maximum range into misses.</span><span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; synchronized_data.ranges.<span class="hljs-built_in">size</span>(); ++i) &#123;  <span class="hljs-keyword">const</span> Eigen::Vector4f&amp; hit = synchronized_data.ranges[i].point_time;  <span class="hljs-keyword">const</span> Eigen::Vector3f origin_in_local =      range_data_poses[i] *      synchronized_data.origins.at(synchronized_data.ranges[i].origin_index);  <span class="hljs-keyword">const</span> Eigen::Vector3f hit_in_local = range_data_poses[i] * hit.head&lt;<span class="hljs-number">3</span>&gt;();  <span class="hljs-keyword">const</span> Eigen::Vector3f delta = hit_in_local - origin_in_local;  <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> range = delta.norm();  <span class="hljs-keyword">if</span> (range &gt;= options_.min_range()) &#123;    <span class="hljs-keyword">if</span> (range &lt;= options_.max_range()) &#123;      accumulated_range_data_.returns.push_back(hit_in_local);    &#125; <span class="hljs-keyword">else</span> &#123;      accumulated_range_data_.misses.push_back(          origin_in_local +          options_.missing_data_ray_length() / range * delta);    &#125;  &#125;&#125;</code></pre><p>具体待跑起来后分析实际数据</p></li></ul></blockquote><p>如果将 3D 雷达用在 2D SLAM的话,提供一个”截断”的参数,就是把一定高度范围内的扫描点映射到 2D 的一个平面上。</p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D.max_zTRAJECTORY_BUILDER_2D.min_z</code></pre><p>一组激光雷达的距离数据是一段时间内测量出来的，而这一段时间内机器人是运动的，这就导致了激光数据会产生畸变。这些距离数据被封装到一帧一帧的ROS message中，每一帧都带有时间戳信息，Cartographer会把多(&gt;=1)帧集合(accumulate)成一个大帧作为算法的输入。Cartographer认为这每一帧都是独立的，以帧为单位补偿运动导致的激光雷达数据产生的畸变，然后再把这些帧集合到一块去。所以当然Cartographer接收到的数据帧的频率越高，Cartographer的补偿效果越好，算法的输入数据质量越高。</p><p>所以<code>TRAJECTORY_BUILDER_nD.num_accumulated_range_data</code>参数的意义就是集合多少帧运动补偿后形成算法输入的大数据帧。(该参数要根据实际雷达的数据采集频率和每一帧数据的扫描范围来定，比如Rplidar A3的一帧数据是采样一圈；而VLP-16的一帧数据可以调成一个udp数据包而不是一圈数据，并且这样更好，因为转一圈也是需要时间的，这样就可以把一圈内的数据也做了运动补偿)</p><p>相关代码(<code>cartographer/mapping/internal/2d/local_trajectory_builder_2d.h</code> line:142-205)：</p><pre><code class="hljs c++"><span class="hljs-keyword">if</span> (num_accumulated_ == <span class="hljs-number">0</span>) &#123;  accumulation_started_ = <span class="hljs-built_in">std</span>::chrono::steady_clock::now();&#125;...<span class="hljs-keyword">if</span> (num_accumulated_ == <span class="hljs-number">0</span>) &#123;  <span class="hljs-comment">// 'accumulated_range_data_.origin' is uninitialized until the last</span>  <span class="hljs-comment">// accumulation.</span>  accumulated_range_data_ = sensor::RangeData&#123;&#123;&#125;, &#123;&#125;, &#123;&#125;&#125;;&#125;<span class="hljs-comment">// Drop any returns below the minimum range and convert returns beyond the</span><span class="hljs-comment">// maximum range into misses.</span><span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; synchronized_data.ranges.<span class="hljs-built_in">size</span>(); ++i) &#123;  ...  <span class="hljs-keyword">if</span> (range &gt;= options_.min_range()) &#123;    <span class="hljs-keyword">if</span> (range &lt;= options_.max_range()) &#123;      accumulated_range_data_.returns.push_back(hit_in_local);    &#125; <span class="hljs-keyword">else</span> &#123;      accumulated_range_data_.misses.push_back(          origin_in_local +          options_.missing_data_ray_length() / range * delta);    &#125;  &#125;&#125;++num_accumulated_;<span class="hljs-keyword">if</span> (num_accumulated_ &gt;= options_.num_accumulated_range_data()) &#123;  num_accumulated_ = <span class="hljs-number">0</span>;  <span class="hljs-keyword">const</span> transform::Rigid3d gravity_alignment = transform::Rigid3d::Rotation(      extrapolator_-&gt;EstimateGravityOrientation(time));  <span class="hljs-comment">// TODO(gaschler): This assumes that 'range_data_poses.back()' is at time</span>  <span class="hljs-comment">// 'time'.</span>  accumulated_range_data_.origin = range_data_poses.back().translation();  <span class="hljs-keyword">return</span> AddAccumulatedRangeData(      time,      TransformToGravityAlignedFrameAndFilter(          gravity_alignment.cast&lt;<span class="hljs-keyword">float</span>&gt;() * range_data_poses.back().inverse(),          accumulated_range_data_),      gravity_alignment);&#125;</code></pre><h4 id="Voxel-Filter"><a href="#Voxel-Filter" class="headerlink" title="Voxel Filter"></a>Voxel Filter</h4><p>较近的表面(如路面)经常扫描得到更多的points,而远处的物体的points经常比较稀少. 为了降低计算量, 需要对点云数据进行下采样, 简单的随机采样仍然会导致低密度区的点更少,而高密度区的点仍然比较多.因此cartographer 采用 voxel_filter (体素滤波)的方法。通过输入的点云数据创建一个三维体素栅格（可把体素栅格想象为微小的空间三维立方体的集合），然后在每个体素（即，三维立方体）内，用体素中所有点的重心来近似显示体素中其他点，这样该体素就内所有点就用一个重心点最终表示，对于所有体素处理后得到过滤后的点云。<code>TRAJECTORY_BUILDER_nD.voxel_filter_size</code>即为立方体的大小。如果立方体较小的话会导致更密集的数据，所耗的计算量更大。而较大的话可能会导致数据丢失但是计算速度会更快。</p><p>在提供了定大小的 voxel_filter, Cartographer还提供了一个 adaptive_voxel_filter,  adaptive_voxel_filter 可以在最大边长<code>TRAJECTORY_BUILDER_nD.*adaptive_voxel_filter.max_length</code>的限制下优化确定voxel_filter_size来实现目标的points数<code>TRAJECTORY_BUILDER_nD.*adaptive_voxel_filter.min_num_points</code> 。</p><p>官方给的默认配置：</p><p><code>trajectory_builder_2d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D = &#123;  ...  voxel_filter_size = <span class="hljs-number">0.025</span>,  adaptive_voxel_filter = &#123;    max_length = <span class="hljs-number">0.5</span>,    min_num_points = <span class="hljs-number">200</span>,    max_range = <span class="hljs-number">50.</span>,  &#125;,  ...&#125;</code></pre><p><code>trajectory_builder_3d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_3D = &#123;  ...    voxel_filter_size = <span class="hljs-number">0.15</span>,  high_resolution_adaptive_voxel_filter = &#123;    max_length = <span class="hljs-number">2.</span>,    min_num_points = <span class="hljs-number">150</span>,    max_range = <span class="hljs-number">15.</span>,  &#125;,  low_resolution_adaptive_voxel_filter = &#123;    max_length = <span class="hljs-number">4.</span>,    min_num_points = <span class="hljs-number">200</span>,    max_range = MAX_3D_RANGE,  &#125;,  ...&#125;</code></pre><h3 id="IMU-Data"><a href="#IMU-Data" class="headerlink" title="IMU Data"></a>IMU Data</h3><p>IMU对于SLAM可以提供非常有用的信息，Cartographer 直接利用IMU所提供的三轴线加速度与角速度信息，可以提供一个较为精确的重力方向，以及提供带有噪声但是整体方向大概正确的关于机器人旋转的信息。为了滤掉IMU的噪声，gravity is observed over a certain amount of time。在2D SLAM 中，可以做到无外界的一些补充信息来源而实时处理数据，所以 2D SLAM 可以选择是否使用 IMU 的消息。但是在 3D SLAM 中需要提供 IMU 数据作判断scans方向的先验，可以大大降低 scan 匹配的复杂性。</p><blockquote><p>cartographer中关于时间的参数单位均为s(秒).</p></blockquote><pre><code class="hljs ini"><span class="hljs-attr">TRAJECTORY_BUILDER_nD.use_imu_data</span> = <span class="hljs-literal">true</span><span class="hljs-attr">TRAJECTORY_BUILDER_nD.imu_gravity_time_constant</span> = <span class="hljs-number">10.0</span></code></pre><p>下文 imu 数据还在 Global optimization 中的应用.</p><h2 id="Local-SLAM"><a href="#Local-SLAM" class="headerlink" title="Local SLAM"></a>Local SLAM</h2><p>Once a scan has been assembled and filtered from multiple range data, it is ready for the local SLAM algorithm. Local SLAM可以利用来自与pose extrapolator的输出信息作为一个先验，把一帧新的scans通过scan matching的方法插入到当下的submap中。使用pose extrapolator的idea是，通过除了激光雷达以外的传感器的数据来预测当前scan插入到submap的位置，比如里程计信息、IMU等。</p><h3 id="Two-scan-matching-strategies"><a href="#Two-scan-matching-strategies" class="headerlink" title="Two scan matching strategies"></a>Two scan matching strategies</h3><h4 id="CeresScanMatcher-以及-RealTimeCorrelativeScanMatcher"><a href="#CeresScanMatcher-以及-RealTimeCorrelativeScanMatcher" class="headerlink" title="CeresScanMatcher 以及 RealTimeCorrelativeScanMatcher"></a>CeresScanMatcher 以及 RealTimeCorrelativeScanMatcher</h4><ul><li><p>CeresScanMatcher 利用上面说的先验 initial guess ,寻找 scan 与当前的 submap 最匹配的位置。通过对 submap 进行插值然后与 scan 进行对齐（It does this by interpolating the submap and sub-pixel aligning the scan）。这种做法比较快速，但是无法修复远大于子图分辨率的误差。如果你的传感器的设置与累积的那个时间区间都是合理的话,仅仅使用 CeresScanMatcher 通常是最好的选择。</p><p>CeresScanMatcher 可以为每一个输入源配置一个权重weight，权重就是对其数据的信任度，可以把它视为静态协方差。这个weight是无量纲的，并且是不能相互比较的。weight越大，Cartographer在进行scan matching的时候就对他更关注。数据来源可以包括 occupied space (points from the scan), translation and rotation from the pose extrapolator (or <code>RealTimeCorrelativeScanMatcher</code>)</p><pre><code class="hljs css"><span class="hljs-selector-tag">TRAJECTORY_BUILDER_3D</span><span class="hljs-selector-class">.ceres_scan_matcher</span><span class="hljs-selector-class">.occupied_space_weight</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_3D</span><span class="hljs-selector-class">.ceres_scan_matcher</span><span class="hljs-selector-class">.occupied_space_weight_0</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_3D</span><span class="hljs-selector-class">.ceres_scan_matcher</span><span class="hljs-selector-class">.occupied_space_weight_1</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.ceres_scan_matcher</span><span class="hljs-selector-class">.translation_weight</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.ceres_scan_matcher</span><span class="hljs-selector-class">.rotation_weight</span></code></pre><p>In 3D, the <code>occupied_space_weight_0</code> and <code>occupied_space_weight_1</code> parameters are related, respectively, to the high resolution and low resolution filtered point clouds.</p><p>CeresScanMatcher的名字来源于<a href="http://ceres-solver.org/" target="_blank" rel="noopener">Ceres Solver</a>。这个scan matching问题被建模为一个最小二乘问题，两帧间的motion是待优化变量。（GN等方法）Ceres optimizes the motion using a descent algorithm for a given number of iterations. Ceres can be configured to adapt the convergence speed to your own needs.</p><pre><code class="hljs lua">TRAJECTORY_BUILDER_nD.ceres_scan_matcher.ceres_solver_options.use_nonmonotonic_stepsTRAJECTORY_BUILDER_nD.ceres_scan_matcher.ceres_solver_options.max_num_iterationsTRAJECTORY_BUILDER_nD.ceres_scan_matcher.ceres_solver_options.num_threads</code></pre><p><code>use_nonmonotonic_steps</code>这个暂时还未深究，参见<a href="http://ceres-solver.org/nnls_solving.html#non-monotonic-steps" target="_blank" rel="noopener">Ceres-Solver官方文档讲解</a></p><p>官方给的默认配置：</p><p><code>trajectory_builder_2d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D = &#123;  ...  ceres_scan_matcher = &#123;    occupied_space_weight = <span class="hljs-number">1.</span>,    translation_weight = <span class="hljs-number">10.</span>,    rotation_weight = <span class="hljs-number">40.</span>,    ceres_solver_options = &#123;      use_nonmonotonic_steps = <span class="hljs-literal">false</span>,      max_num_iterations = <span class="hljs-number">20</span>,      num_threads = <span class="hljs-number">1</span>,    &#125;,  &#125;,  ...&#125;</code></pre></li><li><p>RealTimeCorrelativeScanMatcher 在你比较不信任你的其他传感器或者不存在其他的传感器的情况下可以启用。它的做法类似于回环检测中的做法将 scan 与当前 submap 进行 match。Best match 然后被用作 CeresScanMatcher 的先验。这种 match 的方式对计算资源要求较高，并且开了后，就忽略了其他传感器的数据。但这种做法在 feature rich 的环境中的鲁棒性非常好。</p><p>同样 RealTimeCorrelativeScanMatcher 也可以根据对 sensors 的信任度进行配置(即可以配置不同权重/置信度/weight)。它的工作原理是在一个搜索窗口中(搜索窗口的大小由搜索的最大距离半径和角度范围来指定)。在此窗口中进行 scan match 的时候,可以为 translation 和 rotation 选择不同的权重。</p><blockquote><p><em>TODO</em></p><p><em>例如当已知机器人不会旋转过多的话,就可以改变对应 weight。</em></p><p> <em>所以这个weight到底是指？具体还是看代码吧</em></p></blockquote><pre><code class="hljs lua">TRAJECTORY_BUILDER_nD.use_online_correlative_scan_matchingTRAJECTORY_BUILDER_nD.real_time_correlative_scan_matcher.linear_search_windowTRAJECTORY_BUILDER_nD.real_time_correlative_scan_matcher.angular_search_windowTRAJECTORY_BUILDER_nD.real_time_correlative_scan_matcher.translation_delta_cost_weightTRAJECTORY_BUILDER_nD.real_time_correlative_scan_matcher.rotation_delta_cost_weight</code></pre><p>官方给的默认配置：</p><p><code>trajectory_builder_2d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D = &#123;  ...  use_online_correlative_scan_matching = <span class="hljs-literal">false</span>,  real_time_correlative_scan_matcher = &#123;    linear_search_window = <span class="hljs-number">0.1</span>,    angular_search_window = <span class="hljs-built_in">math</span>.<span class="hljs-built_in">rad</span>(<span class="hljs-number">20.</span>),    translation_delta_cost_weight = <span class="hljs-number">1e-1</span>,    rotation_delta_cost_weight = <span class="hljs-number">1e-1</span>,  &#125;,  ...&#125;</code></pre></li></ul><h3 id="motion-filter"><a href="#motion-filter" class="headerlink" title="motion_filter"></a>motion_filter</h3><p>为避免将过多的 scan 插入到 submap 里，当两个 scan 成功 match 后，会得到两个 match 之间的运动关系。当两者运动关系不明显的话,这个 match 结果就不会被插入到 submap 中去。这个操作通过运动滤波器中卡time,distance以及angle的阈值来实现。</p><pre><code class="hljs css"><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.motion_filter</span><span class="hljs-selector-class">.max_time_seconds</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.motion_filter</span><span class="hljs-selector-class">.max_distance_meters</span><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.motion_filter</span><span class="hljs-selector-class">.max_angle_radians</span></code></pre><p>官方给的默认配置：</p><p><code>trajectory_builder_2d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D = &#123;  ...  motion_filter = &#123;    max_time_seconds = <span class="hljs-number">5.</span>,    max_distance_meters = <span class="hljs-number">0.2</span>,    max_angle_radians = <span class="hljs-built_in">math</span>.<span class="hljs-built_in">rad</span>(<span class="hljs-number">1.</span>),  &#125;,  ...&#125;</code></pre><h3 id="Submap"><a href="#Submap" class="headerlink" title="Submap"></a>Submap</h3><p>当Local SLAM接收到一定数量的range data时可认为此时当前的submap被完成了，这个即由<code>TRAJECTORY_BUILDER_nD.submaps.num_range_data</code>参数来指定。Local SLAM的结果在时间上积累后会产生漂移误差，Global SLAM可以来fix这个误差。这个Submap必须足够的小从而可以认为它是局部正确的。但从另一方面来看，他又必须足够的大从而可以做回环。</p><pre><code class="hljs css"><span class="hljs-selector-tag">TRAJECTORY_BUILDER_nD</span><span class="hljs-selector-class">.submaps</span><span class="hljs-selector-class">.num_range_data</span></code></pre><p>这里总结一下，雷达数据到建图的流程如下：</p><p><img src="assets/CartographerLaserData-1575206288976.png" srcset="/img/loading.gif" alt="CartographerLaserData"></p><p>Submap 可以采用不止一种的数据结构来存储。但是现在大多都是采用的概率栅格地图（probability grids）的方式来存储。但是在2D中，还可以采用TSDF（Truncated Signed Distance Fields）地图类型。</p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D.submaps.grid_options_2d.grid_type</code></pre><p>概率栅格地图的资料很多，去搜一下即可。这里就贴文档原句了。Odds are updated according to “<em>hits</em>” (where the range data is measured) and “<em>misses</em>” (the free space between the sensor and the measured points)。可以根据对被占据occupied和free space的雷达数据的置信度，加减hits和misses的weight值（Both <em>hits</em> and <em>misses</em> can have a different weight in occupancy probability calculations giving more or less trust to occupied or free space measurements）。</p><blockquote><p><em>TODO</em></p><p><em>这里为啥分了两个…是因为不同的地图类型吗</em></p></blockquote><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D.submaps.range_data_inserter.probability_grid_range_data_inserter.hit_probabilityTRAJECTORY_BUILDER_2D.submaps.range_data_inserter.probability_grid_range_data_inserter.miss_probabilityTRAJECTORY_BUILDER_3D.submaps.range_data_inserter.hit_probabilityTRAJECTORY_BUILDER_3D.submaps.range_data_inserter.miss_probability</code></pre><p>官方给的默认配置：</p><p><code>trajectory_builder_2d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D = &#123;  ...  submaps = &#123;    num_range_data = <span class="hljs-number">90</span>,    grid_options_2d = &#123;      grid_type = <span class="hljs-string">"PROBABILITY_GRID"</span>,      resolution = <span class="hljs-number">0.05</span>,    &#125;,    range_data_inserter = &#123;      range_data_inserter_type = <span class="hljs-string">"PROBABILITY_GRID_INSERTER_2D"</span>,      probability_grid_range_data_inserter = &#123;        insert_free_space = <span class="hljs-literal">true</span>,        hit_probability = <span class="hljs-number">0.55</span>,        miss_probability = <span class="hljs-number">0.49</span>,      &#125;,    &#125;,  &#125;,  ...&#125;</code></pre><p>2D SLAM中一个submap仅有一个栅格地图（probability grid）被存储。在3D SLAM中因为scan matching性能的原因，维护了两个<em>hybrid</em> probability grid (the term “hybrid” only refers to an internal tree-like data representation and is abstracted to the user)，并分别应用了一个adaptive_voxel_filter。：</p><ul><li>一个是用于远距离测量的低分辨率 hybrid grid</li><li>另一个是用于近距离测量的高分辨率 hybrid grid</li></ul><p>Scan match 首先将远处的低分辨率的点云与低分辨率 hybrid grid 对齐，然后通过高分辨率的近处点云与高分辨率的hybrid grid对齐来refine pose。</p><pre><code class="hljs lua">TRAJECTORY_BUILDER_2D.submaps.grid_options_2d.resolutionTRAJECTORY_BUILDER_3D.submaps.high_resolutionTRAJECTORY_BUILDER_3D.submaps.low_resolutionTRAJECTORY_BUILDER_3D.high_resolution_adaptive_voxel_filter.max_rangeTRAJECTORY_BUILDER_3D.low_resolution_adaptive_voxel_filter.max_range</code></pre><blockquote><p><em>TODO</em></p><p><em>上面的adaptive_voxel_filter.max_range是指的近和远的距离的界定吗</em></p></blockquote><p>Cartographer ROS提供了一个可以在rviz中可视化submaps的插件。可以选择submap通过他们的id。3D SLAM中rviz仅仅显示3D hybrid probability grids的2D 投影（in grayscale）。通过Rviz左侧栏可以切换high resolution hybrid grids来看。</p><p><code>trajectory_builder_3d.lua</code></p><pre><code class="hljs lua">TRAJECTORY_BUILDER_3D = &#123;  ...  submaps = &#123;    high_resolution = <span class="hljs-number">0.10</span>,    high_resolution_max_range = <span class="hljs-number">20.</span>,    low_resolution = <span class="hljs-number">0.45</span>,    num_range_data = <span class="hljs-number">160</span>,    range_data_inserter = &#123;      hit_probability = <span class="hljs-number">0.55</span>,      miss_probability = <span class="hljs-number">0.49</span>,      num_free_space_voxels = <span class="hljs-number">2</span>,    &#125;,  &#125;,  ...&#125;</code></pre><h2 id="Global-SLAM"><a href="#Global-SLAM" class="headerlink" title="Global SLAM"></a>Global SLAM</h2><p>当Local SLAM成功生成submaps的同时，在后端运行着一个全局的优化程序（sparse pose adjustment）。通过调整submap的位置来保持全局一致。然后还会考虑回环优化。</p><p>每当<code>POSE_GRAPH.optimize_every_n_nodes</code>数目的node被插入地图的时候运行一次optimization。</p><p>通常先将<code>POSE_GRAPH.optimize_every_n_nodes</code> 置零来关闭Global SLAM，然后专心的来调Local SLAM。这通常是调试Cartographer的第一步。</p><p>我们把估计出来的一个Scan的绝对位姿称为trajectory上的一个节点(Node)，那么节点与节点的彼此之间的相对位姿就可以称为一个约束(Constraint)。<a href="https://zhuanlan.zhihu.com/p/50055546" target="_blank" rel="noopener">参考链接</a></p><blockquote><p>上面的说法的确比较容易理解，并且做Pose Graph的确仅依赖与位姿。但是通过代码来看，这个Node所带的信息不止有Scan的绝对位姿（gravity aligned，用代码中的注释：Transform to approximately gravity align the tracking frame as determined by local SLAM.），还带有gravity aligned的PointCloud。</p><p>trajectory_node_data.proto文件：</p><pre><code class="hljs protobuf">syntax = <span class="hljs-string">"proto3"</span>;<span class="hljs-keyword">package</span> cartographer.mapping.proto;<span class="hljs-keyword">import</span> <span class="hljs-string">"cartographer/sensor/proto/sensor.proto"</span>;<span class="hljs-keyword">import</span> <span class="hljs-string">"cartographer/transform/proto/transform.proto"</span>;<span class="hljs-comment">// Serialized state of a mapping::TrajectoryNode::Data.</span><span class="hljs-class"><span class="hljs-keyword">message</span> <span class="hljs-title">TrajectoryNodeData</span> </span>&#123;  <span class="hljs-built_in">int64</span> timestamp = <span class="hljs-number">1</span>;  transform.proto.Quaterniond gravity_alignment = <span class="hljs-number">2</span>;  sensor.proto.CompressedPointCloud      filtered_gravity_aligned_point_cloud = <span class="hljs-number">3</span>;  sensor.proto.CompressedPointCloud high_resolution_point_cloud = <span class="hljs-number">4</span>;  sensor.proto.CompressedPointCloud low_resolution_point_cloud = <span class="hljs-number">5</span>;  <span class="hljs-keyword">repeated</span> <span class="hljs-built_in">float</span> rotational_scan_matcher_histogram = <span class="hljs-number">6</span>;  transform.proto.Rigid3d local_pose = <span class="hljs-number">7</span>;&#125;</code></pre><p>cartographer/mapping/trajectory_node.h文件：</p><pre><code class="hljs c++"><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">TrajectoryNode</span> &#123;</span>  <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">Data</span> &#123;</span>    common::Time time;    <span class="hljs-comment">// Transform to approximately gravity align the tracking frame as</span>    <span class="hljs-comment">// determined by local SLAM.</span>    Eigen::Quaterniond gravity_alignment;    <span class="hljs-comment">// Used for loop closure in 2D: voxel filtered returns in the</span>    <span class="hljs-comment">// 'gravity_alignment' frame.</span>    sensor::PointCloud filtered_gravity_aligned_point_cloud;    <span class="hljs-comment">// Used for loop closure in 3D.</span>    sensor::PointCloud high_resolution_point_cloud;    sensor::PointCloud low_resolution_point_cloud;    Eigen::VectorXf rotational_scan_matcher_histogram;    <span class="hljs-comment">// The node pose in the local SLAM frame.</span>    transform::Rigid3d local_pose;  &#125;;  <span class="hljs-function">common::Time <span class="hljs-title">time</span><span class="hljs-params">()</span> <span class="hljs-keyword">const</span> </span>&#123; <span class="hljs-keyword">return</span> constant_data-&gt;time; &#125;  <span class="hljs-comment">// This must be a shared_ptr. If the data is used for visualization while the</span>  <span class="hljs-comment">// node is being trimmed, it must survive until all use finishes.</span>  <span class="hljs-built_in">std</span>::<span class="hljs-built_in">shared_ptr</span>&lt;<span class="hljs-keyword">const</span> Data&gt; constant_data;  <span class="hljs-comment">// The node pose in the global SLAM frame.</span>  transform::Rigid3d global_pose;&#125;;</code></pre></blockquote><p>后端是一个Pose Graph的优化。通过调整nodes和submaps之间的约束关系来优化最终的图。Constraints直觉上来感觉就是由一根根小绳子将所有的nodes给捆起来，pose adjustement就是把这些小绳子给全部接起来。这也就是所谓的Pose Graph。</p><blockquote><p>Rviz中可以对这些Constraints进行可视化，这对调试Global SLAM来说是非常方便的。</p><p>还可以开启<code>POSE_GRAPH.constraint_builder.log_matches</code>来看关于constraints builder的report。</p></blockquote><p>约束分为非全局约束与全局约束：</p><ul><li>非全局约束也被称作(也被称作是 inter submaps constraints)。它在一条trajectory上离得近的 nodes 之间被自动构建。直觉上来看，这些约束使得trajectory的局部结构是一致的。</li><li>全局约束(也被称作是loop closure constraints或者intra submaps contraints)的运行：通常是在一个新的new submap和之前的nodes之间进行搜索，当满足空间上足够的相近（被一个search window限定）以及一个极强的scan match结果。直观的来讲,相当于在两个绳子(约束)之间打一个结点使得两根绳子里的更近。</li></ul><pre><code class="hljs lua">POSE_GRAPH.constraint_builder.max_constraint_distancePOSE_GRAPH.fast_correlative_scan_matcher.linear_search_windowPOSE_GRAPH.fast_correlative_scan_matcher_3d.linear_xy_search_windowPOSE_GRAPH.fast_correlative_scan_matcher_3d.linear_z_search_windowPOSE_GRAPH.fast_correlative_scan_matcher*.angular_search_window</code></pre><p>官方默认配置：</p><p><code>pose_graph.lua</code></p><pre><code class="hljs lua">POSE_GRAPH = &#123;  ...  constraint_builder = &#123;    max_constraint_distance = <span class="hljs-number">15.</span>,    ...    fast_correlative_scan_matcher = &#123;      linear_search_window = <span class="hljs-number">7.</span>,      angular_search_window = <span class="hljs-built_in">math</span>.<span class="hljs-built_in">rad</span>(<span class="hljs-number">30.</span>),      ...    &#125;,    fast_correlative_scan_matcher_3d = &#123;      ...      linear_xy_search_window = <span class="hljs-number">5.</span>,      linear_z_search_window = <span class="hljs-number">1.</span>,      angular_search_window = <span class="hljs-built_in">math</span>.<span class="hljs-built_in">rad</span>(<span class="hljs-number">15.</span>),      ...    &#125;,  &#125;  ...&#125;</code></pre><blockquote><p>// TODO ICRA RM两台机器人，这一点可以考虑</p><p>Cartographer不止可以单单在单个轨迹上loop closure，还可以在多个机器人的多个轨迹上align。这个部分的文档 the parameters related to “global localization” out of the scope of this document.</p></blockquote><p>为了限制constraints的数量（也是降低计算力），Cartographer对这些node做了一个下采样，通过<code>POSE_GRAPH.constraint_builder.sampling_ratio</code>参数来控制。下采样过度会导致约束缺失以及不容易进行loop closure，下采样力度过小会导致Global SLAM的运行速度过慢以及不能实时的loop closure。</p><p>官方默认配置：</p><p><code>pose_graph.lua</code></p><pre><code class="hljs lua">POSE_GRAPH = &#123;  ...  constraint_builder = &#123;    sampling_ratio = <span class="hljs-number">0.3</span>,    ...  &#125;  ...&#125;</code></pre><p>当一个node和一个submap进行constraint building的时候，通过的是一个叫做FastCorrelativeScanMatcher的机制。这个scan matcher是Cartographer的独创并且使得real-time loop closures scan matching成为可能。它引入了分支界限法（Branch and bound），可以在不同分辨率的地图网格上进行工作并且非常有效的去除误匹配。关于这个在Cartographer的论文中被详细描述了。所用的搜索树的深度是可以被控制的。</p><pre><code class="hljs lua">POSE_GRAPH.constraint_builder.fast_correlative_scan_matcher.branch_and_bound_depthPOSE_GRAPH.constraint_builder.fast_correlative_scan_matcher_3d.branch_and_bound_depthPOSE_GRAPH.constraint_builder.fast_correlative_scan_matcher_3d.full_resolution_depth</code></pre><p>官方默认配置：</p><p><code>pose_graph.lua</code></p><pre><code class="hljs lua">POSE_GRAPH = &#123;  ...  constraint_builder = &#123;    max_constraint_distance = <span class="hljs-number">15.</span>,    ...    fast_correlative_scan_matcher = &#123;      ...      branch_and_bound_depth = <span class="hljs-number">7</span>,    &#125;,    fast_correlative_scan_matcher_3d = &#123;      ...      branch_and_bound_depth = <span class="hljs-number">8</span>,      full_resolution_depth = <span class="hljs-number">3</span>,      ...    &#125;,  &#125;  ...&#125;</code></pre><p>一旦FastCorrelativeScanMatcher达到一定的效果的时候（大于<code>POSE_GRAPH.constraint_builder.min_score</code>参数值），会把它再继续扔到CeresScanMatcher中来进行refine。</p><pre><code class="hljs lua">POSE_GRAPH.constraint_builder.min_scorePOSE_GRAPH.constraint_builder.ceres_scan_matcher_3dPOSE_GRAPH.constraint_builder.ceres_scan_matcher</code></pre><p>官方默认配置：</p><p><code>pose_graph.lua</code></p><pre><code class="hljs lua">POSE_GRAPH = &#123;  ...  constraint_builder = &#123;    ...    min_score = <span class="hljs-number">0.55</span>,    ...    ceres_scan_matcher = &#123;      occupied_space_weight = <span class="hljs-number">20.</span>,      translation_weight = <span class="hljs-number">10.</span>,      rotation_weight = <span class="hljs-number">1.</span>,      ceres_solver_options = &#123;        use_nonmonotonic_steps = <span class="hljs-literal">true</span>,        max_num_iterations = <span class="hljs-number">10</span>,        num_threads = <span class="hljs-number">1</span>,      &#125;,    &#125;,    ceres_scan_matcher_3d = &#123;      occupied_space_weight_0 = <span class="hljs-number">5.</span>,      occupied_space_weight_1 = <span class="hljs-number">30.</span>,      translation_weight = <span class="hljs-number">10.</span>,      rotation_weight = <span class="hljs-number">1.</span>,      only_optimize_yaw = <span class="hljs-literal">false</span>,      ceres_solver_options = &#123;        use_nonmonotonic_steps = <span class="hljs-literal">false</span>,        max_num_iterations = <span class="hljs-number">10</span>,        num_threads = <span class="hljs-number">1</span>,      &#125;,    &#125;,  &#125;,  ...&#125;</code></pre><h3 id="The-Optimization-Problem"><a href="#The-Optimization-Problem" class="headerlink" title="The Optimization Problem"></a>The Optimization Problem</h3><p>当 Cartographer 运行优化问题的时候, Cartographer会通过多个残差项来对submaps进行调整。每一项残差通过被加权的cost function来计算（和SLAM中常见的做法一样）。这些每一个cost function取自多个数据源，全局(回环)约束<code>global (loop closure) constraints</code>，非全局(scan match)的约束<code>the non-global (matcher) constraints</code>，IMU 的测量值<code>IMU acceleration and rotation measurements</code>，local SLAM 的粗略的 pose 估计<code>local SLAM rough pose estimations</code>，外部的odometry 信息或者GPS等<code>an odometry source or a fixed frame (such as a GPS system)</code>。可通过下面项来进行配置：</p><pre><code class="hljs lua">POSE_GRAPH.constraint_builder.loop_closure_translation_weightPOSE_GRAPH.constraint_builder.loop_closure_rotation_weightPOSE_GRAPH.matcher_translation_weightPOSE_GRAPH.matcher_rotation_weightPOSE_GRAPH.optimization_problem.*_weightPOSE_GRAPH.optimization_problem.ceres_solver_options</code></pre><blockquote><p>TODO ?</p><p>One can find useful information about the residuals used in the optimization problem by toggling <code>POSE_GRAPH.max_num_final_iterations</code></p></blockquote><p>官方默认配置：</p><p><code>pose_graph.lua</code></p><pre><code class="hljs lua">POSE_GRAPH = &#123;  ...  constraint_builder = &#123;    ...    loop_closure_translation_weight = <span class="hljs-number">1.1e4</span>,    loop_closure_rotation_weight = <span class="hljs-number">1e5</span>,    ...  &#125;,  ...  matcher_translation_weight = <span class="hljs-number">5e2</span>,  matcher_rotation_weight = <span class="hljs-number">1.6e3</span>,  optimization_problem = &#123;    huber_scale = <span class="hljs-number">1e1</span>,    acceleration_weight = <span class="hljs-number">1e3</span>,    rotation_weight = <span class="hljs-number">3e5</span>,    local_slam_pose_translation_weight = <span class="hljs-number">1e5</span>,    local_slam_pose_rotation_weight = <span class="hljs-number">1e5</span>,    odometry_translation_weight = <span class="hljs-number">1e5</span>,    odometry_rotation_weight = <span class="hljs-number">1e5</span>,    fixed_frame_pose_translation_weight = <span class="hljs-number">1e1</span>,    fixed_frame_pose_rotation_weight = <span class="hljs-number">1e2</span>,    log_solver_summary = <span class="hljs-literal">false</span>,    ceres_solver_options = &#123;      use_nonmonotonic_steps = <span class="hljs-literal">false</span>,      max_num_iterations = <span class="hljs-number">50</span>,      num_threads = <span class="hljs-number">7</span>,    &#125;,  &#125;,&#125;</code></pre><p>在代码中，各种cost function的定义文件路径：</p><div align="left">    <img src="assets/Screenshot from 2019-12-07 21-31-20.png" srcset="/img/loading.gif" style="zoom:100%;" /></div><p>Cartographer提供了如下的cost functions：</p><div align="left">    <img src="assets/Screenshot from 2019-12-07 21-31-48.png" srcset="/img/loading.gif" style="zoom:100%;" /></div><h3 id="全局优化时对于IMU数据的处理"><a href="#全局优化时对于IMU数据的处理" class="headerlink" title="全局优化时对于IMU数据的处理"></a>全局优化时对于IMU数据的处理</h3><p>Global optimization 对 imu 的 pose 信息提供了更多的灵活性。默认的 Ceres 会优化你 IMU 和 tracking frame 之间的外参。如果你不信任你的 imu 的数据的话, Ceres’ global optimization 的结果可以被记录然后来用来优化它俩之间的外参矩阵。如果 Ceres 不能够很好的优化 IMU 的 pose (它俩之间的外参矩阵)或者你非常信任你校准的它俩之间的外参矩阵的话，可以当做常量来使用自己标定的外参矩阵。</p><pre><code class="hljs lua">POSE_GRAPH.optimization_problem.log_solver_summaryPOSE_GRAPH.optimization_problem.use_online_imu_extrinsics_in_3d</code></pre><h3 id="Huber-loss"><a href="#Huber-loss" class="headerlink" title="Huber loss"></a>Huber loss</h3><p>在residuals中，采用Huber loss function 从而控制outliers的影响。Huber loss function的Huber scale是定的，通过下面参数指定。</p><pre><code class="hljs lua">POSE_GRAPH.optimization_problem.huber_scale</code></pre><p>这个值选的越大，潜在的outliers对系统的影响可能就越大。</p><p>关于Huber loss <a href="https://blog.csdn.net/qq_29981283/article/details/83042231" target="_blank" rel="noopener">博客1</a> <a href="https://blog.csdn.net/lj6052317/article/details/87885658" target="_blank" rel="noopener">博客2</a>：</p><p>huber loss 是一种优化平方loss的一种方式，使得loss变化没有那么大。</p><p><img src="assets/20190222210736618.png" srcset="/img/loading.gif" alt="img"></p><p><img src="assets/20190222210820112.png" srcset="/img/loading.gif" alt="img"></p><h3 id="最终的全局优化"><a href="#最终的全局优化" class="headerlink" title="最终的全局优化"></a>最终的全局优化</h3><p>一旦 当 trajectory 完成后，Cartographer 经常会运行一个新的全局优化，迭代的次数通常比之前的要多得多。这样做的原因是要尽可能的去优化最终建图的效果，并且通常并没有实时性的要求。所以选择大量的迭代是正确的选择.</p><pre><code class="hljs lua">POSE_GRAPH.max_num_final_iterations</code></pre><h5 id="附-PointCloud2消息类型的数据格式"><a href="#附-PointCloud2消息类型的数据格式" class="headerlink" title="附: PointCloud2消息类型的数据格式"></a>附: PointCloud2消息类型的数据格式</h5><p><a href="http://docs.ros.org/api/sensor_msgs/html/msg/PointCloud2.html" target="_blank" rel="noopener">官方点云数据格式</a></p><p><a href="https://blog.csdn.net/Fourier_Legend/article/details/83656798" target="_blank" rel="noopener">点云数据格式解释</a></p><p>手动解析点云数据时不可以直接解析，因为在field里是以二进制方式存储的，可以通过<a href="https://answers.ros.org/question/273182/trying-to-understand-pointcloud2-msg/" target="_blank" rel="noopener">ros包里的工具来进行解析</a></p>]]></content>
    
    
    <categories>
      
      <category>SLAM 原理与实践</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Robotics</tag>
      
      <tag>SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用 Pitest 进行变异测试</title>
    <link href="/2019/11/22/%E4%BD%BF%E7%94%A8Pitest%E8%BF%9B%E8%A1%8C%E5%8F%98%E5%BC%82%E6%B5%8B%E8%AF%95/"/>
    <url>/2019/11/22/%E4%BD%BF%E7%94%A8Pitest%E8%BF%9B%E8%A1%8C%E5%8F%98%E5%BC%82%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h1 id="使用Pitest进行变异测试"><a href="#使用Pitest进行变异测试" class="headerlink" title="使用Pitest进行变异测试"></a>使用Pitest进行变异测试</h1><h2 id="变异测试-Mutation-Testing"><a href="#变异测试-Mutation-Testing" class="headerlink" title="变异测试(Mutation Testing)"></a>变异测试(Mutation Testing)</h2><p>变异测试是软件测试方法中的一个重要测试方法。变异测试旨在找出有效的测试用例。简单来说，就是对代码做一些简单的修改（变异），然后用同样的测试用例进行输入，看输出是否会发生改变。如果输出发生变化，即该变异(mutation)被kill掉，该test case就是有效的test case；若所有测试用例扔进去跑一遍，该mutation还没有被kill掉，则说明这时候test case的有效覆盖还不够完善。</p><p>变异分为strong mutation和weak mutation。具体详细定义和二者之间的好处见该<a href="assets/mutation-test.pdf">pdf</a>的16-19。</p><p>变异测试的主要流程：</p><p><img src="assets/Screenshot from 2019-11-20 15-44-36.png" srcset="/img/loading.gif" alt=""></p><p>后面的那个自动化测试的步骤，就可以由pitest来完成。</p><p>具体的变异手段有以下：</p><p><img src="assets/Screenshot from 2019-11-20 15-49-46.png" srcset="/img/loading.gif" alt=""></p><p>具体每一个啥意思，也很容易明白，看一眼例子就行了。见<a href="assets/mutation-test.pdf">pdf</a>第22-32页。</p><h2 id="Pitest的使用"><a href="#Pitest的使用" class="headerlink" title="Pitest的使用"></a>Pitest的使用</h2><p>然后就可以开始进行变异测试了。<a href="https://pitest.org/" target="_blank" rel="noopener">Pitest</a>是一个基于Junit4的工具包。</p><p>在基于maven的使用pitest的方式下，首先在pom.xml中添加如下字段：</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">plugin</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.pitest<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>pitest-maven<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>LATEST<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span> <span class="hljs-tag">&lt;/<span class="hljs-name">plugin</span>&gt;</span></code></pre><p>在写好待测试类和基于Junit4的测试用例后，就可以通过指令<code>mvn org.pitest:pitest-maven:mutationCoverage</code>，就会生成对应测试报告，里面提供的信息包括了它自动做了哪些变异以及测试用例的覆盖情况，会给出一个打分。</p><p><img src="assets/Screenshot from 2019-11-20 16-04-03.png" srcset="/img/loading.gif" alt=""></p><p>进行的mutation具体操作详情和mutation是否被kill掉的情况，从而可以判断出test case的覆盖情况。</p><p><img src="assets/Screenshot from 2019-11-20 16-04-57.png" srcset="/img/loading.gif" alt=""></p><p>它默认的会测试你整个工程代码。当然你也可以在配置文件中指定想要测试的类和测试用例：</p><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">plugin</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.pitest<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>pitest-maven<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>LATEST<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">targetClasses</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">param</span>&gt;</span>com.your.package.root.want.to.mutate*<span class="hljs-tag">&lt;/<span class="hljs-name">param</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">targetClasses</span>&gt;</span>        <span class="hljs-tag">&lt;<span class="hljs-name">targetTests</span>&gt;</span>            <span class="hljs-tag">&lt;<span class="hljs-name">param</span>&gt;</span>com.your.package.root*<span class="hljs-tag">&lt;/<span class="hljs-name">param</span>&gt;</span>        <span class="hljs-tag">&lt;/<span class="hljs-name">targetTests</span>&gt;</span>    <span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">plugin</span>&gt;</span></code></pre><p>这些都只是简单的初步使用，具体更多的配置可见<a href="https://pitest.org/quickstart/maven/" target="_blank" rel="noopener">官方文档</a></p>]]></content>
    
    
    <categories>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Software-Test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VINS-Mono 整体框架概述</title>
    <link href="/2019/10/20/VINS-Mono-%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%E6%A6%82%E8%BF%B0/"/>
    <url>/2019/10/20/VINS-Mono-%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<p>VINS-Mono，即Monocular Visual-Inertial SLAM，是港科大沈劭劼老师组开源的系统，主打<strong>单目+IMU</strong>。分别在Robot端和IOS端均做了实现并开源了出来。本篇为VINS-Mono框架的概览。本人才学疏浅，有理解不深和错误的地方，请多多指教！</p><h1 id="VINS-Mono-整体框架概述"><a href="#VINS-Mono-整体框架概述" class="headerlink" title="VINS-Mono 整体框架概述"></a>VINS-Mono 整体框架概述</h1><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Related Papers</p><blockquote><p><strong>Online Temporal Calibration for Monocular Visual-Inertial Systems</strong>, Tong Qin, Shaojie Shen, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS, 2018), <strong>best student paper award</strong> <a href="https://ieeexplore.ieee.org/abstract/document/8593603" target="_blank" rel="noopener">pdf</a></p><p><strong>VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator</strong>, Tong Qin, Peiliang Li, Zhenfei Yang, Shaojie Shen, IEEE Transactions on Robotics <a href="https://ieeexplore.ieee.org/document/8421746/?arnumber=8421746&amp;source=authoralert" target="_blank" rel="noopener">pdf</a></p><p>此外，本文档参考沈劭劼老师于2017年6月9日在泡泡机器人上的公开课课件和视频。</p></blockquote><p>VINS，即Visual-Inertial System，视觉-惯导融合系统。实际中有很多应用场景，如无人机定位、移动端的AR应用。一个完善的VINS系统，需要有下面三个特性：</p><ul><li>物理尺度信息</li><li>Robust and smooth odometry，即local accuracy</li><li>Loop closure，全局一致性global consistency</li></ul><p>VINS系统相关的工作：</p><ul><li><p>MSCKF（2007）</p><ul><li>Google Tango Project所用技术</li></ul></li><li><p>okvis（2015）</p><ul><li>开源，eth-asl组的工作</li></ul></li><li><p>Visual-Inertial ORB-SLAM（2017）</p><ul><li>原作者好像没有开源，<a href="https://arxiv.org/pdf/1610.05949.pdf" target="_blank" rel="noopener">论文地址</a></li><li>国内有大神对原论文做了实现并且开源了出来，<a href="http://paopaorobot.org/1057.html" target="_blank" rel="noopener">代码介绍</a> <a href="https://github.com/jingpang/LearnVIORB" target="_blank" rel="noopener">仓库地址</a>，并且好像还有不少其他的实现版本<a href="https://github.com/YoujieXia/VI_ORB_SLAM2" target="_blank" rel="noopener">版本1</a> <a href="https://github.com/ZuoJiaxing/Learn-ORB-VIO-Stereo-Mono" target="_blank" rel="noopener">版本2</a>等</li></ul></li><li><p>Apple ARKit</p><ul><li>沈老师在报告中提到，在iPhone上（vins-mono也做了ios端的实现），Apple ARKit的近距离表现要比vins-mono好，远距离还是可以比一比的</li></ul></li><li><p>Qualcomm Snapdragon 835</p></li></ul><p>VINS-Mono，即Monocular Visual-Inertial SLAM，是港科大沈劭劼老师组开源的系统，主打<strong>单目+IMU</strong>。分别在Robot端和IOS端均做了实现并开源了出来。</p><h2 id="问题挑战"><a href="#问题挑战" class="headerlink" title="问题挑战"></a>问题挑战</h2><h3 id="Monocular-Vision方面"><a href="#Monocular-Vision方面" class="headerlink" title="Monocular Vision方面"></a>Monocular Vision方面</h3><ul><li><p>单目不具备物理尺度的观测</p></li><li><p>Up-to-scale SfM</p></li></ul><h3 id="Monocular-Visual-Inertial-Systems方面"><a href="#Monocular-Visual-Inertial-Systems方面" class="headerlink" title="Monocular Visual-Inertial Systems方面"></a>Monocular Visual-Inertial Systems方面</h3><p>加上IMU后，尺度信息变得可观，但是存在以下问题：</p><div align="left"><img src="vins-mono整理.assets/Screenshot from 2019-10-24 17-27-42.png" srcset="/img/loading.gif"  style="zoom:70%;"></div><p>即：</p><ul><li><p>需要知道初始速度与姿态</p><ul><li>加速度计测出来的是加速度，做二次积分，积第二次的时候当然是需要知道初速度的</li><li>加速度计数据是带有重力加速度的信息。为了避免把重力值积分给积进去，所以相当于是需要知道整个系统的姿态的</li></ul></li><li><p>相机和IMU之间的外参标定，即旋转和平移</p><ul><li>沈老师提到在实验中，两者之间的旋转是非常非常重要的，歪一点点系统就崩了，而平移相对没有那么的敏感</li></ul></li><li><p>为了估计那些估计量，需要整个系统对多个点的多次观测</p><ul><li>所以就是需要一套formulation来做这个multi-observation constraints，递推为SLAM里的Graph SLAM formulation</li></ul></li></ul><h3 id="同步与时间戳问题"><a href="#同步与时间戳问题" class="headerlink" title="同步与时间戳问题"></a>同步与时间戳问题</h3><p><img src="vins-mono整理.assets/Screenshot from 2019-10-24 18-02-49.png" srcset="/img/loading.gif" style="float:;zoom:70%;" /></p><p>最理想的当然是第一种。第二种情况的offset是可以比较容易标定出来的，第三种是最差的，offset是一个时变的量。vins-mono的开源版本代码是直接按照第一种方式来处理的，但是实际表现上在第二种和第三种情况表现也还可以，比如手机上一般就是第二种和第三种情况。</p><h2 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h2><p><img src="vins-mono整理.assets/Screenshot from 2019-10-24 16-35-40.png" srcset="/img/loading.gif" alt=""></p><p>主要就是五块东西。代码中主要开启了四个线程：前段图像跟踪、Sliding-window V-I-BA非线性优化（初始化和imu预积分也在这个线程里）、闭环检测和全局的Pose Graph的优化。</p><h3 id="Measurement-Preprocessing"><a href="#Measurement-Preprocessing" class="headerlink" title="Measurement Preprocessing"></a>Measurement Preprocessing</h3><p>测量值预处理：</p><ul><li><p>对相机图像进行特征点检测和跟踪</p></li><li><p>与IMU进行预积分</p><div align="left"><img src="vins-mono整理.assets/Screenshot from 2019-10-24 19-17-17.png" srcset="/img/loading.gif" style="zoom:50%;" /></div></li></ul><h3 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h3><p>初始化：</p><p>非常非常重要，单目视觉中很多的东西比如imu的bias、尺度等信息都是无法直接观测出来的，都是需要算出来的。整个系统是一个非常非线性化的系统，非线性系统若收敛肯定是要初始值的，特别是这种对状态都无法直接观测的情况下。vins-mono花了比较大的精力对初始值进行处理。</p><p><img src="vins-mono整理.assets/Screenshot from 2019-10-24 19-15-35.png" srcset="/img/loading.gif" style="zoom:70%;" /></p><h3 id="Local-Visual-Inertial-Bundle-Adjustment-with-Relocalization"><a href="#Local-Visual-Inertial-Bundle-Adjustment-with-Relocalization" class="headerlink" title="Local Visual-Inertial Bundle Adjustment with Relocalization"></a>Local Visual-Inertial Bundle Adjustment with Relocalization</h3><p>Local Visual-Inertial Bundle Adjustment with Relocalization：</p><p>初始化之后，就进入了一个基于非线性优化的，滑动窗口Visual-Inertial Bundle Adjustment。将视觉约束、IMU 约束和闭环约束放在一个大的目标函数中进行非线性优化,求解滑窗内所有帧的 PVQ、bias 等。</p><p>基于Graph的形式给表示出来，并且维护一个滑动窗口来保证算VI Odometry时的运算复杂度保持在一个固定的状态下。</p><p>绿色的帧就是回环检测的到的，跟当前的滑动窗口里的已知位置的蓝色帧有共同观测，然后就能根据这些已知位置的帧做一个重定位。vins-mono里的重定位也是一个<strong>紧耦合</strong>的形式，即回环检测的帧和当前的帧之间的位置信息是通过他们互相看到一套同样的特征点的一个互匹配得到的。并且Relocalization后，与后面的Global Pose Graph在某种程度上也牵扯在一起。</p><p><img src="vins-mono整理.assets/Screenshot from 2019-10-24 19-14-29.png" srcset="/img/loading.gif" style="zoom:70%;" /></p><h3 id="Loop-detection"><a href="#Loop-detection" class="headerlink" title="Loop detection"></a>Loop detection</h3><p>回环检测：</p><p>为实现重定位功能，肯定是需要回环检测的。</p><p><img src="vins-mono整理.assets/Screenshot from 2019-10-24 19-35-29.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><h3 id="Global-pose-graph-SLAM"><a href="#Global-pose-graph-SLAM" class="headerlink" title="Global pose graph SLAM"></a>Global pose graph SLAM</h3><p>全局Pose Graph优化：</p><p>后端的一个大规模Pose Graph优化，保证全局一致性。</p><p>在不断地运行中，会把keyframe给保存下来，然后通过不断地回环检测检测到了一些特征点匹配，把这些匹配给通过一个Pose Graph的优化把它们的误差给消除掉。也就相当于是前面那个sliding window是一直可以基于这个Pose Graph做一个重定位的操作的。</p><p><img src="vins-mono整理.assets/Screenshot from 2019-10-24 19-33-23.png" srcset="/img/loading.gif" style="zoom:83%;" /></p>]]></content>
    
    
    <categories>
      
      <category>SLAM 原理与实践</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Robotics</tag>
      
      <tag>SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机组成原理 - 设计一个CPU</title>
    <link href="/2019/07/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AACPU/"/>
    <url>/2019/07/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AACPU/</url>
    
    <content type="html"><![CDATA[<p>计算机组成原理课程的一个大作业：设计一个符合下列要求的 CPU。完整项目请移步：<a href="https://github.com/kehanXue/Design-A-CPU。" target="_blank" rel="noopener">https://github.com/kehanXue/Design-A-CPU。</a></p><a id="more"></a><h1 id="CPU-设计要求"><a href="#CPU-设计要求" class="headerlink" title="CPU 设计要求"></a>CPU 设计要求</h1><h2 id="基本要求"><a href="#基本要求" class="headerlink" title="基本要求"></a>基本要求</h2><ul><li>CPU 字长为 8 位，8 个程序员可见的寄存器，分别命名为 r0, . . . , r7</li><li>地址总线、数据总线各为 8 位，可访问 28 字节的地址空间</li><li>数据采用补码表示，只需支持定点运算</li><li>采用定长三级时序，每个指令周期包含 3 个机器周期(取指周期、间址周期和执行周 期)，每个机器周期由 3 个节拍构成</li><li>系统时钟频率为 130MHZ</li><li>为减低成本，采用 TTL 74181a作为运算器</li></ul><h2 id="指令集特点"><a href="#指令集特点" class="headerlink" title="指令集特点"></a>指令集特点</h2><ul><li>支持零操作数、单操作数和双操作数三种指令</li><li>所有指令的两个操作数不能同时为内存操作数</li><li>支持立即寻址、直接寻址、寄存器直接寻址和相对寻址四种寻址方式</li><li>采用 1 字节或者 2 字节变长指令字，操作码采用定长格式</li></ul><h2 id="支持的指令"><a href="#支持的指令" class="headerlink" title="支持的指令"></a>支持的指令</h2><p><img src="DesignCPUbyMyself.assets/Instructions-Set.png" srcset="/img/loading.gif" alt=""></p><h2 id="要完成的任务"><a href="#要完成的任务" class="headerlink" title="要完成的任务"></a>要完成的任务</h2><ul><li>请设计符合要求的指令格式</li><li>给出所设计 CPU 的逻辑框图</li><li>给出基于硬布线设计控制器的逻辑表达式</li><li>给出基于微程序设计控制器的微指令及控存中的微程序</li></ul><h1 id="指令系统设计"><a href="#指令系统设计" class="headerlink" title="指令系统设计"></a>指令系统设计</h1><p>由于指令的操作数与寻址方式不同, 根据作业中所要求实现的指令类型, 以以下思路设计指令系统.</p><h2 id="分析寻址类型需求"><a href="#分析寻址类型需求" class="headerlink" title="分析寻址类型需求"></a>分析寻址类型需求</h2><ol><li>由于存在$8=2^3$种不同操作指令, 所以可以用3bit的操作码来区分该8种不同操作指令</li></ol><div class="table-container"><table><thead><tr><th>mov</th><th>add</th><th>sub</th><th>and</th><th>or</th><th>not</th><th>jmp</th><th>hlt</th></tr></thead><tbody><tr><td>000</td><td>001</td><td>010</td><td>011</td><td>100</td><td>101</td><td>110</td><td>111</td></tr></tbody></table></div><p>   其中mov, add, sub, and, or为双操作数指令, not和jmp为单操作数指令, hlt为零操作数指令</p><ol><li><p>所有指令的两个操作数不能同时为内存操作数. 该CPU需支持立即寻址, 直接寻址, 寄存器直接寻址和相对寻址.</p><p>其中有以下约束:</p><ul><li>目的操作数不能为立即寻址方式</li><li>相对寻址一般用于转移指令, 所以本大作业设计中不在其他类型指令中出现</li><li>指令字长为单字节或双字节</li></ul><p>设定每一条指令的左操作数为目的操作数, 右操作数为源操作数.</p><p>为了区分不同的寻址方式, 需要在操作数前面加上寻址特征. </p><p>本题目中双操作数指令的寻址方式均有4种, 设定寻址特征如下:</p></li></ol><div class="table-container"><table><thead><tr><th>寻址方式</th><th>寄存器直接寻址-立即数寻址</th><th>寄存器直接寻址-寄存器直接寻址</th><th>寄存器直接寻址-直接寻址</th><th>直接寻址-寄存器直接寻址</th></tr></thead><tbody><tr><td>寻址特征</td><td>00</td><td>01</td><td>10</td><td>11</td></tr></tbody></table></div><p>   单操作数not指令的寻址方式有两种, 直接寻址与寄存器直接寻址. not为单操作数指令, 操作数可以为直接寻址与寄存器直接寻址两种方式. 因为指令的位数够用, 为了保持指令格式的整齐, 仍采用2位的寻址特征. </p><div class="table-container"><table><thead><tr><th>寻址方式</th><th>寄存器直接寻址</th><th>直接寻址</th></tr></thead><tbody><tr><td>寻址特征</td><td>00</td><td>01</td></tr></tbody></table></div><p>   单操作数jmp指令是转移类指令, 有相对偏移与绝对偏移两种方式. 相对偏移的方式即为相对寻址, 跳转后的地址为PC当前值+jmp指令的操作数. 绝对偏移即为立即寻址, 直接跳转到jmp指令的操作数代表的地址.</p><div class="table-container"><table><thead><tr><th>寻址方式</th><th>相对寻址</th><th>立即寻址</th></tr></thead><tbody><tr><td>寻址特征</td><td>00</td><td>01</td></tr></tbody></table></div><p>   确定各种指令中不同操作数的长度:</p><ul><li>立即数寻址取决于剩下的指令位数</li><li>$8=2^3$个程序员可见寄存器, 寄存器寻址操作数长度为3</li><li>$2^8$字节的地址空间, 直接寻址操作数的长度为8</li></ul><h2 id="设计指令系统"><a href="#设计指令系统" class="headerlink" title="设计指令系统"></a>设计指令系统</h2><h3 id="mov"><a href="#mov" class="headerlink" title="mov"></a><strong><em>mov</em></strong></h3><p>分为以下几种情况:</p><p><strong>movRI</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:立即数(8位)</th></tr></thead><tbody><tr><td>000</td><td>00</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>movRR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>空码</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>000</td><td>01</td><td>00000</td><td>xxx</td><td>xxx</td></tr></tbody></table></div><p><strong>movRM</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:主存地址编号(8位)</th></tr></thead><tbody><tr><td>000</td><td>10</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>movMR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:主存地址编号(8位)</th><th>右操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>000</td><td>11</td><td>xxxxxxxx</td><td>xxx</td></tr></tbody></table></div><h3 id="add"><a href="#add" class="headerlink" title="add"></a><strong><em>add</em></strong></h3><p>同mov, 分为以下几种情况:</p><p><strong>addRI</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:立即数(8位)</th></tr></thead><tbody><tr><td>001</td><td>00</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>addRR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>空码</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>001</td><td>01</td><td>00000</td><td>xxx</td><td>xxx</td></tr></tbody></table></div><p><strong>addRM</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:主存地址编号(8位)</th></tr></thead><tbody><tr><td>001</td><td>10</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>addMR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:主存地址编号(8位)</th><th>右操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>001</td><td>11</td><td>xxxxxxxx</td><td>xxx</td></tr></tbody></table></div><h3 id="sub"><a href="#sub" class="headerlink" title="sub"></a><strong><em>sub</em></strong></h3><p>同mov, 分为以下几种情况:</p><p><strong>subRI</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:立即数(8位)</th></tr></thead><tbody><tr><td>010</td><td>00</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>subRR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>空码</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>010</td><td>01</td><td>00000</td><td>xxx</td><td>xxx</td></tr></tbody></table></div><p><strong>subRM</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:主存地址编号(8位)</th></tr></thead><tbody><tr><td>010</td><td>10</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>subMR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:主存地址编号(8位)</th><th>右操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>010</td><td>11</td><td>xxxxxxxx</td><td>xxx</td></tr></tbody></table></div><h3 id="and"><a href="#and" class="headerlink" title="and"></a><strong><em>and</em></strong></h3><p>同mov, 分为以下几种情况:</p><p><strong>andRI</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:立即数(8位)</th></tr></thead><tbody><tr><td>011</td><td>00</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>andRR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>空码</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>011</td><td>01</td><td>00000</td><td>xxx</td><td>xxx</td></tr></tbody></table></div><p><strong>andRM</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:主存地址编号(8位)</th></tr></thead><tbody><tr><td>011</td><td>10</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>andMR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:主存地址编号(8位)</th><th>右操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>011</td><td>11</td><td>xxxxxxxx</td><td>xxx</td></tr></tbody></table></div><h3 id="or"><a href="#or" class="headerlink" title="or"></a><strong><em>or</em></strong></h3><p>同mov, 分为以下几种情况:</p><p><strong>orRI</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:立即数(8位)</th></tr></thead><tbody><tr><td>100</td><td>00</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>orRR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>空码</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>100</td><td>01</td><td>00000</td><td>xxx</td><td>xxx</td></tr></tbody></table></div><p><strong>orRM</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:寄存器编号(3位)</th><th>右操作数:主存地址编号(8位)</th></tr></thead><tbody><tr><td>100</td><td>10</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>orMR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>左操作数:主存地址编号(8位)</th><th>右操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>100</td><td>11</td><td>xxxxxxxx</td><td>xxx</td></tr></tbody></table></div><h3 id="not"><a href="#not" class="headerlink" title="not"></a><strong><em>not</em></strong></h3><p><strong>notR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>操作数:寄存器编号(3位)</th></tr></thead><tbody><tr><td>101</td><td>00</td><td>xxx</td></tr></tbody></table></div><p><strong>notM</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>空码</th><th>操作数:主存地址编号(8位)</th></tr></thead><tbody><tr><td>101</td><td>01</td><td>xxx</td><td>xxxxxxxx</td></tr></tbody></table></div><h3 id="jmp"><a href="#jmp" class="headerlink" title="jmp"></a><strong><em>jmp</em></strong></h3><p><strong>jmpR</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>空码</th><th>操作数:主存地址编号(8位)</th></tr></thead><tbody><tr><td>110</td><td>00</td><td>000</td><td>xxxxxxxx</td></tr></tbody></table></div><p><strong>jmpD</strong></p><div class="table-container"><table><thead><tr><th>操作码</th><th>寻址特征</th><th>空码</th><th>操作数:主存地址编号(8位)</th></tr></thead><tbody><tr><td>110</td><td>01</td><td>000</td><td>xxxxxxxx</td></tr></tbody></table></div><h3 id="hlt"><a href="#hlt" class="headerlink" title="hlt"></a><strong><em>hlt</em></strong></h3><p>无操作数</p><div class="table-container"><table><thead><tr><th>操作码</th><th>空码</th></tr></thead><tbody><tr><td>111</td><td>00000</td></tr></tbody></table></div><h1 id="CPU逻辑框图"><a href="#CPU逻辑框图" class="headerlink" title="CPU逻辑框图"></a>CPU逻辑框图</h1><p><img src="DesignCPUbyMyself.assets/CPUlogic-1561474520649.jpg" srcset="/img/loading.gif" alt=""></p><h1 id="基于硬布线方式的控制器设计"><a href="#基于硬布线方式的控制器设计" class="headerlink" title="基于硬布线方式的控制器设计"></a>基于硬布线方式的控制器设计</h1><p>由于采用定长三级时序, 每个指令周期包含三个机器周期(取指周期, 间址周期和执行周期), 每个周期由三个节拍构成:</p><ul><li>命名取指周期为FF, 间址周期命名为IND, 执行周期命名为EX</li><li><p>三个节拍分别命名为$T_0, T_1, T_2$</p></li><li><p>I为间址标志. 若FF周期的$T_2$时刻测得I=1, 则IND触发器置”1”, 标志进入间址周期. 若I=0, 则EX触发器置”1”, 标志进入执行周期. 若在IND时期的$T_2$时刻测得IND=0, 则EX置”1”, 表示进入执行周期, 表示只有一次间址; 若IND=1, 则表示多次间址, 继续进行间址寻址操作.</p></li></ul><p>根据每一条指令的节拍安排, 总结出表(见附件DesignCPUHard.xls, 由于表格太大了, 文档里放不下)</p><p>化简得出以下微操作命令的表达式:</p><p><img src="DesignCPUbyMyself.assets\IMG_7317.JPG" srcset="/img/loading.gif" alt=""></p><p><img src="DesignCPUbyMyself.assets\IMG_7318.JPG" srcset="/img/loading.gif" alt="IMG_7318"></p><p><img src="DesignCPUbyMyself.assets\IMG_7319.JPG" srcset="/img/loading.gif" alt="IMG_7319"></p><p><img src="DesignCPUbyMyself.assets\IMG_7320.JPG" srcset="/img/loading.gif" alt="IMG_7320"></p><p><img src="DesignCPUbyMyself.assets\IMG_7321.JPG" srcset="/img/loading.gif" alt="IMG_7321"></p><h1 id="基于微程序方式的控制器设计"><a href="#基于微程序方式的控制器设计" class="headerlink" title="基于微程序方式的控制器设计"></a>基于微程序方式的控制器设计</h1><p>微程序，将一条机器指令编写成一个微程序，每个微程序包括若干条微指令，每一条微指令对应一个或几个微操作命令。然后把这些微程序存到一个控制寄存器中，用寻找用户程序机器指令的方法来寻找每个微程序中的微指令。<br>微程序设计省去了组合逻辑设计中队逻辑表达式的化简步骤，无需考虑逻辑门级数等，用规整的存储逻辑代替不规则的硬接线逻辑来实现计算机控制器的技术。</p><h2 id="微指令格式设计"><a href="#微指令格式设计" class="headerlink" title="微指令格式设计"></a>微指令格式设计</h2><table>    <tr>        <td style='text-align:center;'>2位</td>        <td style='text-align:center;'>2位</td>        <td style='text-align:center;'>5位</td>        <td style='text-align:center;'>7位</td>    </tr>    <tr>        <td colspan="3" style='text-align:center;'>控制操作字段</td>        <td style='text-align:center;'>顺序操作字段</td>    </tr></table><p>微指令的操作字段采用字段直接编码方式，该字段发出各种控制信号。顺序控制字段仅在微程序的最后指向取值微程序，它可指出下条微指令的地址（简称下地址），以控制微指令序列的执行顺序。</p><p>前2位存放取值微程序相关的微操作</p><div class="table-container"><table><thead><tr><th style="text-align:center">00</th><th style="text-align:center">无操作</th></tr></thead><tbody><tr><td style="text-align:center">01</td><td style="text-align:center">PC-&gt;MAR</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">M(MAR)-&gt;MDR</td></tr><tr><td style="text-align:center">11</td><td style="text-align:center">MDR-&gt;IR</td></tr></tbody></table></div><p>中间2位存放一些读、写信号的置有效和程序计算器等微操作</p><div class="table-container"><table><thead><tr><th style="text-align:center">000</th><th style="text-align:center">无操作</th></tr></thead><tbody><tr><td style="text-align:center">001</td><td style="text-align:center">1-&gt;R</td></tr><tr><td style="text-align:center">010</td><td style="text-align:center">(PC)+1-&gt;PC</td></tr><tr><td style="text-align:center">011</td><td style="text-align:center">1-&gt;W</td></tr></tbody></table></div><p>中间5位存放转移、加减和取反等操作</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center">00000</td><td style="text-align:center">无操作</td></tr><tr><td style="text-align:center">00001</td><td style="text-align:center">MDR-&gt;ACC</td></tr><tr><td style="text-align:center">00010</td><td style="text-align:center">JDR-&gt;ACC</td></tr><tr><td style="text-align:center">00011</td><td style="text-align:center">AD(IR)-&gt;MAR</td></tr><tr><td style="text-align:center">00100</td><td style="text-align:center">AD(IR)-&gt;JAR</td></tr><tr><td style="text-align:center">00101</td><td style="text-align:center">M(JAR)-&gt;JDR</td></tr><tr><td style="text-align:center">00110</td><td style="text-align:center">M(MAR)-&gt;MDR</td></tr><tr><td style="text-align:center">00111</td><td style="text-align:center">MDR-&gt;M(JAR)</td></tr><tr><td style="text-align:center">01000</td><td style="text-align:center">JDR-&gt;M(MAR)</td></tr><tr><td style="text-align:center">01001</td><td style="text-align:center">ACC-&gt;M(MAR)</td></tr><tr><td style="text-align:center">01010</td><td style="text-align:center">ACC-&gt;M(JAR)</td></tr><tr><td style="text-align:center">01011</td><td style="text-align:center">LATCH-&gt;M(MAR)</td></tr><tr><td style="text-align:center">01100</td><td style="text-align:center">LATCH-&gt;M(JAR)</td></tr><tr><td style="text-align:center">01101</td><td style="text-align:center">MDR+(ACC)-&gt;LATCH</td></tr><tr><td style="text-align:center">01110</td><td style="text-align:center">JDR+(ACC)-&gt;LATCH</td></tr><tr><td style="text-align:center">01111</td><td style="text-align:center">MDR-(ACC)-&gt;LATCH</td></tr><tr><td style="text-align:center">10000</td><td style="text-align:center">JDR-(ACC)-&gt;LATCH</td></tr><tr><td style="text-align:center">10001</td><td style="text-align:center">MDR&amp;(ACC)-&gt;LATCH</td></tr><tr><td style="text-align:center">10010</td><td style="text-align:center">JDR&amp;(ACC)-&gt;LATCH</td></tr><tr><td style="text-align:center">10011</td><td style="text-align:center">MDR(ACC)-&gt;LATCH”</td></tr><tr><td style="text-align:center">10100</td><td style="text-align:center">JDR(ACC)-&gt;LATCH”</td></tr><tr><td style="text-align:center">10101</td><td style="text-align:center">LATCH-&gt;M(MAR)</td></tr><tr><td style="text-align:center">10110</td><td style="text-align:center">LATCH-&gt;M(JAR)</td></tr><tr><td style="text-align:center">10111</td><td style="text-align:center">!(ACC)-&gt;LATCH</td></tr><tr><td style="text-align:center">11000</td><td style="text-align:center">(PC)+AD(IR)-&gt;EAR</td></tr><tr><td style="text-align:center">11001</td><td style="text-align:center">EAR-&gt;PC</td></tr><tr><td style="text-align:center">11010</td><td style="text-align:center">AD(IR)-&gt;PC</td></tr><tr><td style="text-align:center">11011</td><td style="text-align:center">无效</td></tr><tr><td style="text-align:center">11100</td><td style="text-align:center">无效</td></tr><tr><td style="text-align:center">11101</td><td style="text-align:center">无效</td></tr><tr><td style="text-align:center">11110</td><td style="text-align:center">无效</td></tr><tr><td style="text-align:center">11111</td><td style="text-align:center">无效</td></tr></tbody></table></div><h2 id="微程序"><a href="#微程序" class="headerlink" title="微程序"></a>微程序</h2><p><img src="DesignCPUbyMyself.assets\IMG_7322.JPG" srcset="/img/loading.gif" alt=""></p><div class="table-container"><table><thead><tr><th style="text-align:center">PC-&gt;BUS-&gt;MAR,1-&gt;R</th><th style="text-align:center">M+1</th><th style="text-align:center">M</th><th style="text-align:center">取值周期微程序</th></tr></thead><tbody><tr><td style="text-align:center">M(MAR)-&gt;MDR,(PC)+1-&gt;PC</td><td style="text-align:center">M+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MDR-&gt;BUS-&gt;IR</td><td style="text-align:center">M+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">OP(IR)-&gt;BUS-&gt;ID</td><td style="text-align:center">M+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">A+1</td><td style="text-align:center">A</td><td style="text-align:center">movRI周期微程序</td></tr><tr><td style="text-align:center">DA(IR)-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">B+1</td><td style="text-align:center">B</td><td style="text-align:center">movRR周期微程序</td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">B+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;JAR</td><td style="text-align:center">B+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR，1-&gt;R</td><td style="text-align:center">C+1</td><td style="text-align:center">C</td><td style="text-align:center">movRM周期微程序  (间址)</td></tr><tr><td style="text-align:center">M(MAR)-&gt;MDR</td><td style="text-align:center">C+2</td><td style="text-align:center"></td><td style="text-align:center">（间址）</td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">C+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MDR-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">D+1</td><td style="text-align:center">D</td><td style="text-align:center">movMR周期微程序</td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">D+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR</td><td style="text-align:center">D+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(MAR)-&gt;BUS-&gt;JDR</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">DA(IR)-&gt;BUS-&gt;ACC</td><td style="text-align:center">E+1</td><td style="text-align:center">E</td><td style="text-align:center">addRI周期微程序</td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">E+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;MDR</td><td style="text-align:center">E+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MDR+(ACC)-&gt;LATCH</td><td style="text-align:center">E+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">F+1</td><td style="text-align:center">F</td><td style="text-align:center">addRR周期微程序</td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">F+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">F+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">F+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">F+5</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR+(ACC)-&gt;LATCH</td><td style="text-align:center">F+6</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR</td><td style="text-align:center">G+1</td><td style="text-align:center">G</td><td style="text-align:center">addRM周期微程序(间址)</td></tr><tr><td style="text-align:center">M(MAR)-&gt;BUS-&gt;MDR</td><td style="text-align:center">G+2</td><td style="text-align:center"></td><td style="text-align:center">（间址)</td></tr><tr><td style="text-align:center">MDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">G+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">G+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">G+5</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR+(ACC)-&gt;LATCH</td><td style="text-align:center">G+6</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(MAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">H+1</td><td style="text-align:center">H</td><td style="text-align:center">addMR周期微程序</td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR</td><td style="text-align:center">H+2</td><td style="text-align:center"></td><td style="text-align:center">(间址)</td></tr><tr><td style="text-align:center">M(MAR)-&gt;MDR</td><td style="text-align:center">H+3</td><td style="text-align:center"></td><td style="text-align:center">(间址)</td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">H+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">H+5</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MDR+(ACC)-&gt;LATCH</td><td style="text-align:center">H+6</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">DA(IR)-&gt;BUS-&gt;ACC</td><td style="text-align:center">I+1</td><td style="text-align:center">I</td><td style="text-align:center">subRI周期微程序</td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">I+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;MDR</td><td style="text-align:center">I+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MDR-(ACC)-&gt;LATCH</td><td style="text-align:center">I+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">J+1</td><td style="text-align:center">J</td><td style="text-align:center">subRR周期微程序</td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">J+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">J+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">J+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">J+5</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-(ACC)-&gt;LATCH</td><td style="text-align:center">J+6</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR</td><td style="text-align:center">K+1</td><td style="text-align:center">K</td><td style="text-align:center">subRM周期微程序(间址)</td></tr><tr><td style="text-align:center">M(MAR)-&gt;BUS-&gt;MDR</td><td style="text-align:center">K+2</td><td style="text-align:center"></td><td style="text-align:center">（间址)</td></tr><tr><td style="text-align:center">MDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">K+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ID(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">K+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">K+5</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-(ACC)-&gt;LATCH</td><td style="text-align:center">K+6</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(MAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">L+1</td><td style="text-align:center">L</td><td style="text-align:center">subMR周期微程序</td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">L+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">L+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR</td><td style="text-align:center">L+4</td><td style="text-align:center"></td><td style="text-align:center">(间址)</td></tr><tr><td style="text-align:center">M(MAR)-&gt;MDR</td><td style="text-align:center">L+5</td><td style="text-align:center"></td><td style="text-align:center">(间址)</td></tr><tr><td style="text-align:center">MDR-(ACC)-&gt;LATCH</td><td style="text-align:center">L+6</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">DA(IR)-&gt;BUS-&gt;ACC</td><td style="text-align:center">N+1</td><td style="text-align:center">N</td><td style="text-align:center">andRI周期微程序</td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">N+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;MDR</td><td style="text-align:center">N+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MDR&amp;(ACC)-&gt;LATCH</td><td style="text-align:center">N+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">O+1</td><td style="text-align:center">O</td><td style="text-align:center">andRR周期微程序</td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">O+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">O+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">O+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">O+5</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR&amp;(ACC)-&gt;LATCH</td><td style="text-align:center">O+6</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR</td><td style="text-align:center">P+1</td><td style="text-align:center">P</td><td style="text-align:center">andRM周期微程序(间址)</td></tr><tr><td style="text-align:center">M(MAR)-&gt;BUS-&gt;MDR</td><td style="text-align:center">P+2</td><td style="text-align:center"></td><td style="text-align:center">（间址)</td></tr><tr><td style="text-align:center">MDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">P+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ID(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">P+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">P+5</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR&amp;(ACC)-&gt;LATCH</td><td style="text-align:center">P+6</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(MAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">Q+1</td><td style="text-align:center">Q</td><td style="text-align:center">andMR周期微程序</td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">Q+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">Q+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR</td><td style="text-align:center">Q+4</td><td style="text-align:center"></td><td style="text-align:center">(间址)</td></tr><tr><td style="text-align:center">M(MAR)-&gt;MDR</td><td style="text-align:center">Q+5</td><td style="text-align:center"></td><td style="text-align:center">(间址)</td></tr><tr><td style="text-align:center">MDR&amp;(ACC)-&gt;LATCH</td><td style="text-align:center">Q+6</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">DA(IR)-&gt;BUS-&gt;ACC</td><td style="text-align:center">R+1</td><td style="text-align:center">R</td><td style="text-align:center">orRI周期微程序</td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">R+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;MDR</td><td style="text-align:center">R+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MDR\</td><td style="text-align:center">(ACC)-&gt;LATCH</td><td style="text-align:center">R+4</td><td style="text-align:center"></td><td></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">S+1</td><td style="text-align:center">S</td><td style="text-align:center">orRR周期微程序</td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">S+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">S+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">S+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">S+5</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR\</td><td style="text-align:center">(ACC)-&gt;LATCH</td><td style="text-align:center">S+6</td><td style="text-align:center"></td><td></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR</td><td style="text-align:center">T+1</td><td style="text-align:center">T</td><td style="text-align:center">orRM周期微程序(间址)</td></tr><tr><td style="text-align:center">M(MAR)-&gt;BUS-&gt;MDR</td><td style="text-align:center">T+2</td><td style="text-align:center"></td><td style="text-align:center">（间址)</td></tr><tr><td style="text-align:center">MDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">T+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">ID(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">T+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">T+5</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR\</td><td style="text-align:center">(ACC)-&gt;LATCH</td><td style="text-align:center">T+6</td><td style="text-align:center"></td><td></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(MAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR</td><td style="text-align:center">U+1</td><td style="text-align:center">U</td><td style="text-align:center">orMR周期微程序(间址)</td></tr><tr><td style="text-align:center">M(MAR)-&gt;MDR</td><td style="text-align:center">U+2</td><td style="text-align:center"></td><td style="text-align:center">(间址)</td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">U+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">U+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">U+5</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MDR\</td><td style="text-align:center">(ACC)-&gt;LATCH</td><td style="text-align:center">U+6</td><td style="text-align:center"></td><td></td></tr><tr><td style="text-align:center">LATCH-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;JAR</td><td style="text-align:center">V+1</td><td style="text-align:center">V</td><td style="text-align:center">notR周期微程序</td></tr><tr><td style="text-align:center">M(JAR)-&gt;JDR</td><td style="text-align:center">V+2</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">JDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">V+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">!(ACC)-&gt;LATCH</td><td style="text-align:center">V+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;BUS-&gt;M(JAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;BUS-&gt;MAR</td><td style="text-align:center">W+1</td><td style="text-align:center">W</td><td style="text-align:center">notM周期微程序(间址)</td></tr><tr><td style="text-align:center">M(MAR)-&gt;MDR</td><td style="text-align:center">W+2</td><td style="text-align:center"></td><td style="text-align:center">（间址）</td></tr><tr><td style="text-align:center">MDR-&gt;BUS-&gt;ACC</td><td style="text-align:center">W+3</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">!(ACC)-&gt;LATCH</td><td style="text-align:center">W+4</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">LATCH-&gt;Bus-&gt;M(MAR)</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">(PC)+AD(IR)-&gt;EAR</td><td style="text-align:center">X+1</td><td style="text-align:center">X</td><td style="text-align:center">jmpR周期微程序</td></tr><tr><td style="text-align:center">EAR-&gt;BUS-&gt;PC</td><td style="text-align:center">M</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">AD(IR)-&gt;PC</td><td style="text-align:center">M</td><td style="text-align:center">Y</td><td style="text-align:center"></td></tr><tr><td style="text-align:center"></td><td style="text-align:center">M</td><td style="text-align:center">Z</td><td style="text-align:center">htl周期微程序</td></tr></tbody></table></div><p>authors：薛轲翰，谢其骏</p>]]></content>
    
    
    <categories>
      
      <category>计算机组成原理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CPU</tag>
      
      <tag>Course</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读 - DroNet: Deep learning 在无人机导航中的应用</title>
    <link href="/2019/06/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Learning%E5%9C%A8%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%AF%BC%E8%88%AA%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <url>/2019/06/08/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Deep-Learning%E5%9C%A8%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%AF%BC%E8%88%AA%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>DroNet: Learning to Fly by Driving 提出了一个结构非常简单但是又非常强大的网络结构，可以通过输入的每帧图像输出当前飞行器 yaw 的目标角度值与前方障碍物的概率值，从而可以利用这两个信息推断出飞行器当前运动时的 yaw 应转角度 $\theta_k$ 与前进飞行速度 $v_k$，从而达到自主导航的目的。</p><a id="more"></a><h1 id="DroNet-Learning-to-Fly-by-Driving"><a href="#DroNet-Learning-to-Fly-by-Driving" class="headerlink" title="DroNet: Learning to Fly by Driving"></a>DroNet: Learning to Fly by Driving</h1><blockquote><p>A. Loquercio, A.I. Maqueda, C.R. Del Blanco, D. Scaramuzza<br>DroNet: Learning to Fly by Driving,<br>IEEE Robotics and Automation Letters (RA-L), 2018.<br><a href="http://rpg.ifi.uzh.ch/docs/RAL18_Loquercio.pdf" target="_blank" rel="noopener">PDF</a> <a href="https://youtu.be/ow7aw9H4BcA" target="_blank" rel="noopener">YouTube</a> <a href="https://github.com/uzh-rpg/rpg_public_dronet" target="_blank" rel="noopener">Software and Datasets</a>.</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>DroNet 是一个结构非常简单但是又非常强大的网络结构，可以通过输入的每帧图像输出当前飞行器 yaw 的目标角度值与前方障碍物的概率值，从而可以利用这两个信息推断出飞行器当前运动时的 yaw 应转角度 $\theta_k$ 与前进飞行速度 $v_k$，从而达到自主导航的目的。对比论文发布当时的其他相关网络模型，达到了最好的准确度与处理速度的平衡。</p><p><img src="/assets/1560001100081.png" srcset="/img/loading.gif" alt="1560001100081"></p><p>该系统在非机载处理资源上运行 （Intel Core i7 2.6 GHz CPU）上可以达到 30Hz 的控制指令输出。可想如果机载选择 TX2 此类处理器, 采用 GPU 进行推理计算的话, 速度会更快。 </p><p>该模型的训练数据采用室外场景下在地面交通工具上采集的数据集，比如自行车, 汽车等在城市环境内第一视角的图像与其他数据。实验结果惊奇的发现该方法不仅在室外不同视角下表现极好（5m飞行高度），在室内场景中也有极强的泛化能力。飞机可以在没有先验信息的环境中也可以有一个非常好的导航效果。</p><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><p>文章以“无人机应该像其他地面交通工具一样, 在 roadway 中有相同的 behavior”为出发点，通过来自于地面交通工具的数据集，做了以下主要工作：</p><ul><li>提出了一种 residual convolutional architecture (DroNet)，可以预测飞机要转的偏航角与前方发生与障碍物碰撞的概率，可以为飞机在城市环境中提供安全的飞行。通过来自于室外场景下汽车，自行车的数据集来训练该网络。</li><li>建立了一个关于预测是前方障碍物概率的数据集。</li><li>可以到到很好的 performance 和 real-time。</li><li>通过一些扩展场景的检验，发现该系统的泛化能力极强，可以在没有任何先验信息的环境中正常运行，包括数据集中没有的室内走廊场景，一个高高度（视角不一样）等场景.</li></ul><p>文章作者还提到该方法并不是为了替代传统的 map-localize-plan 方法, 作者认为将来有一天传统方法与基于深度学习的方法会互补.</p><h2 id="关于网络与训练方法"><a href="#关于网络与训练方法" class="headerlink" title="关于网络与训练方法"></a>关于网络与训练方法</h2><p><strong>网络的结构图如下:</strong></p><p><img src="/assets/Screenshot from 2019-06-08 19-56-55.png" srcset="/img/loading.gif" alt=""></p><p>输出的转角与障碍物信息两个功能在前面共用同一个网络结构（共享同一套参数）。输入的图像是一张200x200x1 的灰度图，通过一个 5x5 的卷积核降维后，通过三层 res block，然后经过 dropout（作者实验时设的是0.5）后再分叉,  通过 Relu 后作用于两个全连接层（节点数为 1）分别输出信息（大小范围均在0-1之间）。关于为何有这样的网络设计的想法，作者并未多提。</p><blockquote><p>通过代码看到最终 collision 这个全连接节点最终输出还被作用了一个 sigmod 函数，这个论文中并未提到为何。猜测是用于归一化（概率值必须大于 0 小于 1，而刚开始训练的时候 loss 大多依据（后面会讲到） steering 的 loss，可能会出现 [-1, 0] 之间的数，所以要归一化到 (0,1)） / 某种意义上的数据增强？具体只能等跑训练代码的时候看一下实际该参数的值。</p><p>cnn_models.py</p><pre><code class="hljs python"><span class="hljs-number">85</span>    <span class="hljs-comment"># Collision channel</span><span class="hljs-number">86</span>    coll = Dense(output_dim)(x)<span class="hljs-number">87</span>    coll = Activation(<span class="hljs-string">'sigmoid'</span>)(coll)</code></pre></blockquote><p><strong>关于训练方法:</strong></p><p>关于转角的预测本质是一个回归问题，关于障碍物的检测本质是一个二分类问题（虽然最后输出的是概率, 但是从数据集本质来看是一个二分类问题，这个后续详述）。该网络比较特殊（两种不同的问题模型的输出，共享网络），所以要设计出一种合理的 loss 函数. </p><p>根据两类问题的本质，转角预测本质为回归问题所以采用均方误差（MSE）衡量 loss；障碍物概率预测本质为二分类问题所以采用二值交叉熵（BCE）来衡量学习到的分布与样本真实分布的差异，作为 loss。但是整个网络不能简单使用两个 loss 叠加来来作为最终的 loss，会导致特别差的收敛结果，因为回归问题和分类问题在模型刚开始训练的时候，梯度大小差异非常大<a href="https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf" target="_blank" rel="noopener">参考文献</a>。</p><p>实际中，回归问题的梯度在刚开始的时候会非常大，MSE 的梯度正比于转角的误差值。所以策略就是刚开始的时候几乎只选择用转角的 loss，后面随着 epoch 的增加慢慢增大障碍物概率检测的 loss 的权重，等到两者 loss 在一个数量级可比的时候，optimizer 就会自动为两者找到一个很好的 solution。该方式的解释也在上一个参考文献链接里，不设权重或者权重恒定的方法都会导致不好的结果或者收敛时间过长，所以依据此作者提出了下面的 loss：</p><script type="math/tex; mode=display">L_{t o t}=L_{M S E}+\max \left(0,1-\exp ^{-d e c a y\left(e p o c h-e p o c h_{0}\right)}\right) L_{B C E}</script><p>该方式就可以达到上面所说的期望的训练过程中的 loss 函数变化情况。作者在实验时选择 $decay=\frac{1}{10}$,  $epoch_{0}=10$。</p><p>optimizer 选择 Adam，初始学习率设为0.001，decay=1e-5。</p><p>最后作者为 optimization 还采用了 hard negative mining 来建立负样本集, 在每一个 epoch 中选择 loss 最高的 k 个样本, 采用上面计算 loss 的式子计算整体 loss。k 会随着时间会减小。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>关于转角预测（Steering angle）采用来自 Udacity’s project 的公共数据集，该数据集是基于汽车拍摄的。里面有三个摄像头以及 imu，gps，steering angle 等其他同步的数据，作者只选用前置摄像头与 steering angle 作为模型训练时所采用的数据。</p><p>关于用于计算障碍物概率的数据集，由于没有合适的数据集，作者们自制了一套相关数据集，137 个场景序列中包含了 32000 张图片。根据视野障碍物是否离得特别近来标注 0（无碰撞风险）和 1（有碰撞风险）。例图如下，绿色表示无碰撞风险，红色表示有碰撞风险。</p><p><img src="/assets/1560001239435.png" srcset="/img/loading.gif" alt="1560001239435"></p><h2 id="飞机运动控制方法"><a href="#飞机运动控制方法" class="headerlink" title="飞机运动控制方法"></a>飞机运动控制方法</h2><p>整体的导航思路很简单，飞机一直在同一高度飞行，只控制飞机的两个自由度，机体坐标系下前进的速度 $v_k$ 和世界坐标系下的 yaw 值 $\theta_k$.</p><ul><li><p>根据网络输出的前方发生与障碍物碰撞的概率 $p_t$ 计算前进的速度 $v_k$:</p><script type="math/tex; mode=display">v_{k}=(1-\alpha) v_{k-1}+\alpha\left(1-p_{t}\right) V_{\max }</script><p>公式很简单, 即前方发生碰撞概率越大, 前进的速度越低, 发生碰撞概率为 1 的时候速度为0.然后加了低通滤波使速度输出更平滑($0&lt;\alpha&lt;1$). </p></li><li><p>根据网络输出的 steering angle 换算成实际要转的偏航角大小。网络输出的范围为 [-1, 1]，换算成$\left[-\frac{\pi}{2}, \frac{\pi}{2}\right]$。然后也同理加了个低通滤波：</p><script type="math/tex; mode=display">\theta_{k}=(1-\beta) \theta_{k-1}+\beta \frac{\pi}{2} s_{k}</script></li></ul><p>然后根据这两个值赋给飞机就可以控制飞机运动了。作者在试验中选择的 $\alpha=0.7$ 和 $\beta=0.5$。$V_{\max }$根据实验场景不同选择合适的值即可。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>作者在大量场景中进行了测试，具体的测试结果这里不在赘述。</p><p>DroNet 模型是一个平衡结果准确性与运算速度的最佳的模型。</p>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep-Learning</tag>
      
      <tag>Navigation</tag>
      
      <tag>UAV</tag>
      
      <tag>Robotics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NVIDIA Jetson Xavier 通过 USB Ethernet 实现 ssh 功能与高带宽进行 sftp 文件传输</title>
    <link href="/2019/05/08/NVIDIA-Jetson-Xavier-%E9%80%9A%E8%BF%87-USB-Ethernet-%E5%AE%9E%E7%8E%B0-ssh-%E5%8A%9F%E8%83%BD%E4%B8%8E%E9%AB%98%E5%B8%A6%E5%AE%BD%E8%BF%9B%E8%A1%8C-sftp-%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93/"/>
    <url>/2019/05/08/NVIDIA-Jetson-Xavier-%E9%80%9A%E8%BF%87-USB-Ethernet-%E5%AE%9E%E7%8E%B0-ssh-%E5%8A%9F%E8%83%BD%E4%B8%8E%E9%AB%98%E5%B8%A6%E5%AE%BD%E8%BF%9B%E8%A1%8C-sftp-%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93/</url>
    
    <content type="html"><![CDATA[<p>通过阅读 Xavier 刷机后提供的文档，发现它的背面的 type-C 的 usb 口可以实现 ssh 与 sftp 功能，hosts可以通过链接桥连接的方式来与 Xavier 建立链接。</p><a id="more"></a><ol><li>首先当然通过那个 type-C 的口把 Xavier 和你的电脑连起来.</li><li>然后你会看到你的 Network Manager中就会多出来两个”网卡”, 然后在 NVIDIA 的那个下面建立网络.类型选择正常的 Ethernet 即可. 配置DHCP(自动配置ip) 和手动都可以,手动配置的时候注意一下配置好 netmask 和 gateway. Xavier那边的 ip 是192.168.55.1, 觉得手动麻烦的话最好 DHCP 方式即可.建立好连接后就可以用 Xavier 的192.168.55.1这个 ip 即可.ssh 和 scp 都可以正常用.(scp时的带宽是真的大…瞬间几十M甚至可能上百M)</li></ol><p>发现的bug:</p><p>连着这个 usb 网络时不能正常 ssh 登录其他 Jetson 设备…比如登另一块 TX2 的时候就一直 Permission denied…文档里好像说这个问题了,解决办法待更新.</p>]]></content>
    
    
    <categories>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Onboard-Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模式识别实践 - K-L变换实现人脸识别</title>
    <link href="/2019/05/06/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%AE%9E%E8%B7%B5-KL%E5%8F%98%E6%8D%A2%E5%AE%9E%E7%8E%B0%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    <url>/2019/05/06/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%AE%9E%E8%B7%B5-KL%E5%8F%98%E6%8D%A2%E5%AE%9E%E7%8E%B0%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<p>本篇介绍使用K-L变换实现一个简单的人脸识别算法。完整的代码、报告和数据见Github：<a href="https://github.com/kehanXue/pca-face-recognition" target="_blank" rel="noopener">https://github.com/kehanXue/pca-face-recognition</a> 。K-L变换也常称为主成分变换(PCA)，是一种基于图像统计特性的变换，它的协方差矩阵除对角线以外的元素都是零(所以大家也叫它最佳变换)，消除了数据之间的相关性。</p><a id="more"></a><h1 id="应用K-L变换在OCL库中进行人脸识别"><a href="#应用K-L变换在OCL库中进行人脸识别" class="headerlink" title="应用K-L变换在OCL库中进行人脸识别"></a>应用K-L变换在OCL库中进行人脸识别</h1><h2 id="一-原理简述与程序框图"><a href="#一-原理简述与程序框图" class="headerlink" title="一. 原理简述与程序框图"></a>一. 原理简述与程序框图</h2><h3 id="1-原理"><a href="#1-原理" class="headerlink" title="1. 原理"></a>1. 原理</h3><h4 id="1-1-K-L变换"><a href="#1-1-K-L变换" class="headerlink" title="1.1 K-L变换"></a>1.1 K-L变换</h4><blockquote><p>K-L变换也常称为主成分变换(PCA)，是一种基于图像统计特性的变换，它的协方差矩阵除对角线以外的元素都是零(所以大家也叫它最佳变换)，消除了数据之间的相关性，从而在信息压缩方面起着重要作用。</p></blockquote><p>在模式识别和图像处理中一个主要的问题就是降维，在实际的模式识别问题中，我们选择的特征经常彼此相关，在识别这些特征时，数量很多，大部分都是无用的。如果我们能减少特征的数量，即减少特征空间的维数，那么我们将以更少的存储和计算复杂度获得更好的准确性。如何寻找一种合理的综合性方法，使得：</p><ol><li>减少特征量的个数。</li><li>尽量不损失或者稍损失原特征中所包含的信息。</li><li>使得原本相关的特征转化为彼此不相关(用相关系数阵衡量)。</li></ol><p><strong>K-L变换</strong>即主成分分析就可以简化大维数的数据集合。K-L 变换以原始数据的<strong>协方差矩阵的归一化正交特征矢量构成的正交矩阵</strong>作为变换矩阵,对原始数据进行正交变换,在变换域上实现数据压缩。它具有去相关性、能量集中等特性,属于均方误差测度下,失真最小的一种变换,是最能去除原始数据之间相关性的一种变换。它还可以用于许多图像的处理应用中，例如：压缩、分类、特征选择等。</p><p>K-L变换的实质就是去除各维度之间的相关性。就是建立新的坐标系，将原本高度相关的数据在新坐标系下的协方差矩阵除对角线以外的元素都是零。从坐标系的角度来看，图像的矩阵可以看作为二维平面上一组像素点坐标的集合，变换你结果Y可以看作是图像矩阵X在一个新的坐标系下的相同像素点的集合。该新的坐标系为原坐标系的旋转，旋转矩阵即为K-L变换矩阵。</p><blockquote><p>在原坐标系下的x和y具有非常强的相关性，而变换后两者之间的相关性被去除。</p></blockquote><p>K-L变换的目的，即在于找出使得X矢量中的各个分量相关性降低或去除的方向，对图像进行旋转，使其在新空间的坐标轴指向各个主分量方向——主成分分析或者主成分变换。扩展至多维空间，K-L变换可实现多维空间中的去相关，并将能量集中在少数主分量上。</p><p>构建新坐标系的过程就是主成分变换的过程。y代表新坐标系的坐标点，x代表原来坐标系的点。</p><script type="math/tex; mode=display">y=W * x</script><p>其中$W$就是变换矩阵, $W$矩阵就是$x$的协方差矩阵的特征向量矩阵的转置矩阵。</p><h4 id="1-2-主成分选取"><a href="#1-2-主成分选取" class="headerlink" title="1.2 主成分选取"></a>1.2 主成分选取</h4><p>PCA 则是选取协方差矩阵前 k 个最大的特征值的特征向量构成 K-L 变换矩阵。保留多少个主成分取决于保留部分的累积方差在方差总和中所占百分比（即累计贡献率），它标志着前几个主成分概括信息之多寡。实践中,粗略规定一个百分比便可决定保留几个主成分;如果多留一个主成分,累积方差增加无几,便不再多留。</p><h4 id="1-3-人脸空间的建立"><a href="#1-3-人脸空间的建立" class="headerlink" title="1.3 人脸空间的建立"></a>1.3 人脸空间的建立</h4><p>假设一幅人脸图像包含 $N$ 个像素点,它可以用一个 $N$ 维向量 $x$ 表示,这样,训练样本库就可以使用 ${x_i(i=1,…,M)}$ 来表示. 协方差矩阵 $C$ 的正交特征向量就是组成人脸空间的基向量,即特征脸. 将特征值由小到大排列:</p><script type="math/tex; mode=display">\lambda_{1} \geqslant \lambda_{2} \geqslant \ldots \geqslant \lambda_{\mathrm{r}}</script><p>其对应的特征向量为 $\mu_{\mathrm{k}}$. 这样每一幅人脸图像对应于子空间中的一点。同样,子空间的任意一点也对应于一幅图像.</p><h4 id="1-4-人脸识别"><a href="#1-4-人脸识别" class="headerlink" title="1.4 人脸识别"></a>1.4 人脸识别</h4><p>有了这样一个由”特征脸”张成的降维子空间,任何一幅人脸图像都可以向其投影得到一组坐标系数,这组系数表明了该图像在子空间中的位置,从而可以作为人脸识别的依据。</p><ol><li><p>计算数据库中每张图片在子空间中的坐标,得到一组坐标,作为下一步识别匹配的搜索空间。</p></li><li><p>计算新输入图片在子空间中的坐标,采用最小距离法,遍历搜索空间,得到与其距离最小的坐标向量,该向量对应的人脸图像即为识别匹配的结果。</p></li></ol><h3 id="2-实验步骤"><a href="#2-实验步骤" class="headerlink" title="2. 实验步骤"></a>2. 实验步骤</h3><ol><li><p>选取数据集，并将其读入．将每一张图像均reshape为列向量$X_i$,并组合成数据矩阵.</p><script type="math/tex; mode=display">\mathbf{X}=\left(\mathbf{X}_{1}, \mathbf{X}_{2}, \ldots, \mathbf{X}_{n}\right)</script></li><li><p>求均值向量</p><script type="math/tex; mode=display">\boldsymbol{\mu}=\frac{1}{n} \sum_{i}^{n} \boldsymbol{X}_{i}</script></li><li><p>求中心化后的数据矩阵</p><script type="math/tex; mode=display">\mathrm{C}=\left(\mathbf{X}_{1}-\boldsymbol{\mu}, \mathbf{X}_{2}-\boldsymbol{\mu}, \ldots, \mathbf{X}_{n}-\boldsymbol{\mu}\right)</script></li><li><p>求$C^{\top} \mathrm{C}$的协方差矩阵的特征值, 选取出$k$个使得它的总能量达到设定值, 求出特征脸(特征向量)$\mathbf{e}_{\mathbf{i}}$, 将$k$个这样的向量按列排列成变换矩阵$W$</p><script type="math/tex; mode=display">W=\left(\mathbf{e}_{1}, \mathbf{e}_{2}, \ldots, \mathbf{e}_{k}\right)</script></li><li><p>计算每一幅图像的投影($k$维列向量)</p><script type="math/tex; mode=display">Y_{i}=W^{\top}\left(X_{i}-\mu\right)</script></li><li><p>计算待识别的人脸的投影(k维列向量), 设待识别的人脸为$Z$</p><script type="math/tex; mode=display">\operatorname{ch} \mathbf{Z}=\mathbf{W}^{\top}(\mathbf{Z}-\boldsymbol{\mu})</script></li><li><p>遍历搜索进行匹配,找到最近邻的人脸图像,根据标签得到$Z$属于第几个人.</p><script type="math/tex; mode=display">Y_{j}=\min | | Y_{i}-\operatorname{ch} z| |</script></li></ol><h3 id="3-程序框图"><a href="#3-程序框图" class="headerlink" title="3. 程序框图"></a>3. 程序框图</h3><pre><code class="hljs mermaid">graph TB;A(&quot;读入训练集数据，每个人10张照片中选取8张作为训练集&quot;)--&gt;B(&quot;求均值向量avgX&quot;);B--&gt;C(&quot;训练数据中心化&quot;);C--&gt;D(&quot;进行K-L变换，求出特征矩阵与所有特征值&quot;);D--&gt;E(&quot;进行主成分分析，选取构成能力95%的特征值&quot;);E--&gt;F(&quot;得到变换矩阵W，计算出特征脸&quot;);F--&gt;G(&quot;将训练数据投影到该新的特征空间中&quot;);G--&gt;H(&quot;取每个人的两张照片构成测试集，计算准确率&quot;);</code></pre><h2 id="二-实验结果分析"><a href="#二-实验结果分析" class="headerlink" title="二. 实验结果分析"></a>二. 实验结果分析</h2><h3 id="1-训练数据中心化样例"><a href="#1-训练数据中心化样例" class="headerlink" title="1. 训练数据中心化样例"></a>1. 训练数据中心化样例</h3><p>Fig.1.2.1训练数据中心化后的图片.</p><p><img src="/assets/avgFace-1556715073400.jpg" srcset="/img/loading.gif" alt=""></p><p>这里展示其中20张训练数据中心化后的人脸样例.</p><h3 id="2-计算出的特征脸样例"><a href="#2-计算出的特征脸样例" class="headerlink" title="2. 计算出的特征脸样例"></a>2. 计算出的特征脸样例</h3><p>Fig.1.2.2 特征脸样例</p><p><img src="/assets/featureFace.jpg" srcset="/img/loading.gif" alt="特征脸"></p><p>这里展示其中20张特征脸样例. </p><h3 id="3-训练数据投影到新的特征空间"><a href="#3-训练数据投影到新的特征空间" class="headerlink" title="3. 训练数据投影到新的特征空间"></a>3. 训练数据投影到新的特征空间</h3><p>Fig.1.2.3 训练数据投影到新的特征空间后的数据分布图</p><p><img src="/assets/refs-1556715136687.jpg" srcset="/img/loading.gif" alt=""></p><p>​    训练数据投影到新的特征空间后对应的新的数据矩阵,横轴表示维度(选取的主分量的数量),纵轴代表投影后的各个分量的大小,可以看出靠前的分量占得总能量更大.</p><h3 id="4-训练数据的数量对测试分类精确度的影响"><a href="#4-训练数据的数量对测试分类精确度的影响" class="headerlink" title="4. 训练数据的数量对测试分类精确度的影响"></a>4. 训练数据的数量对测试分类精确度的影响</h3><p>我们的训练数据集中一共有40个人的照片,每个人的照片各十张.我们将每个人的后两张图片作为测试样本集.然后根据前面通过改变训练样本的数量来观察其对分类精确度的影响.我们通过改变每个人选取的照片数量$n$来控制训练数据集的大小.我们分别取了$n=3,4,…,8$,绘制出了其精确度变化的曲线.</p><p>Fig.1.2.4 模型的精确度随训练数据集的数量的变化.</p><p><img src="/assets/acc-1555498077760.jpg" srcset="/img/loading.gif" alt=""></p><p>可见,随着训练样本数的增加,模型的精确度也在上升,其中当每个人的照片至少取4张时才可以开始进行比较好的判别.</p><h2 id="三-程序代码"><a href="#三-程序代码" class="headerlink" title="三. 程序代码"></a>三. 程序代码</h2><p>matlab程序代码:</p><pre><code class="hljs matlab">clearclose all;clc<span class="hljs-comment">% 按文件夹读取数据,数据保存到allData中</span>db = dir(<span class="hljs-string">'orl_faces'</span>);face = cell(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>); allData = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>);labelList = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>);acc_index = <span class="hljs-number">0</span>;<span class="hljs-keyword">for</span> train_data_nums = <span class="hljs-number">6</span>:<span class="hljs-number">-1</span>:<span class="hljs-number">2</span>    num = <span class="hljs-number">0</span>;    <span class="hljs-built_in">type</span> = <span class="hljs-number">0</span>;    labelMap = containers.Map(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>);    <span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">4</span> : <span class="hljs-built_in">length</span>(db)        fi = dir([db(<span class="hljs-built_in">i</span>).folder <span class="hljs-string">'/'</span> db(<span class="hljs-built_in">i</span>).name]);        <span class="hljs-built_in">type</span> = <span class="hljs-built_in">type</span> + <span class="hljs-number">1</span>;        <span class="hljs-keyword">for</span> <span class="hljs-built_in">j</span> = <span class="hljs-number">3</span> : <span class="hljs-built_in">length</span>(fi)-train_data_nums            num = num + <span class="hljs-number">1</span>;            face&#123;<span class="hljs-built_in">i</span> - <span class="hljs-number">2</span>, <span class="hljs-built_in">j</span> - <span class="hljs-number">2</span>&#125; = imread([fi(<span class="hljs-built_in">j</span>).folder <span class="hljs-string">'/'</span> fi(<span class="hljs-built_in">j</span>).name]);            labelMap(num) = <span class="hljs-built_in">type</span>;            <span class="hljs-keyword">if</span> num == <span class="hljs-number">1</span>                [imageLen, imageWid] = <span class="hljs-built_in">size</span>(face&#123;<span class="hljs-built_in">i</span> - <span class="hljs-number">2</span>, <span class="hljs-built_in">j</span> - <span class="hljs-number">2</span>&#125;);            <span class="hljs-keyword">end</span>            allData(<span class="hljs-number">1</span>: imageLen * imageWid, num) = double(<span class="hljs-built_in">reshape</span>(face&#123;<span class="hljs-built_in">i</span> - <span class="hljs-number">2</span>, <span class="hljs-built_in">j</span> - <span class="hljs-number">2</span>&#125;, [imageLen * imageWid, <span class="hljs-number">1</span>]));            <span class="hljs-comment">% imshow(face&#123;i - 2, j - 2&#125;);</span>        <span class="hljs-keyword">end</span>        labelList(<span class="hljs-number">1</span>, <span class="hljs-built_in">type</span>) = <span class="hljs-built_in">i</span><span class="hljs-number">-3</span>;    <span class="hljs-keyword">end</span>    <span class="hljs-comment">% 求均值向量</span>    [allDataRows, allDataCols] = <span class="hljs-built_in">size</span>(allData);    <span class="hljs-comment">% avgX = 1.0/allDataCols * sum(allData, 2);</span>    avgX = <span class="hljs-built_in">mean</span>(allData, <span class="hljs-number">2</span>);    <span class="hljs-comment">% plot(avgX);</span>    <span class="hljs-comment">% imshow(reshape(avgX, [imageLen, imageWid]));</span>    <span class="hljs-keyword">for</span> num = <span class="hljs-number">1</span>:allDataCols        allData(:, num) = allData(:, num)-avgX;    <span class="hljs-keyword">end</span>    <span class="hljs-comment">% for num = 1:20</span>    <span class="hljs-comment">%     subplot(4, 5, num);imshow(reshape(allData(:, num), [imageLen, imageWid]));</span>    <span class="hljs-comment">% end</span>    <span class="hljs-comment">% 求协方差矩阵</span>    <span class="hljs-comment">% covC = 1.0/allDataCols * C * C';</span>    covC = allData' * allData;    <span class="hljs-comment">% covC = cov(allData);</span>    [coeffC, latentC, explainedC] = pcacov(covC);    <span class="hljs-comment">% 选取构成能量95%的特征值</span>    numOfLatent = <span class="hljs-number">1</span>;    proportion = <span class="hljs-number">0</span>;    <span class="hljs-keyword">while</span>(proportion &lt; <span class="hljs-number">95</span>)        proportion = proportion + explainedC(numOfLatent);        numOfLatent = numOfLatent+<span class="hljs-number">1</span>;    <span class="hljs-keyword">end</span>    numOfLatent = numOfLatent - <span class="hljs-number">1</span>;    <span class="hljs-comment">% 求特征脸</span>    W = allData*coeffC;    W = W(:, <span class="hljs-number">1</span>:numOfLatent);    <span class="hljs-comment">% plot(W(:), 1);</span>    <span class="hljs-comment">% for k = 1:20</span>    <span class="hljs-comment">%     subplot(4, 5, k);imshow(reshape(W(:, k), [imageLen, imageWid]));</span>    <span class="hljs-comment">% end</span>    <span class="hljs-comment">% 将训练数据投影到该新的特征空间中</span>    reference = W' * allData;    <span class="hljs-comment">% plot(reference);</span>    <span class="hljs-comment">% 测试训练结果准确率</span>    distances = <span class="hljs-built_in">zeros</span>(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>);    testNum = <span class="hljs-number">0</span>;MNIST 数据集来自美国国家标准与技术研究所, National Institute of Standards and Technology (NIST). 训练集 (training set) 由来自 <span class="hljs-number">250</span> 个不同人手写的数字构成, 其中 <span class="hljs-number">50</span><span class="hljs-comment">% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据.</span>    type_index = <span class="hljs-number">0</span>;    accCnt = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">4</span> : <span class="hljs-built_in">length</span>(db)        fi = dir([db(<span class="hljs-built_in">i</span>).folder <span class="hljs-string">'/'</span> db(<span class="hljs-built_in">i</span>).name]);        type_index = type_index + <span class="hljs-number">1</span>;        <span class="hljs-keyword">for</span> <span class="hljs-built_in">j</span> = <span class="hljs-built_in">length</span>(fi)<span class="hljs-number">-1</span> : <span class="hljs-built_in">length</span>(fi)            testNum = testNum + <span class="hljs-number">1</span>;            imgTest = imread([fi(<span class="hljs-built_in">j</span>).folder <span class="hljs-string">'/'</span> fi(<span class="hljs-built_in">j</span>).name]);            imgTest = <span class="hljs-built_in">reshape</span>(imgTest, [imageLen * imageWid, <span class="hljs-number">1</span>]);            imgTest = double(imgTest(:));            imgTest = W' * (imgTest - avgX);            dis = <span class="hljs-built_in">realmax</span>(<span class="hljs-string">'double'</span>);            <span class="hljs-keyword">for</span> k = <span class="hljs-number">1</span>:allDataCols                temp = norm(imgTest - reference(:, k));                distances(:, k) = norm(imgTest - reference(:, k));                <span class="hljs-keyword">if</span>(dis &gt; temp)                    aimOne = k;                    dis = temp;                <span class="hljs-keyword">end</span>            <span class="hljs-keyword">end</span>            <span class="hljs-keyword">if</span> labelMap(aimOne) == type_index                accCnt = accCnt + <span class="hljs-number">1</span>;            <span class="hljs-keyword">end</span>        <span class="hljs-keyword">end</span>    <span class="hljs-keyword">end</span>    <span class="hljs-comment">% plot(distances);</span>    acc_index = acc_index + <span class="hljs-number">1</span>;    accuray(acc_index+<span class="hljs-number">3</span>) = accCnt/testNum;<span class="hljs-keyword">end</span><span class="hljs-comment">% plot(accuray);</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pattern-Recognition</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模式识别入门（一） - 模式识别概论</title>
    <link href="/2019/05/02/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%85%A5%E9%97%A8-%E4%B8%80-%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E6%A6%82%E8%AE%BA/"/>
    <url>/2019/05/02/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%85%A5%E9%97%A8-%E4%B8%80-%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E6%A6%82%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<p>本学期选修了学校自动化学院的张绍武教授《模式识别》课程。此为期末总结时候的复习总结。此篇介绍了模式识别入门的一些常识与概念。（由于期末复习来不及了……后面停更了）</p><a id="more"></a><h1 id="Chapter-1-概论"><a href="#Chapter-1-概论" class="headerlink" title="Chapter 1 概论"></a>Chapter 1 概论</h1><h2 id="1-1-模式识别基本概念"><a href="#1-1-模式识别基本概念" class="headerlink" title="1.1 模式识别基本概念"></a>1.1 模式识别基本概念</h2><h3 id="1-1-1-模式识别的基本定义"><a href="#1-1-1-模式识别的基本定义" class="headerlink" title="1.1.1 模式识别的基本定义"></a>1.1.1 模式识别的基本定义</h3><ul><li><p>样本 (sample, object)</p><p>一类事物的一个具体体现, 对具体的个别事物进行观测所得到的某种形式的信号. 一般用向量表示.</p></li><li><p>模式 (pattern)</p><p>表示一类事物.</p></li></ul><blockquote><p>样本是具体的事物,而模式是对同一类事物概念性的概括</p></blockquote><ul><li><p>模式类与模式联合使用时,模式表示具体的事物,而模式类则是对这一类事物的概念性描述</p><p>模式与模式类的例子:</p><ul><li>模式类: 老年人; 模式: 王老太, 老头, 老太</li><li>模式类: 老头; 模式: 王老头</li><li>模式类: 老太; 模式: 王老太</li></ul></li></ul><p><strong>模式识别</strong> (Pattern Recognition)</p><p>主要研究相似与分类的问题, 用计算机实现对人和各种事物或者现象的分析, 描述, 判断与识别.</p><p><strong><em>&gt; Pattern recognition:</em></strong></p><blockquote><p>is the study of how machines can observe the environment, learn to distinguish patterns of interest from their background, and make sound and reasonable decisions about the categories of the patterns. (Anil K. Jain)</p></blockquote><p><strong>模式识别的作用与目的</strong></p><p>将某一具体事物正确的归入某一类别. 也就是从样本到类别的映射.</p><p><img src="/assets/Screenshot from 2019-05-02 12-42-57.png" srcset="/img/loading.gif" alt=""></p><h3 id="1-1-2-模式识别的发展史"><a href="#1-1-2-模式识别的发展史" class="headerlink" title="1.1.2 模式识别的发展史"></a>1.1.2 模式识别的发展史</h3><ul><li>1929年 G. Tauschek发明阅读机, 能够阅读0-9的数字</li><li>30年代 Fisher提出统计分类理论, 奠定了统计模式识别的基础. 因此, 在60~70年代, 统计模式识别发展很快, 但由于被识别的模式愈来愈复杂, 特征也愈多, 就出现“维数灾难”. 随着计算机运算速度的迅猛发展, 这个问题得到一定解决. 统计模式识别仍是模式识别的主要理论</li><li>50年代 Noam Chemsky 提出形式语言理论, 美籍华人付京荪提出句法结构模式识别</li><li>60年代 L.A. Zadeh提出了模糊集理论,模糊模式识别理论得到了较广泛的应用</li><li>80年代 Hopfield提出神经元网络模型理论.人工神经元网络在模式识别和人工智能上得到较广泛的应用</li><li>90年代 V.N. Vapnik 提出了小样本学习理论,支持向量机也受到了很大的重视</li><li>2012年后, Deep Learning.</li></ul><h3 id="1-1-3-关于模式识别的国内-国际学术组织"><a href="#1-1-3-关于模式识别的国内-国际学术组织" class="headerlink" title="1.1.3 关于模式识别的国内, 国际学术组织"></a>1.1.3 关于模式识别的国内, 国际学术组织</h3><ul><li><p>1973年 IEEE发起了第一次关于模式识别的国际会议“ICPR”,成立了国际模式识别协会—“IAPR”,每2年召开一次国际学术会议</p></li><li><p>1977年 IEEE的计算机学会成立了模式分析与机器智能(PAMI)委员会,每2年召开一次模式识别与图象处理学术会议</p></li><li><p>CVPR, PRCV等</p></li><li><p>电子学会, 通信学会, 自动化学会, 中文信息学会</p><p>(TODO)</p></li></ul><h2 id="1-2-模式识别系统"><a href="#1-2-模式识别系统" class="headerlink" title="1.2 模式识别系统"></a>1.2 模式识别系统</h2><p>执行模式识别的计算机系统称为模式识别系统. 该系统被用来执行模式分类的具体任务.</p><p>一个典型的模式识别系统如下图所示结构框图组成</p><p><img src="/assets/Screenshot from 2019-05-02 12-55-05.png" srcset="/img/loading.gif" alt=""></p><p>一般由<strong>数据获取, 预处理, 特征提取选择, 分类器设计及分类决策</strong>五部分组成.分类器设计在<strong>训练过程</strong>中完成, 利用样本进行训练, 确定分类器的具体参数.而分类决策在<strong>识别过程</strong>中起作用, 对待识别的样本进行分类决策.</p><p><img src="/assets/Screenshot from 2019-05-02 14-19-23.png" srcset="/img/loading.gif" alt=""></p><p><strong>流程样例:</strong></p><p><img src="/assets/Screenshot from 2019-05-02 14-24-49.png" srcset="/img/loading.gif" alt=""></p><p><strong>说明:</strong></p><ul><li>这一系统构造适合于统计模式识别, 模糊模式识别, 人工神经网络中有监督方法</li><li>对于结构模式识别方法,只需用基元提取代替特征提取与选择</li><li>对于聚类分析, <strong>分类器设计与决策合二为一</strong>, 一步完成</li></ul><p><strong>步骤:</strong></p><ul><li><p><strong>信息获取</strong> (data acquisition)</p></li><li><p><strong>预处理</strong> (preprocessing)</p><p>目的: 去除所获取信息中的噪声,增强有用的信息,及一切必要的使信息纯化的处理过程</p><p>常用技术: A\D ,二值化,一维信号滤波去噪,及图象的平滑、变换、增强、恢复、滤波等处理</p></li><li><p><strong>特征选择与提取</strong> (feature selection and extraction)</p><p>一般说来它包括将所获取的原始量测数据转换成能反映事物本质,并将其最有效分类的特征表示</p><p>样本及模式都是用特征来描述,识别与训练在<strong>特征空间</strong>中进行.量测仪器或传感器获取的原始数据组成的空间叫<strong>测量空间</strong>.</p><blockquote><p>特征选择与提取模块的功能是对所获取的信息实现<strong>从测量<br>空间到特征空间的转换</strong></p></blockquote><p>找到合适的特征描述对识别的可靠性,计算复杂度、有效性都是十分重要的</p></li><li><p><strong>分类器设计</strong> (classifier design)</p><p>分类器设计的主要功能是通过训练确定判决规则,按此类判决规则分类时,错误率最低. 把这些判决规则建成标准库.</p></li><li><p><strong>分类决策</strong> (classification decision)</p><p>在特征空间中对被识别的对象进行分类. 分界线的类型可由设计者直接确定,也可通过训练过程产生,但是这些分界线的具体参数则利用训练样本<strong>经训练过程确定</strong></p><ul><li><p>分类决策是对事物辨识的最后一步,其主要方法是计算待识别事物的属性,分析它是否满足是某类事物的条件</p></li><li><p>对于每个事物来说,由它的属性得到它的描述,表示成相应的特征向量,它在特征空间中表示成一个点,称为数据点</p></li><li><p>特征空间的分布中往往表现出同类事物的特征向量聚集在一起,聚集在一个相对集中的区域,而不同事物则分别占据不同的区域</p></li><li><p>待识别的事物,如果它的特征向量出现在某一类事物经常出现或可能出现的区域内,该事物就被识别为该类事物</p></li><li>在特征空间中,哪个区域是某类事物典型所在区域,需要用数学式子划定,这样一来,满足这种数学式子与否就成为分类决策的依据</li><li>如何<strong>确定这些数学式子</strong>就是分类器设计的任务,而一旦这种数学<br>式子确定后,分类决策的方法也就确定了</li></ul></li></ul><h2 id="1-3-模式识别方法"><a href="#1-3-模式识别方法" class="headerlink" title="1.3 模式识别方法"></a>1.3 模式识别方法</h2><ul><li><p>模板匹配方法 (templete matching)</p></li><li><p>统计方法 (statistical pattern recognition)</p></li><li><p>神经网络方法 (neural network)</p></li><li><p>结构方法 (句法方法) (structural pattern recognition)</p></li><li><p>模糊模式识别 (Fuzzy pattern recognition)</p></li><li><p>逻辑推理方法 (Logic inference)</p><p>(TODO 详细的后面补充)</p></li></ul><h2 id="1-4-模式识别应用"><a href="#1-4-模式识别应用" class="headerlink" title="1.4 模式识别应用"></a>1.4 模式识别应用</h2><p>略</p><h2 id="1-5-模式识别基本问题"><a href="#1-5-模式识别基本问题" class="headerlink" title="1.5 模式识别基本问题"></a>1.5 模式识别基本问题</h2><h3 id="1-5-1-模式-样本-表示方法"><a href="#1-5-1-模式-样本-表示方法" class="headerlink" title="1.5.1 模式(样本)表示方法"></a>1.5.1 模式(样本)表示方法</h3><ol><li><p>向量表示: 假设一个样本有n个变量(特征)</p><script type="math/tex; mode=display">X=\left[\mathrm{x}_{1}, \mathrm{x}_{2}, \ldots, \mathrm{x}_{\mathrm{n}}\right]^{\top}</script></li><li><p>矩阵表示</p></li><li>几何表示</li></ol><h3 id="1-5-2-模式类的紧致性"><a href="#1-5-2-模式类的紧致性" class="headerlink" title="1.5.2 模式类的紧致性"></a>1.5.2 模式类的紧致性</h3><ol><li><p>紧致集: 同一类模式类样本的分布比较集中, 没有或者临界样本很少, 这样的模式类称为<strong>紧致集</strong></p><p><img src="/assets/Screenshot from 2019-05-02 20-47-24.png" srcset="/img/loading.gif" alt=""></p></li><li><p>临界点(样本): 在多类样本中, 某些样本值有微小的变化时就变成了另一类样本称为<strong>临界样本(点)</strong></p></li><li><p>紧致集的性质:</p><ol><li>与样本总数相比, 临界点的数量非常少</li><li>集合内任意两点可以用光滑线连线, 在线上的点属于同一集合</li><li>集合内的每一个点都有足够强大的领域, 在邻域内包含同一集合的点</li></ol></li><li><p>模式识别的要求:</p><ol><li>满足紧致集, 才能很好的分类</li><li>如果不能满足紧致集, 就要采取变换的方法来使其满足紧致集.</li></ol><blockquote><p>只要各个模式类是可分的, 总存在这样一个空间, 使变换到这个空间中的集合满足紧致集的要求</p></blockquote></li></ol><h3 id="1-5-3-相似与分类"><a href="#1-5-3-相似与分类" class="headerlink" title="1.5.3 相似与分类"></a>1.5.3 相似与分类</h3><p>两个样本$X<em>{i}, X</em>{j}$之间的相似度量满足以下要求:</p><ol><li>应该为非负值</li><li>样本本身相似性度量应该最大</li><li>度量应满足对称性</li><li>在满足紧致性的条件下, 相似性应该是点间距离的单调函数</li></ol><h4 id="1-5-3-1-用各种距离表示相似性"><a href="#1-5-3-1-用各种距离表示相似性" class="headerlink" title="1.5.3.1 用各种距离表示相似性"></a>1.5.3.1 用各种距离表示相似性</h4><p>已知两个样本:</p><script type="math/tex; mode=display">X_{\mathrm{i}}=\left[\mathrm{x}_{\mathrm{i} 1}, \mathrm{x}_{\mathrm{i} 2}, \mathrm{x}_{\mathrm{i} 3}, \ldots, \mathrm{x}_{\mathrm{in}}\right]^{\top}</script><script type="math/tex; mode=display">X_{\mathrm{j}}=\left[\mathrm{x}_{\mathrm{j} 1}, \mathrm{x}_{\mathrm{j} 2}, \mathrm{x}_{\mathrm{j} 3}, \ldots, \mathrm{x}_{\mathrm{jn}}\right]^{\top}</script><ol><li><p>绝对值距离:</p><script type="math/tex; mode=display">d_{i j}=\sum_{k=1}^{n}\left|X_{i k}-X_{j k}\right|</script></li><li><p>欧几里得距离:</p><script type="math/tex; mode=display">d_{i j}=\sqrt{\sum_{k=1}^{n}\left(X_{i k}-X_{j k}\right)^{2}}</script></li><li><p><strong>明考夫斯基距离</strong>:</p><script type="math/tex; mode=display">d_{i j}(\boldsymbol{q})=\left(\sum_{k=1}^{n}\left|\boldsymbol{X}_{i k}-\boldsymbol{X}_{j k}\right|^{q}\right)^{\mathbf{1} / \boldsymbol{q}}</script><p>其中当q=1时为绝对值距离,当q=2时为欧氏距离</p></li><li><p>切比雪夫距离:</p><script type="math/tex; mode=display">d_{i j}(\infty)=\max _{1 \leq k \leq n}\left|X_{i k}-X_{j k}\right|</script><p>切氏距离为q趋向于无穷大时的明氏距离的极限情况</p></li><li><p><strong>马哈拉诺比斯距离</strong> (Mahalanobis diatance):</p><script type="math/tex; mode=display">d_{i j}(M)=\sqrt{\left(X_{i}-X_{j}\right)^{T} \Sigma^{-1}\left(X_{i}-X_{j}\right)}</script><p>其中${x_i, x_j}$为特征向量, ${\Sigma}$为协方差矩阵.使用条件为样本符合正态分布</p></li><li><p>夹角余弦:</p><script type="math/tex; mode=display">C_{i j}=\cos ^{-1} \frac{\sum_{k=1}^{n} X_{i k} X_{j k}}{\sqrt{\left(\sum_{k=1}^{n} X_{i k}^{2}\right)\left(\sum_{k=1}^{n} X_{j k}^{2}\right)}}</script><p>即之间夹角小的样本具有相似性, 可分为一类.</p></li><li><p>相关系数:</p><script type="math/tex; mode=display">r_{i j}=\frac{\sum_{k=1}^{n}\left(X_{k i}-\overline{X}_{i}\right)\left(X_{k j}-\overline{X}_{j}\right)}{\sqrt{\sum_{k=1}^{n}\left(X_{k i}-\overline{X}_{i}\right)^{2} \sum_{k=1}^{n}\left(X_{k j}-\overline{X}_{j}\right)^{2}}}</script><p>$\overline{X}<em>{i}, \overline{X}</em>{j}$为$X<em>{i}, X</em>{j}$的均值.</p><p><strong>注意</strong>: 在求相关系数之前,要将数据标准化(极差标准化,方差标准化)</p></li></ol><h4 id="1-5-3-2-分类的主观性和客观性"><a href="#1-5-3-2-分类的主观性和客观性" class="headerlink" title="1.5.3.2 分类的主观性和客观性"></a>1.5.3.2 分类的主观性和客观性</h4><ol><li><p>分类带有主观性:目的不同,分类不同</p><p>例: 鲸鱼、牛、马从生物学的角度来讲都属于哺乳类,但是从产业角度来讲鲸鱼属于水产业,牛和马属于畜牧业</p></li><li><p>分类的客观性:科学性</p><p>判断分类必须有客观标准,因此分类是追求客观性的. 但主观性也很难避免,这就是分类的复杂性.</p></li></ol><h4 id="1-5-3-3-特征的生成"><a href="#1-5-3-3-特征的生成" class="headerlink" title="1.5.3.3 特征的生成"></a>1.5.3.3 特征的生成</h4><p>特征是决定相似性与分类的关键,寻找合适的特征是认知与识别的核心问题。可粗略地分为底层、中层、高层3个层次.</p><ol><li><p>低层特征:</p><ol><li>无序尺度: 有明确的数量和数值.</li><li>有序尺度: 有先后、好坏的次序关系,如酒分为上,中,下三个等级.</li><li>名义尺度: 无数量、无次序关系,如有红、黄两种颜色.</li></ol></li><li><p>中层特征:</p><ol><li>经过计算、变换得到的特征</li></ol></li><li><p>高层特征:</p><ol><li><p>在中层特征的基础上有目的的经过运算形成</p><p>例如: 椅子的重量=体积*比重, 体积与长、宽、高有关;比重与材料、纹理、颜色有关.这里低、中、高三层特征都有了.</p></li></ol></li></ol><h4 id="1-5-3-4-数据标准化"><a href="#1-5-3-4-数据标准化" class="headerlink" title="1.5.3.4 数据标准化"></a>1.5.3.4 数据标准化</h4><ol><li><p>极差标准化</p><p>极差: 一批样本中,每个特征的最大值与最小值之差.</p><script type="math/tex; mode=display">R_{i}=\max _{j}\left\{x_{i j}\right\}-\min _{j}\left\{x_{i j}\right\}</script><p>极差标准化两种方式:</p><script type="math/tex; mode=display">X_{i j}=\left( \begin{array}{l}{x_{i j}-\overline{x}_{i} ) /_{R_{i}}}\end{array}\right.</script><script type="math/tex; mode=display">X_{i j}=\frac{x_{i j}-\min _{j}\left\{x_{i j}\right\}}{R_{i}}</script></li><li><p>方差标准化</p><p>${S_i}$为方差:</p><script type="math/tex; mode=display">\overline{x}_{i}=\frac{1}{n} \sum_{j=1}^{n} x_{i j}</script><script type="math/tex; mode=display">S_{i}=\sqrt{\frac{1}{n-1} \sum_{j=1}^{n}\left(x_{i j}-\overline{x}_{i}\right)^{2}}</script><p>方差标准化:</p><script type="math/tex; mode=display">X_{i j}=\left(x_{i j}-\overline{x}_{i}\right) / S_{i}</script></li><li><p>归一化标准化</p><script type="math/tex; mode=display">X_{i j}=^{x_{i j}} / \sum_{j=1}^{n} x_{i j}</script></li></ol><p>标准化的方法很多,原始数据是否应该标准化,应采用什么方法标准化,都要根据具体情况来定.</p><p>Reference: 西北工业大学自动化学院张绍武教授《模式识别》课程PPT</p>]]></content>
    
    
    <categories>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pattern-Recognition</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模式识别入门（零） - 线性代数与概率论数学基础</title>
    <link href="/2019/05/02/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%85%A5%E9%97%A8-%E9%9B%B6-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A6%82%E7%8E%87%E8%AE%BA%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"/>
    <url>/2019/05/02/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%85%A5%E9%97%A8-%E9%9B%B6-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%8E%E6%A6%82%E7%8E%87%E8%AE%BA%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<p>本学期选修了学校自动化学院的张绍武教授《模式识别》课程。此为期末总结时候的复习总结。此篇主要包含了一些数学常识与基础。</p><a id="more"></a><h1 id="Chapter-0-数学基础"><a href="#Chapter-0-数学基础" class="headerlink" title="Chapter 0 数学基础"></a>Chapter 0 数学基础</h1><h2 id="0-1-行列式与线性方程组"><a href="#0-1-行列式与线性方程组" class="headerlink" title="0.1 行列式与线性方程组"></a>0.1 行列式与线性方程组</h2><p>解行列式: </p><ol><li>按行 (列) 展开计算</li><li>化为三角行列式计算</li></ol><pre><code class="hljs matlab">det(A)</code></pre><p>解线性方程组:</p><script type="math/tex; mode=display">\left\{\begin{array}{c}{a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}=b_{1}} \\ {a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}=b_{2}} \\ {\ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots} \\ {a_{n 1} x_{1}+a_{n 2} x_{2}+\cdots+a_{n n} x_{n}=b_{n}}\end{array}\right.</script><ol><li><p>若系数行列式$\mathrm{D}=\left|a_{i j}\right| \neq 0$, 则方程组存在唯一解</p><script type="math/tex; mode=display">x_{1}=\frac{D_{1}}{D}, x_{2}=\frac{D_{2}}{D}, \cdots, x_{n}=\frac{D_{n}}{D}</script></li><li><p>若为齐次方程组($b=0$), 有非零解的充要条件是$\mathrm{D}=\left|a_{i j}\right|=0$</p></li></ol><h2 id="0-2-矩阵"><a href="#0-2-矩阵" class="headerlink" title="0.2 矩阵"></a>0.2 矩阵</h2><script type="math/tex; mode=display">A_{m \times n}=\left[a_{i j}\right]_{m} \times_{\mathrm{n}}​</script><p>方阵:</p><script type="math/tex; mode=display">m=n</script><p>对角阵:</p><script type="math/tex; mode=display">\wedge=\operatorname{diag}\left(a_{11}, a_{22}, \ldots, a_{n n}\right)</script><pre><code class="hljs matlab"><span class="hljs-built_in">diag</span>(A)</code></pre><p>单位阵:</p><script type="math/tex; mode=display">E=\operatorname{diag}(1,1, \dots, 1)</script><pre><code class="hljs matlab"><span class="hljs-built_in">eye</span>(n)</code></pre><p>上三角与下三角阵: (TODO)</p><pre><code class="hljs matlab"><span class="hljs-built_in">triu</span>(A)<span class="hljs-built_in">tril</span>(A)</code></pre><h2 id="0-3-矩阵运算"><a href="#0-3-矩阵运算" class="headerlink" title="0.3 矩阵运算"></a>0.3 矩阵运算</h2><p>矩阵乘法 C=AB:</p><script type="math/tex; mode=display">c_{i j}=\sum_{k} a_{i k} b_{k j}</script><p>矩阵的转置:</p><script type="math/tex; mode=display">\mathrm{A}=\left(\mathrm{a}_{i j}\right), \mathrm{A}^{\prime}=\mathrm{A}^{\mathrm{T}}=\left(\mathrm{a}_{j i}\right)</script><p>对称方阵:</p><script type="math/tex; mode=display">\mathrm{A}^{\prime}=\mathrm{A}, 即 \mathrm{a}_{i j}=\mathrm{a}_{j i}</script><p>方阵的行列式性质:</p><ol><li>如果$|\mathrm{A}| \neq 0$, $A$ 称为非奇异阵, 否则为奇异阵.</li><li>$\left|A^{\prime}\right|=|A|, \quad|A B|=|A||B|$</li></ol><p>逆矩阵:</p><p>如果$AB = BA = E$, 则称$A$可逆, $B$为$A$的逆.</p><p>方阵$A$可逆的充要条件为</p><script type="math/tex; mode=display">|\mathrm{A}| \neq 0</script><pre><code class="hljs matlab">B = inv(A)</code></pre><h2 id="0-4-分块矩阵及其运算"><a href="#0-4-分块矩阵及其运算" class="headerlink" title="0.4 分块矩阵及其运算"></a>0.4 分块矩阵及其运算</h2><p>用横线和竖线把矩阵分成若干小块,每个小块为一个矩阵,它可以作为一个元素参加运算。</p><script type="math/tex; mode=display">\left[ \begin{array}{ccccc}{1} & {0} & {\vdots} & {1} & {0} \\ {0} & {1} & {\vdots} & {0} & {1} \\ {\cdots} & {\cdots} & {\vdots} & {\cdots} & {\cdots} \\ {0} & {0} & {\vdots} & {1} & {2} \\ {0} & {0} & {\vdots} & {2} & {1}\end{array}\right] = \left[ \begin{array}{ll}{E_{2}} & {E_{2}} \\ {O} & {B_{22}}\end{array}\right]</script><p>分块对角阵(TODO):</p><script type="math/tex; mode=display">|\mathrm{A}|=\left|\mathrm{A}_{11}\right|\left|\mathrm{A}_{22}\right| \ldots\left|\mathrm{A}_{\mathrm{rr}}\right|</script><pre><code class="hljs matlab">A = <span class="hljs-built_in">blkdiag</span>(A11, A22, ..., Arr)A11 = A(<span class="hljs-number">1</span>:m, <span class="hljs-number">1</span>:n)</code></pre><h2 id="0-5-向量"><a href="#0-5-向量" class="headerlink" title="0.5 向量"></a>0.5 向量</h2><p>n维向量:</p><script type="math/tex; mode=display">\mathbf{x}=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{\mathrm{T}}</script><p>线性相关与线性无关:</p><p>设有n维向量组: $\mathbf{x}<em>{1}, \mathbf{x}</em>{2}, \dots, \mathbf{x}<em>{\mathrm{m}}$, 如果只有当$k</em>{1}=k<em>{2}=\ldots=k</em>{\mathrm{m}}=0$时, 才能使得下式成立, 则称该向量组线性无关, 否则则称线性相关.</p><script type="math/tex; mode=display">k_{1} \mathbf{x}_{1}+k_{2} \mathbf{x}_{2}+\cdots+k_{m} \mathbf{x}_{m}=0</script><p>m个n维的向量的矩阵表示:</p><script type="math/tex; mode=display">\mathrm{A}=\left(\mathbf{a}_{1}, \mathbf{a}_{2}, \ldots, \mathbf{a}_{\mathrm{m}}\right)</script><p>n个n维向量: $\boldsymbol{a}<em>{i}=\left(\mathbf{a}</em>{\mathbf{i} 1}, \mathbf{a}<em>{\mathbf{i} 2}, \ldots, \mathbf{a}</em>{\mathbf{i n}}\right)^{\mathrm{T}}$线性无关的充要条件是</p><script type="math/tex; mode=display">| \mathrm{A}\mathrm  | \neq 0</script><h2 id="0-6-向量-二"><a href="#0-6-向量-二" class="headerlink" title="0.6 向量(二)"></a>0.6 向量(二)</h2><p>若满足下式, 则称A可以由向量组B线性表示:</p><script type="math/tex; mode=display">A=\left(a_{1}, a_{2}, \dots, a_{m}\right)=\left(b_{1}, b_{2}, \dots, b_{m}\right) C= BC</script><p>向量组的秩:(TODO)</p><script type="math/tex; mode=display">\operatorname{rank}(\mathrm{A})=\mathrm{nor}(A的最大线性无关组)</script><ol><li>向量组$\alpha_1, \alpha_2 … \alpha_s$线性无关等价于$\operatorname{rank}(\mathrm{\alpha_1, \alpha_2 … \alpha_s}) = s$</li><li>等价的向量组具有相同的秩</li><li>任意n+1个n维向量线性相关</li></ol><p>向量的内积:</p><script type="math/tex; mode=display">(\mathbf{x}, \mathbf{y})=\sum_{i=1}^{n} x_{i} y_{i}=\mathbf{x}^{T} \mathbf{y}</script><p>向量的模(范数/长度):</p><script type="math/tex; mode=display">|\mathbf{x}|=\sqrt{\mathbf{x}^{T} \mathbf{x}}</script><p>两点的距离:</p><script type="math/tex; mode=display">d\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)=\left|\mathbf{x}_{2}-\mathbf{x}_{1}\right|=\sqrt{\left(\mathbf{x}_{2}-\mathbf{x}_{1}\right)^{T}\left(\mathbf{x}_{2}-\mathbf{x}_{1}\right)}</script><p>两个向量的夹角:</p><script type="math/tex; mode=display">\theta=<\mathbf{x}_{1}, \mathbf{x}_{2}>=\arccos \frac{\mathbf{x}_{1}^{T} \mathbf{x}_{2}}{\left|\mathbf{x}_{1}\right|\left|\mathbf{x}_{2}\right|}</script><h2 id="0-7-向量-三"><a href="#0-7-向量-三" class="headerlink" title="0.7 向量(三)"></a>0.7 向量(三)</h2><p>两个向量正交:</p><script type="math/tex; mode=display">(\mathbf{x}, \mathbf{y})=0, \cos (\theta)=0</script><p>若非零的n维向量$\mathbf{x}<em>{1}, \mathbf{x}</em>{2}, \dots, \mathbf{x}_{\mathrm{m}}$两两正交, 则称为<strong>正交向量组</strong>. 正交向量组队的性质:</p><ol><li><p>正交向量组线性无关</p></li><li><p>若n维向量y可以由正交向量组$\mathbf{x}<em>{1}, \mathbf{x}</em>{2}, \dots, \mathbf{x}_{\mathrm{m}}$线性表示, 则</p><script type="math/tex; mode=display">\mathbf{y}=\frac{\mathbf{y}^{T} \mathbf{x}_{1}}{\mathbf{x}_{1}^{T} \mathbf{x}_{1}} \mathbf{x}_{1}+\frac{\mathbf{y}^{T} \mathbf{x}_{2}}{\mathbf{x}_{2}^{T} \mathbf{x}_{2}} \mathbf{x}_{2}+\cdots+\frac{\mathbf{y}^{T} \mathbf{X}_{m}}{\mathbf{x}_{m}^{T} \mathbf{x}_{m}} \mathbf{x}_{m}</script></li></ol><h2 id="0-8-向量-四"><a href="#0-8-向量-四" class="headerlink" title="0.8 向量(四)"></a>0.8 向量(四)</h2><p>向量空间:</p><p>对加法和乘法运算均封闭的非空向量集合称为一个向量空间.</p><p>向量空间$V$的基: 向量空间的任一向量都可以由线性无关的向量组$a<em>{1}, a</em>{2}, \dots, a<em>{r}$线性表示, 则称向量组$a</em>{1}, a<em>{2}, \dots, a</em>{r}$为$V$的基, $\operatorname{dim} \mathrm{V}=\mathrm{r}$</p><p>向量空间$V$中的任意一个向量$z$可由它的基唯一线性表示, 有序组$\left(x<em>{1}, x</em>{2}, \ldots, x_{r}\right)$称为向量$z$在该基下的坐标.</p><script type="math/tex; mode=display">z=x_{1} \mathbf{a}_{1}+x_{2} \mathbf{a}_{2}+\ldots+x_{r} \mathbf{a}_{r}</script><p>基变换与坐标变换:</p><script type="math/tex; mode=display">\left(\beta_{1} \beta_{2} \ldots \beta_{r}\right)=\left(\alpha_{1} \alpha_{2} \ldots a_{r}\right) C</script><h2 id="0-9-矩阵的特征值与特征向量"><a href="#0-9-矩阵的特征值与特征向量" class="headerlink" title="0.9 矩阵的特征值与特征向量"></a>0.9 矩阵的特征值与特征向量</h2><p>方阵A的特征值$\lambda$与特征向量$\alpha$:</p><script type="math/tex; mode=display">\mathrm{A}{\alpha}=\lambda {\alpha}</script><ol><li>设α是方阵A的属于特征值λ的特征向量,则$kα$也是A的属于特征值λ的特征向量.</li><li>方阵A的两个不同特征值所对应的特征向量是线性无关的</li></ol><p>方阵$A$的特征矩阵$A-λE$和特征多项式$|A-λE|$和特征多项式$|A-λE|$.</p><p>仿真$A$的特征方程:</p><script type="math/tex; mode=display">|A-\lambda E|=0</script><p>特征方程$|A-\lambda E|=0$的解$\lambda$为方阵$A$的特征值, 方程$(\mathrm{A}-\lambda \mathrm{E}) \mathrm{x}=0$的非零解向量就是方阵$A$的属于特征值$\lambda$的特征向量.</p><h2 id="0-10-相似矩阵"><a href="#0-10-相似矩阵" class="headerlink" title="0.10 相似矩阵"></a>0.10 相似矩阵</h2><p>如果存在可逆方阵$P$,使$P^{-1} A P=B$,则称A与B相似,记作$A \sim B$</p><ol><li>相似关系具有反身, 对称, 传递性.</li><li>相似矩阵有相同的行列式, 即$|\mathrm{A}|=|\mathrm{B}|$</li><li>相似矩阵有相同的特征多项式及特征值</li></ol><p>n阶方阵$A$与对角矩阵$\wedge$相似的充要条件是$A$有n个线性无关的特征向量.</p><p>如果$\mathrm{A} \sim \wedge$, 即有</p><script type="math/tex; mode=display">\mathrm{P}^{-1} \mathrm{AP}=\wedge=\operatorname{diag}\left(\mathrm{d}_{1}, \mathrm{d}_{2}, \ldots, \mathrm{d}_{\mathrm{n}}\right)</script><p>, 则$\mathrm{d}<em>{1}, \mathrm{d}</em>{2}, \ldots, \mathrm{d}_{\mathrm{n}}$是$A$的$n$个特征值.</p><p><strong>实对称矩阵</strong>:(TODO)</p><p>如果有n阶矩阵A, 其矩阵的元素都为实数,且矩阵A的转置等于其本身($a<em>{ij} = a</em>{ji}$)(i,j为元素的脚标), 则称A为实对称矩阵</p><ol><li>特征值为实数, 特征向量为实向量</li><li>两个相异的特征值对应的特征向量正交</li><li>n阶实对称方阵$A$有n个线性无关的特征向量</li><li>n阶实对称方阵$A$与对角矩阵相似, 即n阶实对称矩阵A必可对角化,且相似对角阵上的元素即为矩阵本身特征值</li></ol><h2 id="0-11-正交矩阵"><a href="#0-11-正交矩阵" class="headerlink" title="0.11 正交矩阵"></a>0.11 正交矩阵</h2><p>正交矩阵$A$, 有$A A^{\prime}=E$, 即$A^{-1}=A^{\prime}$</p><ol><li>正交矩阵$A, B$的乘积$AB$仍为正交矩阵</li><li>正交矩阵$A$的行列式$|\mathrm{A}|=1$</li></ol><p>正交矩阵$A$的行(列)向量组为正交单位向量组, 即:</p><script type="math/tex; mode=display">\left( \begin{array}{c}{\boldsymbol{a}_{1}} \\ {\boldsymbol{a}_{2}} \\ {\vdots} \\ {\boldsymbol{a}_{n}}\end{array}\right)\left(\boldsymbol{a}_{1}^{\prime}, \boldsymbol{a}_{2}^{\prime}, \cdots, \boldsymbol{a}_{n}^{\prime}\right)=E</script><script type="math/tex; mode=display">\boldsymbol{a}_{i} \boldsymbol{a}_{j}^{T}=\delta_{i j}</script><p>若$A$为实对称矩阵, 则一定存在<strong>正交矩阵$P$</strong>, 使得$P^{-1} A P=\wedge$, $\wedge$是以<strong>$A$的特征值为对角元素的对角矩阵</strong>.</p><h2 id="0-12-二次型"><a href="#0-12-二次型" class="headerlink" title="0.12 二次型"></a>0.12 二次型</h2><p>二次齐次函数：</p><script type="math/tex; mode=display">f\left(x_{1}, x_{2}, \cdots, x_{n}\right)=\sum_{i=1}^{n} a_{i j} x_{i} x_{j}, a_{i j}=a_{j i}</script><p>记$\mathbf{x}=\left(x<em>{1}, x</em>{2}, \ldots x<em>{n}\right)^{\top}$, $A=\left(a</em>{i j}\right)_{n^{*} n}$, 则有</p><script type="math/tex; mode=display">f\left(x_{1}, x_{2}, \cdots, x_{n}\right)=\mathbf{x}^{\prime} A \mathbf{x}</script><p>二次型$f$与对称矩阵$A$存在一一对应: $A$为二次型$f$的矩阵, $f$为矩阵$A$的二次型.</p><blockquote><p>$A=\wedge$时为标准二次型(只含平方项)</p></blockquote><p>对于任何二次型, 总可以找到正交变换将$f$化为标准型</p><script type="math/tex; mode=display">f\left(x_{1}, x_{2}, \cdots, x_{n}\right)=\mathbf{x}^{\prime} \mathbf{A} \mathbf{x}</script><script type="math/tex; mode=display">\mathbf{x}=C \mathbf{y}</script><script type="math/tex; mode=display">\wedge=C^{\prime} A C</script><script type="math/tex; mode=display">f=\mathbf{y}^{\prime} \wedge \mathbf{y}=\lambda_{1} y_{1}^{2}+\lambda_{2} y_{2}^{2}+\cdots \lambda_{n} y_{n}^{2}</script><h2 id="0-13-正定二次型和正定矩阵"><a href="#0-13-正定二次型和正定矩阵" class="headerlink" title="0.13 正定二次型和正定矩阵"></a>0.13 正定二次型和正定矩阵</h2><p>二次型$f\left(\mathrm{x}<em>{1}, \mathrm{x}</em>{2}, \dots, \mathrm{x}<em>{n}\right)$, 如果对于任何$\mathrm{x}</em>{1}^{2}+\mathrm{x}<em>{2}^{2}+\ldots+\mathrm{x}</em>{n}^{2} \neq 0$, 都有$f&gt;0$, 则称$f$为正定二次型.其矩阵$A$为正定矩阵($A&gt;0$).</p><p>n阶方阵$A$正定的充要条件是: A的n个特征值全为正数.</p><p>n阶方阵$A$, 若存在可逆矩阵$B$, 使得$A=B^{\prime} B$, 则$A$为正定矩阵.</p><p>意义(TODO)</p><h2 id="0-14-多元随机变量的统计特征"><a href="#0-14-多元随机变量的统计特征" class="headerlink" title="0.14 多元随机变量的统计特征"></a>0.14 多元随机变量的统计特征</h2><p>ｎ维随机变量: </p><script type="math/tex; mode=display">\mathbf{x}=\left[x_{1}, x_{2}, \ldots, x_{n}\right]^{\mathrm{T}}</script><p>n维随机变量的(总体)均值:</p><script type="math/tex; mode=display">\boldsymbol{\mu}=\mathrm{E}(\mathbf{x})=\int_{\mathbf{x}} \mathbf{x} p(\mathbf{x}) d \mathbf{x}</script><p>n维随机变量的(样本)均值:</p><script type="math/tex; mode=display">\hat{\boldsymbol{\mu}}=\frac{1}{N} \sum_{i=1}^{N} \mathbf{x}_{i}</script><p>n维随机变量的(总体)相关函数矩阵:(TODO)</p><script type="math/tex; mode=display">\mathrm{R}(\mathbf{x})=\left[r_{i j}\right]=\left[\mathrm{E}\left\{x_{i} x_{j}\right\}\right]=\mathrm{E}\left\{\mathbf{x} \mathbf{x}^{T}\right\}</script><p>n维随机变量的(样本)相关函数矩阵:</p><script type="math/tex; mode=display">\hat{\mathrm{R}}(\mathbf{x})=\frac{1}{N} \sum_{i=1}^{N} \mathbf{x}_{i} \mathbf{x}_{i}^{T}</script><p>n维随机变量的(总体)协方差矩阵:</p><script type="math/tex; mode=display">\mathrm{C}(\mathbf{x})=\left[c_{i j}\right]=\left[\mathrm{E}\left\{\left(x_{i}-\mu_{i}\right)\left(x_{j}-\mu_{j}\right)\right\}\right]=\mathrm{E}\left\{(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{T}\right\}</script><p>n维随机变量的(样本)协方差矩阵:</p><script type="math/tex; mode=display">\hat{\mathrm{C}}(\mathbf{x})=\frac{1}{N} \sum_{i=1}^{N}\left(\mathbf{x}_{i}-\boldsymbol{\mu}_{i}\right)\left(\mathbf{x}_{i}-\boldsymbol{\mu}_{i}\right)^{T}</script><h2 id="0-15-n维随机变量协方差矩阵的性质"><a href="#0-15-n维随机变量协方差矩阵的性质" class="headerlink" title="0.15 n维随机变量协方差矩阵的性质"></a>0.15 n维随机变量协方差矩阵的性质</h2><p>ｎ维随机变量的协方差矩阵$C$是实对称矩阵</p><ol><li><p>协方差矩阵$C$的特征值为实数</p></li><li><p>$C$有$n$个线性无关的特征向量</p></li><li><p>存在正交矩阵$U$, 使得$\mathrm{U}^{-1} \mathrm{CU}=\mathrm{U}^{\mathrm{T}} \mathrm{CU}=\wedge$, $\wedge$是以$C$的特征值为对角元素的对角矩阵, </p><script type="math/tex; mode=display">\mathrm{U}=\left[\mathbf{u}_{1}, \mathbf{u}_{2}, \ldots, \mathbf{u}_{\mathrm{n}}\right], \quad \mathrm{C} \mathbf{u}_{\mathrm{i}}=\lambda_{\mathrm{i}} \mathbf{u}_{\mathrm{i}}</script></li></ol><h2 id="0-16-梯度下降法"><a href="#0-16-梯度下降法" class="headerlink" title="0.16 梯度下降法"></a>0.16 梯度下降法</h2><p>准则函数: $J(\mathbf{a})$</p><p>最优化问题: $\mathbf{a}^{*}=\underset{\mathbf{a}}{\operatorname{argmin}} J(\mathbf{a})$</p><p>求解方法: $\mathbf{a}^{*}$应满足方程:</p><script type="math/tex; mode=display">\nabla J(\mathbf{a})=\left[ \begin{array}{cc}{\frac{\partial J}{\partial a_{1}}} & {\frac{\partial J}{\partial a_{2}}} & {\cdots} & {\frac{\partial J}{\partial a_{n}}}\end{array}\right]^{T}=0</script><p>沿梯度的负方向改变$\mathbf{a}$, 函数会很快达到极小点, 梯度趋于0.故迭代算法为:</p><script type="math/tex; mode=display">\mathbf{a}_{k+1}=\mathbf{a}_{k}-\eta \nabla J(\mathbf{a})</script><p>流程图:</p><pre><code class="hljs mermaid">graph TB;A(&quot;选择初始点a_0,给定容许误差ε,设定学习率η.设k&#x3D;0&quot;)--&gt;B[&quot;计算梯度▽J(a_k)&quot;];B--&gt;C[&quot;修改a_k: a_k+1 &#x3D; a_k - η▽J(a_k)&quot;];C--&gt;D&#123;&quot;计算J(a_k+1),并检验|J(a_k+1)-J(a_k)|&lt;ε&quot;&#125;;D--No--&gt;E&gt;&quot;k&#x3D;k+1&quot;];E--&gt;B;D--Yes--&gt;F[&quot;输出结果&quot;]F--&gt;G(&quot;结束&quot;)</code></pre><p>Reference: 西北工业大学自动化学院张绍武教授《模式识别》课程PPT</p>]]></content>
    
    
    <categories>
      
      <category>模式识别</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pattern-Recognition</tag>
      
      <tag>Math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用 Autoware 进行双目相机与激光雷达的联合标定</title>
    <link href="/2019/04/02/%E4%BD%BF%E7%94%A8Autoware%E8%BF%9B%E8%A1%8C-%E5%8F%8C%E7%9B%AE-%E7%9B%B8%E6%9C%BA%E4%B8%8E%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%9A%84%E8%81%94%E5%90%88%E6%A0%87%E5%AE%9A/"/>
    <url>/2019/04/02/%E4%BD%BF%E7%94%A8Autoware%E8%BF%9B%E8%A1%8C-%E5%8F%8C%E7%9B%AE-%E7%9B%B8%E6%9C%BA%E4%B8%8E%E6%BF%80%E5%85%89%E9%9B%B7%E8%BE%BE%E7%9A%84%E8%81%94%E5%90%88%E6%A0%87%E5%AE%9A/</url>
    
    <content type="html"><![CDATA[<p>使用 Autoware 提供的 autoware_camera_lidar_calibrator 工具进行进行 VLP-16 与 ZED Camera 两者之间的标定。</p><a id="more"></a><p>在用Autoware提供的工具进行标定之前,搜索了很多的标定工具,但是看其他的方法或多或少都有一点点的麻烦,比如还要制作比较大的标定板等等,而使用 Autoware 则比较简单, 可以直接通过手动对齐图像中的像素点与激光雷达的 3D points 来进行标定. 标定结束后感觉该方式对于 VLP-16 这种较为稀疏的多线激光雷达来说,标定的精度可能不是很高.</p><h1 id="安装Autoware"><a href="#安装Autoware" class="headerlink" title="安装Autoware"></a>安装Autoware</h1><p>官方推荐使用 Docker 安装，我选择直接源码装了。<a href="https://github.com/autowarefoundation/autoware/wiki/Demo" target="_blank" rel="noopener">跑Demo的地址</a>中有一个 Build Sources 的<a href="https://github.com/CPFL/Autoware/wiki/Source-Build" target="_blank" rel="noopener">链接</a>是源码编译的步骤。源码编译其实也不麻烦，Autoware 是基于 ROS 搞的，rosdep 依赖一装然后跑编译脚本即可。中间没出什么问题，反而Docker的时候因为要装 NVIDIA Docker（因为要用 GPU），笔记本 ubuntu 端一牵扯显卡啥的就老出一堆问题。后面懒得折腾了就直接源码编译了一遍 Autoware。</p><h1 id="开始标定工作"><a href="#开始标定工作" class="headerlink" title="开始标定工作"></a>开始标定工作</h1><p>Autoware装好后，source 一下它的setup.zsh，就可以正常使用了。它提供了 <code>autoware_camera_lidar_calibrator</code> 可以用来联合标定相机与激光雷达。联合标定分为两步走:</p><ol><li>获取相机的内参</li><li>获得相机-Lidar的外参</li></ol><h1 id="标定相机内参"><a href="#标定相机内参" class="headerlink" title="标定相机内参"></a>标定相机内参</h1><p>单目和双目相机均可以用 <code>autoware_camera_lidar_calibrator</code> 来标定。还需准备一个棋盘格。</p><p>标定单目:</p><pre><code class="hljs bash">rosrun autoware_camera_lidar_calibrator cameracalibrator.py --square SQUARE_SIZE --size MxN image:=/image_topic</code></pre><p>标定双目:</p><pre><code class="hljs bash">rosrun autoware_camera_lidar_calibrator cameracalibrator.py --square SQUARE_SIZE  --size MxN right:=/image_topic left:=/image_topic</code></pre><p>参数说明:</p><p><code>--square</code> :棋盘格中的每个方格的边长大小。单位为m</p><p><code>--size</code> :棋盘格的尺寸是几乘几。注意是 <strong>inner</strong> ，也就是出去边长最外圈方格数-1。如果这个参数设置不对的话会在下面的标定步骤中发现标定程序毫无反应</p><p><code>image</code> :发布图像的话题名</p><p><code>right</code>,<code>left</code> : 左右眼图像的话题名。</p><p>最后发现它的参数其实还有一个 <code>camera_info</code>，但是好像是zed的包的小bug或者zed launch时未加标定文件吧，zed的节点发布的这个话题中是没有消息的。标定的时候也没有加这个topic，目前看起来没啥影响。</p><p><img src="/assets/autoware-1.png" srcset="/img/loading.gif" alt="相机内参标定"></p><p>上述节点启动起来后，会弹出一个图像框，右边有几个按钮，通过晃动棋盘格使得右边的 <code>Calibration</code> 按钮变绿。把棋盘格拿到相机前，看到棋盘格上有了mark并且右上方出现了四个滑动条类似的东西，分别表示X、Y、尺度与俯仰，根据提示哪一个自由度完成度不足来移动棋盘格。等 <code>Calibration</code> 按钮变绿了之后就可以点击一下，命令行窗口会给出计算出的标定结果，再点击Save按钮即可保存成在home目录下的命名类似于20190401_1133_autoware_camera_calibration.yaml文件。</p><blockquote><p>我Save的时候报了个错说没有cv2有关的那个对象没write方法，然后pip重新装一下opencv-python即可</p></blockquote><p>标定双目相机的流程与上图类似。结果被打包成为一个压缩包，里面分别有左右眼的参数和ost。但这个结果在后面的相机雷达联合标定中是无法使用的。后面的相机雷达联合标定需要用到的是单目标定出的那个 yaml 参数文件.</p><h1 id="相机雷达联合标定"><a href="#相机雷达联合标定" class="headerlink" title="相机雷达联合标定"></a>相机雷达联合标定</h1><p>相机与雷达的联合标定是要使用上一步的相机内参标定结果的。运行命令如下:</p><pre><code class="hljs bash">roslaunch autoware_camera_lidar_calibrator camera_lidar_calibration.launch intrinsics_file:=/PATH/TO/YYYYmmdd_HHMM_autoware_camera_calibration.yaml image_src:=/image</code></pre><p>参数说明:</p><p><code>intrinsics_file</code> :相机内参标定结果的yaml文件</p><p><code>image_src</code> :为发布图像信息的话题</p><p>启动后会出现一个显示已经纠正过的相机图像的弹框（该弹框需要ROS的image-view2组件，报错找不到这个node的话apt装一下就好了）。</p><p>然后启动雷达开启rviz调出雷达扫描的点云图。然后通过寻找图像窗口中图像的像素点与雷达点云数据的对应关系，先点击图像上的像素点，然后在rviz中通过 <code>publish point</code> 工具点击雷达点云中对应的3D点，看命令行是会有反应/输出信息的。寻找9组后会把生成的标定结果文件生成到home目录下，命名格式为20190401_115333_autoware_lidar_camera_calibration.yaml。</p><p><img src="/assets/autoware-2.png" srcset="/img/loading.gif" alt="相机雷达外参标定"></p><p>标定完9组后自动在home目录下生成结果文件:</p><p><img src="/assets/autoware-3.png" srcset="/img/loading.gif" alt="相机雷达外参标定2"></p>]]></content>
    
    
    <categories>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Robotics</tag>
      
      <tag>Autoware</tag>
      
      <tag>ROS</tag>
      
      <tag>Calibration</tag>
      
      <tag>Multi-Sensors</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NVIDIA Jetson TX2 or AGX Xavier apt 切换国内的源</title>
    <link href="/2019/03/10/Jetson%20TX2%20or%20AGX%20Xavier%E6%8D%A2%E5%9B%BD%E5%86%85%E6%BA%90/"/>
    <url>/2019/03/10/Jetson%20TX2%20or%20AGX%20Xavier%E6%8D%A2%E5%9B%BD%E5%86%85%E6%BA%90/</url>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="NVIDIA-Jetson-TX2-or-AGX-Xavier-apt-切换国内的源"><a href="#NVIDIA-Jetson-TX2-or-AGX-Xavier-apt-切换国内的源" class="headerlink" title="NVIDIA Jetson TX2 or AGX Xavier apt 切换国内的源"></a>NVIDIA Jetson TX2 or AGX Xavier apt 切换国内的源</h1><p>日期可能稍稍有些久远导致不能用，但目前还可以用。</p><h2 id="1-首先备份下之前的-source-list"><a href="#1-首先备份下之前的-source-list" class="headerlink" title="1. 首先备份下之前的 source.list"></a>1. 首先备份下之前的 source.list</h2><pre><code class="hljs bash">sudo cp /etc/apt/sources.list /etc/apt/sources.list.backup</code></pre><h2 id="2-修改-source-list"><a href="#2-修改-source-list" class="headerlink" title="2. 修改 source.list"></a>2. 修改 source.list</h2><p>换源需要换 <strong>ARM</strong> 的源，不要换成了 PC 平台的软件源。这里推荐的两个国内源为清华和中科大的源，将原来文件里面的内容全部替换成下面两个源之一就可以. 并且由于 TX2 的系统是16.04(xenial)的,Xavier 的系统是18.04(bionic), 所以两者的源还要换个系统版本.</p><p>Xavier清华的:</p><pre><code class="hljs bash">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-updates main restricted universe multiversedeb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-updates main restricted universe multiversedeb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-security main restricted universe multiversedeb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-security main restricted universe multiversedeb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-backports main restricted universe multiversedeb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic-backports main restricted universe multiversedeb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic main universe restricteddeb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ bionic main universe restricted</code></pre><p>Jetson TX2清华的:</p><pre><code class="hljs bash">deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates main restricted universe multiverse  deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates main restricted universe multiverse   deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security main restricted universe multiverse   deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security main restricted universe multiverse   deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse   deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse   deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial main universe restricted   deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial main universe restricted</code></pre><p>Xavier中科大的:</p><pre><code class="hljs bash">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-updates main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-updates main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-security main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-security main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial main universe restricted deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial main universe restricted</code></pre><p>Jetson TX2中科大的:</p><pre><code class="hljs bash">deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-updates main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-updates main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-security main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-security main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial-backports main restricted universe multiverse deb http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial main universe restricted deb-src http://mirrors.ustc.edu.cn/ubuntu-ports/ xenial main universe restricted</code></pre><p>修改完保存后 <code>sudo apt update</code> 一些就OK了.</p>]]></content>
    
    
    <categories>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Onboard-Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
