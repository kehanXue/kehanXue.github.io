<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文解读 - EMVS: Event-Based Multi-View Stereo</title>
    <link href="/2020/06/10/EMVS%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <url>/2020/06/10/EMVS%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<p>EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time 描述了一种利用 Event Camera 的高速特性，提出的一种 MVS 方法。将传统 MVS 的 Space-Sweep 方法应用到 Event-based MVS 中。<br><a id="more"></a></p><h1 id="EMVS-Event-Based-Multi-View-Stereo—3D-Reconstruction-with-an-Event-Camera-in-Real-Time"><a href="#EMVS-Event-Based-Multi-View-Stereo—3D-Reconstruction-with-an-Event-Camera-in-Real-Time" class="headerlink" title="EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time"></a>EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time</h1><blockquote><p>Rebecq, H., Gallego, G., Mueggler, E., Scaramuzza, D.,<br><em><a href="https://doi.org/10.1007/s11263-017-1050-6" target="_blank" rel="noopener">EMVS: Event-Based Multi-View Stereo—3D Reconstruction with an Event Camera in Real-Time</a></em>,<br>Int. J. of Computer Vision (IJCV), 126(12):1394-1414, 2018. <a href="http://rpg.ifi.uzh.ch/docs/IJCV17_Rebecq.pdf" target="_blank" rel="noopener">PDF</a>, <a href="https://youtu.be/EFpZcpd9XJ0" target="_blank" rel="noopener">YouTube</a>, <a href="https://github.com/uzh-rpg/rpg_emvs" target="_blank" rel="noopener">Code</a>.</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本篇 paper 主要描述了一种利用 Event Camera 的高速特性，提出的一种 MVS 方法。将传统 MVS 的 Space-Sweep 方法应用到 Event-based MVS 中；在 Event-based 场景运用 Space-Sweep 具有天然的优势：</p><ol><li><p>Event Camera 本来就是检测的场景中的 edge。</p></li><li><p>Event Camera 相比于固定帧率的传统 Camera，viewpoints 更加的稠密。</p><p><img src="/assets/image-20200614164507222.jpeg" srcset="/img/loading.gif" alt="image-20200614164507222" style="zoom:50%;" /></p></li></ol><p>文章的整体思路简而言之，就是将“每一帧”的 Events 进行反投影得到了一束光线，通过计算经过 DSI 空间中每一个 voxel 的光线的数量，高于一个阈值即可确定为一个空间的 3D 点。具体实施起来，分为下文中的五步。</p><p>完整的公式推导见原论文。</p><h2 id="主要过程"><a href="#主要过程" class="headerlink" title="主要过程"></a>主要过程</h2><h3 id="Feature-Viewing-Rays-by-Event-Back-Projection"><a href="#Feature-Viewing-Rays-by-Event-Back-Projection" class="headerlink" title="Feature-Viewing Rays by Event Back-Projection"></a>Feature-Viewing Rays by Event Back-Projection</h3><p>将每一个 Event 进行反投影，得到一条光线。</p><blockquote><p>This higher abundance of measurements and viewpoints in the event-based setting generates many more viewing rays than in frame-based MVS, and therefore, it facilitates the detection of scene points by analyzing the regions of high ray density.</p><p>A major advantage of our method is that no explicit data association is needed.</p></blockquote><p>不需要 Event 之间的数据关联、不需要 intensity 信息。</p><h3 id="Volumetric-Ray-Counting-Creating-the-Disparity-Space-Image-DSI"><a href="#Volumetric-Ray-Counting-Creating-the-Disparity-Space-Image-DSI" class="headerlink" title="Volumetric Ray Counting. Creating the Disparity Space Image (DSI)"></a>Volumetric Ray Counting. Creating the Disparity Space Image (DSI)</h3><p>在 DSI 中进行计数。这个时候问题就来了，如何确定这个 DSI 空间呢？</p><p>作者考虑到 <code>the reconstruction of large scenes in a scalable way</code>，采用了一个分批的方式，将一组 Event 归位一批，选取一个虚拟帧 RV，基于它做一个 <code>local 3D reconstruction</code>。</p><p><img src="/assets/image-20200614172203728.jpeg" srcset="/img/loading.gif" alt="image-20200614172203728" style="zoom:50%;" /></p><h3 id="Detection-of-Scene-Structure-by-Maximization-of-Ray-Density"><a href="#Detection-of-Scene-Structure-by-Maximization-of-Ray-Density" class="headerlink" title="Detection of Scene Structure by Maximization of Ray Density"></a>Detection of Scene Structure by Maximization of Ray Density</h3><p>Local DSI 建立好之后，就卡一个阈值，是一个 <code>local maxima</code> 的操作。一个DSI的实例以及使用 <code>local maxima</code> 而不是 <code>global maxima</code> 的原因如下：</p><p><img src="/assets/image-20200614172541662.jpeg" srcset="/img/loading.gif" alt="image-20200614172541662" style="zoom:50%;" /></p><p>作者使用了一个 Adaptive Gaussian 阈值函数：</p><blockquote><p>a pixel $(x, y)$ is selected if $c(x, y) &gt; T(x, y)$, with $T(x, y)=c(x, y) * G \sigma(x, y)-C$. In practice, we use a 5×5 neighborhood in $G \sigma(x, y)$ and $C$ = −10</p></blockquote><p>从 DSI 中提取结构：</p><p><img src="/assets/image-20200614174118964.jpeg" srcset="/img/loading.gif" alt="image-20200614174118964" style="zoom:50%;" /></p><h3 id="Merging-Depth-Maps-from-Multiple-Reference-Viewpoints"><a href="#Merging-Depth-Maps-from-Multiple-Reference-Viewpoints" class="headerlink" title="Merging Depth Maps from Multiple Reference Viewpoints"></a>Merging Depth Maps from Multiple Reference Viewpoints</h3><p>既然有了上面提到的 Local reconstruction，那么就肯定有 key frame 的概念和一次 local 的大小的确定。</p><blockquote><p>we select a new <em>key</em> reference view as soon as the distance to the previous <em>key</em> reference view exceeds a certain percent- age of the mean scene depth (typically a number between 15 and 40%), and use the subset of events until the next <em>key</em> ref- erence view to estimate the corresponding semi-dense depth map of the scene.</p></blockquote><p>然后再做滤波去除一些噪声。融合的话，有了每组 local 点云的 pose ，有很多的方法就可以选了。</p><h3 id="Map-Cleaning"><a href="#Map-Cleaning" class="headerlink" title="Map Cleaning"></a>Map Cleaning</h3><p>为了得到更好的效果，又进一步做了滤波。</p><blockquote><p>we use a median filter on the semi-dense depth maps</p><p>we also apply a radius filter (Rusu and Cousins 2011) to the final point cloud, which discards the points whose number of neighbors within a given radius is less than a threshold.</p></blockquote><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><h3 id="实现实时、高效的-Event-反投影到-DSI-中"><a href="#实现实时、高效的-Event-反投影到-DSI-中" class="headerlink" title="实现实时、高效的 Event 反投影到 DSI 中"></a>实现实时、高效的 Event 反投影到 DSI 中</h3><p>主要思路：DSI 是由不同 depth 的平面们组成的，也就是说明只有 translation，无 rotation。那么，求出一个相机的当前帧与一个深度为 $Z_0$ 的 plane 之间的 homography，与其他 DSI 中的深度为 $Z_i$ 的 plane 的 homography 就可以很简单的来计算，可以推导为一步公式。</p><p>相机平面、EV 虚拟相机平面、不同深度 $Zi$ DSI 平面之间的关系：</p><script type="math/tex; mode=display">\left(x\left(Z_{0}\right), y\left(Z_{0}\right), 1\right)^{\top} \sim_{\mathrm{H} Z_{0}}(u, v, 1)^{\top}</script><script type="math/tex; mode=display">\left(x\left(Z_{i}\right), y\left(Z_{i}\right), 1\right)^{\top} \sim H_{Z_{i}} H_{Z_{0}}^{-1}\left(x\left(Z_{0}\right), y\left(Z_{0}\right), 1\right)^{\top}</script><script type="math/tex; mode=display">\mathrm{H}_{Z_{i}} \mathrm{H}_{Z_{0}}^{-1} \stackrel{(8)}{=}\left(\mathrm{R}+\frac{1}{Z_{i}} \mathbf{e}_{3}^{\top}\right)^{-1}\left(\mathrm{R}+\frac{1}{Z_{0}} \mathbf{e}_{3}^{\top}\right)</script><p>其中，$\mathbf{e}_{3}$为平面法向量，因为与 EV 相机平面垂直，所以为$(0, 0, 1)$，在后面的公式化简中有用到。</p><p>令$\left(C<em>{x}, C</em>{y}, C<em>{z}\right)^{\top} \doteq \mathbf{C}=-R^{\top} \mathbf{t}$，通过展开、以及利用$\mathbf{e}</em>{3}$性质，</p><script type="math/tex; mode=display">\mathrm{H}_{Z_{i}} \mathrm{H}_{Z_{0}}^{-1} \sim \mathrm{I}+\frac{Z_{0}-Z_{i}}{Z_{0}\left(Z_{i}-C_{z}\right)} \mathbf{C e}_{3}^{\top}</script><p>进一步代入上面几个平面的关系，化简得：</p><script type="math/tex; mode=display">\begin{array}{l}x\left(Z_{i}\right)=\frac{Z_{0}}{Z_{i}} \delta x\left(Z_{0}\right)+\frac{1}{Z_{i}}(1-\delta) C_{x} \\y\left(Z_{i}\right)=\frac{Z_{0}}{Z_{i}} \delta y\left(Z_{0}\right)+\frac{1}{Z_{i}}(1-\delta) C_{y}\end{array}</script><p>其中$\delta=\left(Z<em>{i}-Z</em>{0}\right) /\left(Z<em>{0}-C</em>{z}\right)$。</p><p>至此，计算效率就提高上来了。</p><p>算法流程：</p><p><img src="/assets/image-20200614203043203.jpeg" srcset="/img/loading.gif" alt="image-20200614203043203" style="zoom:50%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>3D-Vision</tag>
      
      <tag>MVS</tag>
      
      <tag>3D-Reconstruction</tag>
      
      <tag>Event-Camera</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文解读 - DroNet: Deep learning 在无人机导航中的应用</title>
    <link href="/2019/06/08/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-Deep-Learning%E5%9C%A8%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%AF%BC%E8%88%AA%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <url>/2019/06/08/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB-Deep-Learning%E5%9C%A8%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%AF%BC%E8%88%AA%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>DroNet: Learning to Fly by Driving 提出了一个结构非常简单但是又非常强大的网络结构，可以通过输入的每帧图像输出当前飞行器 yaw 的目标角度值与前方障碍物的概率值，从而可以利用这两个信息推断出飞行器当前运动时的 yaw 应转角度 $\theta_k$ 与前进飞行速度 $v_k$，从而达到自主导航的目的。</p><a id="more"></a><h1 id="DroNet-Learning-to-Fly-by-Driving"><a href="#DroNet-Learning-to-Fly-by-Driving" class="headerlink" title="DroNet: Learning to Fly by Driving"></a>DroNet: Learning to Fly by Driving</h1><blockquote><p>A. Loquercio, A.I. Maqueda, C.R. Del Blanco, D. Scaramuzza<br>DroNet: Learning to Fly by Driving,<br>IEEE Robotics and Automation Letters (RA-L), 2018.<br><a href="http://rpg.ifi.uzh.ch/docs/RAL18_Loquercio.pdf" target="_blank" rel="noopener">PDF</a> <a href="https://youtu.be/ow7aw9H4BcA" target="_blank" rel="noopener">YouTube</a> <a href="https://github.com/uzh-rpg/rpg_public_dronet" target="_blank" rel="noopener">Software and Datasets</a>.</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>DroNet 是一个结构非常简单但是又非常强大的网络结构，可以通过输入的每帧图像输出当前飞行器 yaw 的目标角度值与前方障碍物的概率值，从而可以利用这两个信息推断出飞行器当前运动时的 yaw 应转角度 $\theta_k$ 与前进飞行速度 $v_k$，从而达到自主导航的目的。对比论文发布当时的其他相关网络模型，达到了最好的准确度与处理速度的平衡。</p><p><img src="/assets/1560001100081.png" srcset="/img/loading.gif" alt="1560001100081"></p><p>该系统在非机载处理资源上运行 （Intel Core i7 2.6 GHz CPU）上可以达到 30Hz 的控制指令输出。可想如果机载选择 TX2 此类处理器, 采用 GPU 进行推理计算的话, 速度会更快。 </p><p>该模型的训练数据采用室外场景下在地面交通工具上采集的数据集，比如自行车, 汽车等在城市环境内第一视角的图像与其他数据。实验结果惊奇的发现该方法不仅在室外不同视角下表现极好（5m飞行高度），在室内场景中也有极强的泛化能力。飞机可以在没有先验信息的环境中也可以有一个非常好的导航效果。</p><h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><p>文章以“无人机应该像其他地面交通工具一样, 在 roadway 中有相同的 behavior”为出发点，通过来自于地面交通工具的数据集，做了以下主要工作：</p><ul><li>提出了一种 residual convolutional architecture (DroNet)，可以预测飞机要转的偏航角与前方发生与障碍物碰撞的概率，可以为飞机在城市环境中提供安全的飞行。通过来自于室外场景下汽车，自行车的数据集来训练该网络。</li><li>建立了一个关于预测是前方障碍物概率的数据集。</li><li>可以到到很好的 performance 和 real-time。</li><li>通过一些扩展场景的检验，发现该系统的泛化能力极强，可以在没有任何先验信息的环境中正常运行，包括数据集中没有的室内走廊场景，一个高高度（视角不一样）等场景.</li></ul><p>文章作者还提到该方法并不是为了替代传统的 map-localize-plan 方法, 作者认为将来有一天传统方法与基于深度学习的方法会互补.</p><h2 id="关于网络与训练方法"><a href="#关于网络与训练方法" class="headerlink" title="关于网络与训练方法"></a>关于网络与训练方法</h2><p><strong>网络的结构图如下:</strong></p><p><img src="/assets/Screenshot from 2019-06-08 19-56-55.png" srcset="/img/loading.gif" alt=""></p><p>输出的转角与障碍物信息两个功能在前面共用同一个网络结构（共享同一套参数）。输入的图像是一张200<em>200</em>1 的灰度图，通过一个 5*5 的卷积核降维后，通过三层 res block，然后经过 dropout（作者实验时设的是0.5）后再分叉,  通过 Relu 后作用于两个全连接层（节点数为 1）分别输出信息（大小范围均在0-1之间）。关于为何有这样的网络设计的想法，作者并未多提。</p><blockquote><p>通过代码看到最终 collision 这个全连接节点最终输出还被作用了一个 sigmod 函数，这个论文中并未提到为何。猜测是用于归一化（概率值必须大于 0 小于 1，而刚开始训练的时候 loss 大多依据（后面会讲到） steering 的 loss，可能会出现 [-1, 0] 之间的数，所以要归一化到 (0,1)） / 某种意义上的数据增强？具体只能等跑训练代码的时候看一下实际该参数的值。</p><p>cnn_models.py</p><pre><code class="hljs python"><span class="hljs-number">85</span>    <span class="hljs-comment"># Collision channel</span><span class="hljs-number">86</span>    coll = Dense(output_dim)(x)<span class="hljs-number">87</span>    coll = Activation(<span class="hljs-string">'sigmoid'</span>)(coll)</code></pre></blockquote><p><strong>关于训练方法:</strong></p><p>关于转角的预测本质是一个回归问题，关于障碍物的检测本质是一个二分类问题（虽然最后输出的是概率, 但是从数据集本质来看是一个二分类问题，这个后续详述）。该网络比较特殊（两种不同的问题模型的输出，共享网络），所以要设计出一种合理的 loss 函数. </p><p>根据两类问题的本质，转角预测本质为回归问题所以采用均方误差（MSE）衡量 loss；障碍物概率预测本质为二分类问题所以采用二值交叉熵（BCE）来衡量学习到的分布与样本真实分布的差异，作为 loss。但是整个网络不能简单使用两个 loss 叠加来来作为最终的 loss，会导致特别差的收敛结果，因为回归问题和分类问题在模型刚开始训练的时候，梯度大小差异非常大<a href="https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf" target="_blank" rel="noopener">参考文献</a>。</p><p>实际中，回归问题的梯度在刚开始的时候会非常大，MSE 的梯度正比于转角的误差值。所以策略就是刚开始的时候几乎只选择用转角的 loss，后面随着 epoch 的增加慢慢增大障碍物概率检测的 loss 的权重，等到两者 loss 在一个数量级可比的时候，optimizer 就会自动为两者找到一个很好的 solution。该方式的解释也在上一个参考文献链接里，不设权重或者权重恒定的方法都会导致不好的结果或者收敛时间过长，所以依据此作者提出了下面的 loss：</p><script type="math/tex; mode=display">L_{t o t}=L_{M S E}+\max \left(0,1-\exp ^{-d e c a y\left(e p o c h-e p o c h_{0}\right)}\right) L_{B C E}</script><p>该方式就可以达到上面所说的期望的训练过程中的 loss 函数变化情况。作者在实验时选择 $decay=\frac{1}{10}$,  $epoch_{0}=10$。</p><p>optimizer 选择 Adam，初始学习率设为0.001，decay=1e-5。</p><p>最后作者为 optimization 还采用了 hard negative mining 来建立负样本集, 在每一个 epoch 中选择 loss 最高的 k 个样本, 采用上面计算 loss 的式子计算整体 loss。k 会随着时间会减小。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>关于转角预测（Steering angle）采用来自 Udacity’s project 的公共数据集，该数据集是基于汽车拍摄的。里面有三个摄像头以及 imu，gps，steering angle 等其他同步的数据，作者只选用前置摄像头与 steering angle 作为模型训练时所采用的数据。</p><p>关于用于计算障碍物概率的数据集，由于没有合适的数据集，作者们自制了一套相关数据集，137 个场景序列中包含了 32000 张图片。根据视野障碍物是否离得特别近来标注 0（无碰撞风险）和 1（有碰撞风险）。例图如下，绿色表示无碰撞风险，红色表示有碰撞风险。</p><p><img src="/assets/1560001239435.png" srcset="/img/loading.gif" alt="1560001239435"></p><h2 id="飞机运动控制方法"><a href="#飞机运动控制方法" class="headerlink" title="飞机运动控制方法"></a>飞机运动控制方法</h2><p>整体的导航思路很简单，飞机一直在同一高度飞行，只控制飞机的两个自由度，机体坐标系下前进的速度 $v_k$ 和世界坐标系下的 yaw 值 $\theta_k$.</p><ul><li><p>根据网络输出的前方发生与障碍物碰撞的概率 $p_t$ 计算前进的速度 $v_k$:</p><script type="math/tex; mode=display">v_{k}=(1-\alpha) v_{k-1}+\alpha\left(1-p_{t}\right) V_{\max }</script><p>公式很简单, 即前方发生碰撞概率越大, 前进的速度越低, 发生碰撞概率为 1 的时候速度为0.然后加了低通滤波使速度输出更平滑($0&lt;\alpha&lt;1$). </p></li><li><p>根据网络输出的 steering angle 换算成实际要转的偏航角大小。网络输出的范围为 [-1, 1]，换算成$\left[-\frac{\pi}{2}, \frac{\pi}{2}\right]$。然后也同理加了个低通滤波：</p><script type="math/tex; mode=display">\theta_{k}=(1-\beta) \theta_{k-1}+\beta \frac{\pi}{2} s_{k}</script></li></ul><p>然后根据这两个值赋给飞机就可以控制飞机运动了。作者在试验中选择的 $\alpha=0.7$ 和 $\beta=0.5$。$V_{\max }$根据实验场景不同选择合适的值即可。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>作者在大量场景中进行了测试，具体的测试结果这里不在赘述。</p><p>DroNet 模型是一个平衡结果准确性与运算速度的最佳的模型。</p>]]></content>
    
    
    <categories>
      
      <category>Paper Reading</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep-Learning</tag>
      
      <tag>Navigation</tag>
      
      <tag>UAV</tag>
      
      <tag>Robotics</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
